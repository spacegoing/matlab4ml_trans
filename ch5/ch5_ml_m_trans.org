#+LATEX_HEADER: \usepackage{ctex}
#+LATEX_COMPILER: xelatex

* 第五章：模式识别之分类算法
分类算法能够学习如何对观测到的数据进行分类。进行分类学习的第一步
是对数据集进行标注。对分类的每个类别，我们都预先定义一个标签
(label)，这个标签可以是任意值（整数、字符串等，一般用正整数），接
着对每个样本的'feve_snp'，我们都对其标注一个预先定义好的标签，用
于表示这个样本属于哪个类别。我们称只有两个类别（标签）的分类问题
为二分类问题；有多个类别（标签）的则称为多分类问题。我们将从样本
的'feve_snp'到标签的映射函数称为'classifier_snp'。

分类问题与第四章中学习的回归问题有一些相似之处。例如，它们都是从
已经标注好的数据集中，学习输入数据到输出数据的映射关系的。两者最
大的区别在于，回归问题的输出数据是连续值，而分类问题的输出数据是
离散值，即分类的类别标签。

举例而言，回归模型可被用于基于前10年的石油价格数据，预测未来的石
油价格。然而，二分类模型可被用于预测石油价格的走势，即是涨还是跌。
在回归问题中，输出数据石油价格是连续变量。而在分类问题中，尽管输
入数据不变，仍然是前10年石油价格数据，即连续变量，但输出数据变成
了涨跌分类，即价格走势是涨还是跌。

本章中，我们将展示如何使用'knn_snp'、'dian_snp'分类、'detree_snp'
分类和'naba_snp'算法。我们还将介绍概率论在分类问题中所扮演的角色。
在本章末尾，我们将理解这些算法的内容，并学会如何在MATLAB中实现、
运行这些算法。

** 'detree_snp'分类
'detree_snp'可被看为是一系列判断结果及其判断条件的图形化展示，尤
其是对那些人脑极难描述清楚的判断条件的展示。一个判断结果往往是根
据一连串层级化的判断条件得出的，这使我们很难单纯地使用表格、数字，
来正式地、易于人脑理解地描述这些判断过程。

与表格、数字相反，树形结构能够很好地帮助我们描述、理解一系列的判
断结果及其条件。通过追溯整棵树，人脑可以立即理解整个判断的过程及
所有可能的结果。与其通过一系列代数推理、公式描述来理解判定结果，
先观察到判定结果、再通过回溯整棵决策树显然更易于人脑理解。

决策树由以下几部分构成：

- 'node_snp'表示变量，其名称即变量名称
- 'branch_snp'表示变量在其定义域中能够选取的范围
- 'leaf_snp'表示分类结果。（没有子节点的节点即为叶节点）

通过上面这些定义，我们能够对数据集中每个样本进行分类，并能够给出
分类结果是正确的概率的大小。因此，'detree_snp'能够给出任意一个样
本有多大概率属于任一分类。下图展示了一颗分类树：

#+BEGIN_QUOTE
图5.1 分类树示例
#+END_QUOTE

通过学习标注后的训练集，我们能够总结出分类规律（树状结构）。当在
测试集完成测试，达到满意的精度后，算法所生成的决策树可被用来对未
经标注的数据进行分类。

#+BEGIN_QUOTE
小贴士：'detree_snp'是最简单的分类算法之一。这种算法通过遍历全部
变量、及其可选的分类条件，来生成最优分类树。
#+END_QUOTE

对于一元二分类问题（样本的类别只由一个变量决定；只有两个类别），
决策树只由一个节点、两个分支和两个叶节点构成。对于多元分类问题
（样本的类别由多个变量共同决定），决策树由多个一元决策树的线性组
合构成。

决策树由'node_snp'，'branch_snp'和'leaf_snp'构成。具体而言，每
个'node_snp'代表一个变量，并由变量名命名。每条边（分支）上标注的
是沿这个分支，这个'node_snp'的取值范围。'leaf_snp'则标注最终分类
结果，及类别标签。

当决策树用于分类时，对未标注的样本，根据其特征向量中每个变量的值，
沿着决策树从根节点走到叶节点，即得到该样本的分类结果。所走过的路
径即是对此样本分类过程中所用到的判断条件。对于一颗决策树，每条从
根节点到'leaf_snp'的路径都是一组判断条件。

使用决策树对样本进行分类的过程可总结为：

1. 从根节点开始
2. 根据当前节点的名称，选择待分类样本特征向量中，对应变量的值
3. 根据当前节点的分支的定义域，及待分类样本对应的值，选择待分类样
   本所属的分支
4. 根据分支到达下一节点。如果此节点是'leaf_snp'，算法结束并返
   回'leaf_snp'的值（即分类结果）；如果此节点是'node_snp'，则回到
   第二步继续执行

为了更好地理解这个过程，我们来考虑图5.2中的例子。这个树包含两个自
变量 $x1$ 和 $x2$。如果待分类样本的特征向量中，$x1<0.3$且$x2>0.6$，
那么它的分类结果就是 ~true~ ，反之则为 ~false~ 。整个分类过程如下：
从根节点（图中节点由$\Delta$表示）开始执行算法，当
前'node_snp'（根节点）的标签是 $x2$，根据分支条件，如果待分类样本
的特征向量中$x2$的值大于$0.6$，我们则选择右分支。右分支的子节点
是'leaf_snp'，因此算法结束并返回分类结果 ~true~ 。如果待分类样本
中$x2$小于$0.6$，则选择左分支。左分支的子节点为$x1$，因此我们根据
待分类样本中$x1$的值进行判断。如果$x1$小于$0.3$，则返回分类结果
~true~ ，反之则返回 ~false~ 。

构建并展示图5.2的代码如下：

#+BEGIN_QUOTE
代码
#+END_QUOTE

先不要着急阅读上面的代码，我们稍后会进行详细解释。首先我们先看上
面代码所生成的决策树的图形展示：

#+BEGIN_QUOTE
图5.2 决策树样例
#+END_QUOTE

对于有编程基础的读者上图非常容易理解。决策树表示成程序就是面向过
程编程中一系列的条件判断(~if-else~)语句。当到达'leaf_snp'即没有更
多 ~if~ 语句时，程序则执行结束并返回分类结果。

当我们到达'leaf_snp'即得到分类结果时，分类过程中所使用的一系列判
断条件都展示在执行算法所经过的路径上。因此，任何决策树都可以看成
是一组判断规则的集合。每条判断规则代表着从树的根节点到叶节点的路
径。一棵树有多少叶节点，表示成集合这个集合中就有多少条规则。由规
则的集合所表达的决策树，与树结构表达的决策树拥有相同的计算复杂度。

接下来我们将展示如何在MATLAB中实现整个流程。'smltb_snp'封装有从训
练数据中学习决策树所需的所有工具。为了重复整个流程，我们将使用机
器学习的经典数据集——Iris Flower数据集。这是一个由英国统计和生物学
家Ronald Fisher在其1936年发表的论文《The use of multiple
measurements in taxonomic problems》中用于展示其线性判别分析算法
所使用的，多元分类（每个样本的特征向量有多个特征值，及因变量）数
据集。

#+BEGIN_QUOTE
（已安装MATLAB的读者不需下载）可以通过如下链接：

https://archive.ics.uci.edu/ml/datasets/iris

从'uci_snp'中下载数据集及其简短描述。
#+END_QUOTE

这个数据集收集了三种Iris花(Iris setosa, Iris virginica, and Iris
versicolor)每种50个样本。每个样本的特征向量包含4个特征值（以厘米
为单位）：

- 萼片(Sepal)长度
- 萼片(Sepal)宽度
- 花瓣(Peal)长度
- 花瓣(Peal)宽度

- 数据集中类别标签为： ~Setosa~, ~Versicolour~, ~Virginica~

接下来我们通过学习Iris数据集中150个样本的特征向量（每个样本4个特
征值）到分类标签（供三种标签）的映射关系，构建决策树。MATLAB已经
自带了Iris数据集。我们可通过执行如下代码将数据集加载
到'workspace_snp'中：

#+BEGIN_QUOTE
代码
#+END_QUOTE

执行代码后，两个变量被加载到了MATLAB： ~meas~, ~species~ 。第一个
变量为150个样本的特征向量构成的矩阵（150x4 ~double~ 类型）。第二
个变量是对每个样本所属类型进行标注的标签向量 (150x1 ~cell~ 类型）。
通过执行如下代码可以得到变量的统计数据：

#+BEGIN_QUOTE
代码
#+END_QUOTE

通过执行上述代码，我们验证了这150个样本所属的类别是均匀分布的。接
下来我们通过绘制数据集的'scatter_plot_snp'来观察每种类别所对应的
特征值的分布情况。

在第三章中，我们已经学习过如何绘制'scatter_plot_snp'，但是这里我
们需要使用绘制函数的额外功能。现在我们希望除了散点，我们还能绘制
每个点所述的类别。我们可以通过 ~gplotmatrix()~ 函数实现这点。
~gplotmatrix()~ 的调用语法是：

#+BEGIN_QUOTE
代码
#+END_QUOTE

这里， ~a~ （m x n1) , ~b~ (m x n2) 表示的是全部样本（m个）的特征
矩阵。执行代码将得到(n1 x n2)副散点图。第i行j列散点图是使用 ~a~
矩阵的第 ~i~ 列特征向量和 ~b~ 矩阵的第 ~j~ 列特征向量绘制的。在我
们的例子中，我们希望根据 ~meas~ 特征矩阵，及 ~species~ 中标注的类
别标签绘制散点图：

#+BEGIN_QUOTE
代码
#+END_QUOTE

执行结果如下图所示：

#+BEGIN_QUOTE
图5.3 按照 ~species~ 进行分类绘制的散点图矩阵
#+END_QUOTE

粗略观察图5.3，我们可以看出 *setosa* 类与其它两类非常不同。相反的，
其余两类在所有散点图中都存在大量重合。

我们首先仔细观察根据花瓣的特征值（长度和宽度）绘制的散点图。我们
可以使用 ~gscatter()~ 函数实现这点。 ~gscatter()~ 函数的输入参数
要求为三个参数，两个长度相等的特征向量和一个标签向量。 ~meas~ 变
量的第3列表示花瓣长度，第4列表示花瓣宽度，因此我们的代码是：

#+BEGIN_QUOTE
代码
#+END_QUOTE

散点图中清晰地展示了三种不同的花在特征空间中是如何分布的：

#+BEGIN_QUOTE
图5.4 按照分类标签标注的散点图
#+END_QUOTE

上图表明，根据花瓣的特征值对花朵进行分类是可行的（上图只根据花瓣
的长度和宽度已经能够看出清晰的分类结果）。为对整个特征值矩阵构建
决策树，我们可以使用 ~fitctree()~ 函数。这个函数返回基于特征矩阵
和标签向量构建的二叉树状决策树：

#+BEGIN_QUOTE
代码
#+END_QUOTE

返回的二叉树中每个节点有两个分支，分支结果是按照特征值矩阵 ~meas~
中每一列向量（样本数 x 1）进行优化得出的（优化算法即 ~fitctree~ 所
封装的算法，再次我们此函数的忽略实现细节）。现在我们可以使用函数
~view()~ 绘制二叉树图。 ~view()~ 函数有两种使用方法。单纯调
用 ~view(ClassTree)~ 命令将返回文字描述的 ~if-else~ 指令集（上面讨
论过指令集等同于二叉树结构）。 ~view(ClassTree, 'mode', 'graph')~
添加参数后调用则会绘制二叉树树状图。下面我们先看第一种调用方法：

#+BEGIN_QUOTE
代码
#+END_QUOTE

正如我们所见， ~fitctree()~ 优化生成的二叉树中只使用了 $x_3$ 和
$x_4$ 两个特征，即花瓣的长度和宽度。下面我们来绘制二叉树的树状图：

#+BEGIN_QUOTE
代码
#+END_QUOTE

下图展示了二叉树树状图。每个'node_snp'及其'branch_snp'表示了选
取某条路径，样本的特征值所必须满足的取值范围。'leaf_snp'表示最终
分类结果：

#+BEGIN_QUOTE
图 5.5：二叉树树状图
#+END_QUOTE

图5.5清晰可见地展示出给花朵分类的整个流程。在完成构建决策树后（即
执行 ~fitctree()~ 后，我们可以非常方便地使用 ~ClassTree~ 对新的样
本进行分类。假设待分类新样本的特征向量如下（萼片的长度、宽度；花
瓣的长度、宽度）：

#+BEGIN_QUOTE
代码
#+END_QUOTE

为对样本进行分类，我们将待分类样本的特征向量 ~measNew~ 输入训练好
的决策树 ~ClassTree~ 中对分类结果进行预测：

#+BEGIN_QUOTE
代码
#+END_QUOTE

~predict()~ 函数返回一个最终分类的标签标量或向量，这取决于输入参
数是单一样本的特征向量还是多个样本的特征值矩阵。上面代码只会返回
一个标量，及待分类样本 ~measNew~ 经 ~ClassTree~ 分类后的分类标签。
当输入多个分类样本时，返回的向量顺序与样本的输入顺序一一对应。

目前为止，我们已经学习了如何使用标注好的数据集训练'detree_snp'。
现在我们需要验证其预测效果优劣。我们都有什么工具来检验决策树的预
测效果呢？

首先，我们需要一个计算训练误差的指标。训练误差衡量决策树对待分类
样本所预测的分类标签，与数据集中真实标签的差距的指标。这种指标可
以初步地表达模型性能的优劣。这种衡量是单调的，即指标值越高，说明
预测结果与真实值差异越大，预测效果越坏。反之，低指标值反应出模型
有很好的预测效果。

使用如下函数计算训练误差：

#+BEGIN_QUOTE
代码
#+END_QUOTE

结果显示决策树能够对绝大多数样本给出正确分类。接下来我们继续验证
模型的预测误差。与训练误差是衡量模型对训练集中已经学习过（现有参
数通过这些样本求出）样本的预测性能不同，预测误差使用预测集中的样
本，即模型仍未学习过的数据（现有参数的整个优化过程中与这些样本无
关），对模型的泛化能力（在预测集中、对新样本的预测能力）进行评估。
我们使用'cv_snp'进行验证。

# 下面两段在原书中出现在下一节
之前强调过，训练误差和'confmat_snp'（下文中会讲述）只能够衡量模型
对训练集拟合程度的好坏，不代表任何对模型泛化能力相关的衡量。然而，
简单地将数据集分为训练集、测试集两块，并使用测试集来衡量模型的泛
化能力并不是十分精确、具有鲁邦性的方法。因为单一的训练集、测试集
划分很可能将过于复杂或简单的样本集中在任一数据集中，造成预测误差
过大或过小。因此，对模型在不同训练集、预测集上进行多次训练、预测，
能够更加鲁棒、精确地衡量模型的泛化能力。然而这样要求大量的数据，
通过使用'cv_snp'，我们在少量数据集中就可以实现以上目标。

交叉验证将整个数据集等分为 $K$ 份。每次验证都抽取一份作为验证集，
其余作为训练集。这个过程将被重复 $K$ 次。每次都使用了整个数据集，
因此每个样本都至少被训练并预测过一次（随机划分的交叉验证并非如此）。
这样就能相对精确、鲁棒地对模型泛化能力进行衡量。

MATLAB中对'cv_snp'的默认设置是10折交叉验证。'cv_snp'将整个数据集
等分为10份，随机选取9份作为训练集训练决策树，使用剩下的1份作为预
测集，对模型预测结果进行检验。我们可以使用如下代码实现'cv_snp'：

#+BEGIN_QUOTE
代码
#+END_QUOTE

我们首先调用 ~crossval()~ 函数，它将使用'cv_snp'的方法对模型进行
检验，并返回一个交叉验证模型 ~cvrtree~ 。接着，我们使
用~kfoldLoss()~ 函数来计算整个 'cv_snp' 的预测误差。结果显示即使
对模型没见过的样本（预测集中、模型训练过程没使用过的样本），决策
树依然能够对绝大多数样本给出正确分类。

** 概率分类模型——'naba_snp'

贝叶斯分类属于统计学中，用于判断样本属于某一分类的概率的一种方法。
这种方法可被用于，例如，我们基于顾客的工作状况、年龄、收入、喜爱
的运动等信息，判断顾客有多大可能购买一辆跑车。

这种方法的理论基础是贝叶斯理论。贝叶斯是英国18世纪的一位数学家。
这个理论给出了'poste_snp'与'prior_snp'和'like_snp'之间的关系。后
验概率是指，当观察到某些情况已经发生后，待观察的事件发生的概率是
多少。

'naba_snp'利用这个理论，进一步假设，当给定一个样本的分类时，样本
的特征向量中，每个特征值的取值概率，与其它特征值的取值'CI_snp'。
这个假设能够大大简化联合概率分布的计算复杂度，因此被称为朴素
(naive)。当数据集真正满足'CI_snp'这个条件时，'naba_snp'与更加复杂
的模型有同样优秀的结果。


***  概率论基础

在正式学习之前，我们先为读者回顾一些概率学基本概念。如果你已经熟
悉这些概念，那么可以跳过本节。我们建议读者首先确保对基本概念的熟
悉再继续阅读下面模型方面的内容。

首先考虑一个简单的例子。假设有一个不透明箱子，里面有7个白球和3个
黑球，并假设每个球之间除了颜色，其它属性（如重量、材质等）是完全
一致的。现在随机从箱子中取出一个球，请问取出黑球的概率是多少？

- 箱子中共有10个球，因此总共有10种取到不同的球的情况。并且取到任
  何一个具体的球的可能性是均匀分布的，任何球都有相等的可能性被取
  到
- 在这10种情况中，只有3种是取到黑球

因此，在 $取到的球是黑球$ 这个 *事件* 中，10种情况中只有3种符合这
个 *事件* 。我们将 *概率* 定义为 *事件* 发生的情况数在总情况数中的比
率，因此我们得到：

$$取到黑球的概率 = 3/10 = 0.3 = 30\%$$

由此可见，一个 *事件* 发生的概率可以被表示为：

- 分数： $3/10$
- 小数： $0.3$
- 百分数： $30\%$

有了粗略的概念后，我们给出 *概率* 数学公式上的定义。一个 *事件*
$E$ 的概率被定义为事件发生的情况数 $s$ 占总共可能的情况数 $n$ 的
比率。假设所有可能的情况都是等可能发生的（非等可能的情况稍后讨论），
那么可用公式表示为：

$$P=P(E)=\frac{符合事件的情况数}{总可能情况数}=\frac{s}{n}$$

我们来看两个例子：

- 扔一个硬币，硬币朝上面是正面的概率是多少？扔硬币结果的总可能情
  况数为 $2$ ，即 $\{正面，反面\}$ ，因此符合事件的情况数是 $1$
  。所以 $P{朝上面=正面}=\frac{1}{2}=0.5=50\%$
- 扔一个色子，朝上面是$5$的概率是多少？总可能的情况数为$6$，即色
  子总共有6个面。符合事件的情况数是1，因此概率为
  $P(朝上面=5)=\frac{1}{6}=0.166=16.6\%$ 

在上面的定义中，我们用到了“等可能性”这个概念。为更清楚地表述这个
概念，我们引用无差别原则（the Principle of Indifference）来进行解
释：

#+BEGIN_QUOTE
有一组情况，如果没有任何可被证实地理由来证明，某些情况发生的可能
性高于另外一些情况，那么我们认为所有情况发生的可能性是相同的
#+END_QUOTE

在计算总共可能发生的情况、符合事件的情况时，我们经常需要用到排列
组合的知识。

我们已经知道概率可被定义为两个数的比率。完整的定义还应包括，概率
的取值范围是$0$到$1$：

$$0\leq P(E) \leq 1$$

- 概率为0的事件被称为不可能事件。例如，假设我们一个箱子中有6个红
  球，那么从箱子中取一个球，取出的球是黑球的概率为0，即不可能事件
- 概率为1的事件被称为确定事件。在上面的例子中，取出红球的概率为1，
  即确定事件

关于概率的经典定义有很多局限，首先它是从频率角度出发，使用离散且
有限的数字进行定义的，这种定义难以扩展到其它领域。此外，定义中假
设了事件发生的等可能性，即我们事先知道所有可能发生的情况，并且知
道每种情况是等可能发生的，这种极强的假设进一步限制了这种定义的应
用范围。

经典概率的定义是从频率论(Frequentist)的角度出发的，现代概率论
(Probability Theory)与之相比的一大进步就是从频率角度出发引入
了'prior'（例如，我们预先知道一个硬币的质地是不均匀的，反面比正面
重，那么我们在抛硬币之前就已经可以假设，抛掷这个硬币的实验结果是
$75\%$ 可能性正面，这里的 $75\%$ 就是'prior'），即在未观察数据集
中样本之前，人们对这个事件固有的先验知识的概念，并将'prior'与数据
集中观察到的实际情况相结合，从而得到事件的概率。现在我们首先将事
件发生的概率定义扩展为，当有无穷多次重复试验时，事件发生的概率所
逼近的极限值。注意，这个定义是可以适用于没有先验知识，且无需假设
等概率发生的可能性。这个定义的唯一假设是，事件的试验是可以被重复
无穷多次，且每次重复的其它条件完全相同。

有了这个定义，我们就可以使用频率角度下的概率值来逼近概率的极限值。
如果我们有关于一个事件的基于相同条件、大量重复次数的试验结果，那
么我们可以假设基于这个试验的频率所得到的值，是逼近于极限情况下的
真实概率值的：

$$频率 \approx 概率$$

从贝叶斯学派的角度出发，概率是对于一个论断（即事件发生的可能程度）
可信程度的度量。这个定义可被应用于任意事件上。从贝叶斯公式出发，
概率是可以双向推断的，我们可以使用先验概率、似然概率（这些概念稍
后会进行解释）推断后验概率，也可以从后验概率出发推断先验概率。在
贝叶斯公式中，先验概率是指，人们（很多情况是专家、论文结论）对某
一事件发生情况的先验、固有经验，与事件的本次实际试验结果完全无关。
因此先验概率完全是主观的。有了先验概率，我们就可以结合实际试验中
得到的结果（频率角度），来估计事件发生的后验概率。这种方法的精髓
就是将先验经验、知识，与可能存在样本误差的试验结果结合起来，对真
实的概率分布进行估计。

目前为止，我们已经讨论过单一事件发生的概率问题。那么如何估计多个
事件发生的概率呢？现在我们假设有两个相互独立事件（即一个事件发生
的可能性与另外一个事件不相关） $A$ 和 $B$ 。例如，我们有52副扑克
牌，当我们从每副扑克牌中抽取一张卡片时，以下两个事件是相互独立的：

- $E1$ 从第一副扑克牌中抽到 $A$
- $E2$ 从第二副扑克牌中抽到梅花花色的牌

这两个事件是相互独立的，无论一个事件有怎样的结果，都不会改变另一
个事件发生的概率。

与之相反，相互依赖的事件是指，事件 $A$ 发生的概率随着事件 $B$ 是
否发生而改变。假设我们有一副扑克牌（52张），如果我们依次不放回地
连续抽取两张扑克，那么以下两个事件是相互依赖的：

- $E1$ 第一张抽到的牌是 $A$
- $E2$ 第二张抽到的牌是 $A$

准确地说， $E2$ 发生的概率是随 $E1$ 是否发生而改变的：

- $E1$ 发生的概率是 $4/52$
- 如果 $E1$ 发生，那么 $E2$ 发生的概率是 $3/54$
- 如果 $E1$ 没发生，那么 $E2$ 发生的概率是 $4/54$

我们接着研究两个事件中其它模式的依赖关系。如果两个事件不可能同时
发生，我们说两个事件是互斥事件。例如在上面的例子中，以下两个事件
是互斥事件：

- $E1$ 抽到的牌是红桃 $A$
- $E2$ 抽到的牌是人脸牌（纸牌中的 J Q K）

如果一次试验中，两个事件必然有一个发生，我们说这两个事件是互补的。
例如上面的例子中，以下两个事件为互补事件：

- $E1$ 抽到的牌是数字牌
- $E2$ 抽到的牌是人脸牌（纸牌中的 J Q K）

现在我们来考虑多个事件的联合概率分布。假设我们有两个相互独立事件
$A$ 和 $B$ ，那么这两个事件的联合概率分布，等于各自事件的概率分布
的乘积：

$$P(A\cap B)=P(A) \times P(B)$$

举例而言，假设我们有两副扑克牌（52张每副）。我们从两副牌中各抽一
张牌，那么以下两个事件是相互独立事件：

- $A$ 第一副中抽到的牌是 $A$
- $B$ 第二副中抽到的牌是梅花

那么两个事件同时发生的概率为：

- $P(A)=4/52$
- $P(B)=13/52$
- $P(A\cap B)=4/52\cdot 13/52 = 1/52$

如果两个事件是相互依赖的，上面的公式就不成立了。但是我们可以通过
条件概率分布来计算联合概率分布。条件概率分布是指，我们有概率分布
$P(B|A)$ 即当事件 $A$ 发生的条件下，事件 $B$ 的概率分布。由此我们
得到如下公式：

$$P(A\cap B) = P(A) \times P(B|A)$$

例如，假设一个箱子中有2个白球和3个红球，我们从箱子中逐次不放回地
取出两个球。那么，两个球同时是白球的概率为：

- 概率 $P(A)$ 第一次取到白球的概率是 $2/5$
- 条件概率 $P(B|A)$ 如果第一次取到的是白球，那么第二次仍取到白球
  的概率为 $1/4$

根据公式，我们知道两个事件的联合概率为：

$$P(A\cap B)=2/5 \cdot 1/4=1/10$$

理解了上面一系列的例子后，我们开始正式定义条件概率。当已知事件
$B$ 发生后，事件 $A$ 发生的概率，称为事件 $A$ 的条件概率，并使用
符号 $P(A|B)$ 表示。条件概率可以通过如下公式计算（从现在开始我们
使用符号 $P(A,B)$ 代替前文中的 $P(A\cap B)$ ）：

$$P(A|B)=\frac{P(A,B)}{P(B)}$$

通常我们只有在事件 $A$ 依赖于事件 $B$ 时才使用条件概率。如果事件
$A$ 和 $B$ 是相互独立的，那么条件概率退化为（因为相互独立事件
$P(A,B)=P(A)\times P(B)$ ，代入条件概率公式即得如下结果）：

$$P(A|B)=P(A)$$

举例而言，在上面从盒子取两个球的例子中，假设我们已经知道取出的第
一个球是白球，那么取第二个球仍是白球的概率是多少呢？在上面的例子
中我们已经得出，两个球都是白球的概率是 $P(A,B)=1/10$ 。代入条件概
率公式即得如下结果：

$$P(B|A)=\frac{P(A,B)}{P(B)}=\frac{1/10}{2/5}=1/4$$

我们再考虑一个例子。现在假设我们投掷一枚色子，我们已经知道结果是
个奇数，请问结果是 $1$ 的概率是多少？这里我们令事件 $A$ 为投掷结
果是奇数，事件 $B$ 为投掷结果是 $1$ 。

在这个例子中，投掷结果为奇数的概率为 $P(A)=3/6=1/2$ 。因为 $1$ 本
身就是奇数，因此联合概率 $P(A,B)$ 即投掷结果是 $1$ 且是奇数的概率
为 $P(A,B) = 1/6$ 。因此我们得到条件概率：

$$P(B|A)=\frac{P(A,B)}{P(A)}=\frac{1/6}{1/2}=\frac{1}{3}$$




*** MATLAB 中的贝叶斯方法

本节我们开始介绍'naba_snp'及其在MATLAB中的实现。就像在本章开头部
分所说的，'naba_snp'利用'CI_snp'，进一步假设，当给定一个样本的分
类时，样本的特征向量中，每个特征值的取值概率，与其它特征值的取
值'CI_snp'。这个假设能够大大简化联合概率分布的计算复杂度，因此被
称为朴素(naive)。当数据集真正满足'CI_snp'这个条件时，'naba_snp'与
更加复杂的模型有同样优秀的结果。

在MATLAB中，使用'naba_snp'需要两步：

- 第一步， *训练* ：'naba_snp'首先使用预先标注好类别标签的数据集，在
  这个数据集上求解模型参数，即根据'CI_snp'假设，分别估计每个特征
  值的概率分布
- 第二步，预测分类标签（下面简称 *预测* ）：对新的，未经标注的数
  据集，使用第一步中训练好的分类器，计算每个样本属于任一类别
  的'poste_snp'。预测分类标签的结果是使每个样本'poste_snp'最大的
  那个标签

在'detree_snp'部分中我们已经使用过 Iris 花朵数据集。这个数据集非
常精炼，是众多教程中帮助读者理解'naba_snp'的经典数据集。本书中我
们延续这一传统。我们将继续使用 Iris 数据集来学习'naba_snp'。具体
而言，我们将使用 Iris 数据集中的花瓣(petals)数据（长度和宽度）构
建贝叶斯分类器。

为了训练'naba_snp'，我们将使用 ~fitcnb()~ 这个函数。这个函数可以
返回一个多分类问题的朴素贝叶斯分类器。在实践中，我们最好预先将类
别标签排序，这样我们才能使用 ~fitcnb()~ 函数解决多分类问题。在此，
我们将使用花瓣的长度和宽度作为输入数据（特征向量），类别标签则有
setosa, versicolor 和 virginica。

与之前相同，我们使用如下代码加载 Iris 数据集：

#+BEGIN_QUOTE
代码
#+END_QUOTE

首先，我们从 ~meas~ 矩阵中提取第三、四列特征，即花瓣的长度和宽度。
接着，我们创建一个 ~table~ 类型的变量 ~PetalTable~ 来储存这些特征
值：

#+BEGIN_QUOTE
代码
#+END_QUOTE

训练'naba_snp'代码如下：

#+BEGIN_QUOTE
代码
#+END_QUOTE

执行上面的代码后， ~fitcnb()~ 函数将返回一个类型为
~ClassificationNaiveBayes~ 的变量 ~NaiveModelPetal~ 。这个变量有
许多的方法和属性，我们可以使用 ~.~ 操作来访问。例如，我们可以通过
如下代码来查看训练好的（已求解出参数的）贝叶斯分类器，对每个类别
所估计的高斯分布（即 $P(特征值|标签)$ ）的均值和标准差：

#+BEGIN_QUOTE
代码
#+END_QUOTE

在上面 $3 \times 2$ 的 ~cell~ 矩阵中，每个 ~cell~ 单元格
（ $2\times 1$ ~double~ 类型）都保存了其所对应的均值和方差。对应
关系为，每一行代表一类，在这里从上到下分别表示 setosa, versicolor
和 virginica；每一列代表一个特征值，在这里从左到右分别表示花瓣的
长度和花瓣的宽度。因此，为了得到贝叶斯分类器所估计的 ~versicolor~
类的，花瓣的长度概率分布函数的均值和标准差，我们可以执行如下代码
（在下面的执行结果中，第一个值是均值，第二个值是标准差）：

#+BEGIN_QUOTE
代码
#+END_QUOTE

同理，得到 ~setosa~ 类花瓣宽度的代码为：

#+BEGIN_QUOTE
代码
#+END_QUOTE

为检验训练完毕模型的拟合效果，我们可以计算模型的训练误差。训练误
差（训练误差 training error, 也在MATLAB的某些工具箱中也称为
再代入误差 resubstitution error，前者更为通用、易于理解）计算的是
训练好的模型，对于训练集中样本的分类结果的错误分类的比率。训练误
差能够告诉我们模型对训练集的拟合效果的好坏。我们可以通过如下代码
进行计算：

#+BEGIN_QUOTE
代码
#+END_QUOTE

结果显示，有 $4\%$ 的样本被错误分类了。训练误差虽然计算简单，但是
它不能告诉我们训练模型都犯了什么类型的错误。具体而言我们无法回答
以下问题：

- 这 $4\%$ 的误差，在三个类别中是均匀分布的吗？
- 如果不是均匀分布，那么这 $4\%$ 的误差是由单一类别引起的，其它类
  别全部分类正确吗？

为了更好地理解模型错分的样本，我们可以计算一种被称为'confmat_snp'
的矩阵。与训练误差类似，'confmat_snp'也仅使用样本的真实标签，和分
类器所预测的标签进行计算，但是包含更为丰富的内容。我们常常使用混
淆矩阵来评估分类器的性能，而非简单地使用训练误差。下面的表格展示
了二分类问题的'confmat_snp'（混淆矩阵及其缩写，属于机器学习领域读
者必须熟知的概念，因此不翻译英文，请读者务必熟记）：

|              | Predicted Positive | Predicted Negative  |
|--------------+--------------------+---------------------|
| Actual TRUE  | TP (True Positive) | TN                  |
| Actual FALSE | FP                 | FN (False Negative) |

其中， ~Actual TRUE~ 表示的是在实际数据集中，真实分类标签为~TRUE~
的样本（注意此处仅考虑二分类问题，分类标签仅有 ~TRUE~ 和 ~FALSE~
两种）； ~Predicted Positive~ 表示的是分类器预测结果为 ~TRUE~ 的
样本。表格中的每个值代表如下含义：

- ~TP~ 表示实际标签为 ~TRUE~ ，且分类器预测为 ~TRUE~ 的样本个数
  （即分类器能够正确分类的，样本真实标签为 ~TRUE~ 的样本个数）
- ~FN~ 表示实际标签为 ~FALSE~ ，且分类器预测为 ~FALSE~ 的样本个数
  （即分类器能够正确分类的，样本真实标签为 ~FALSE~ 的样本个数）
- ~TN~ 表示实际标签为 ~TRUE~ ，但分类器预测为 ~FALSE~ 的样本个数
  （即被分类器错误分类的，样本真实标签为 ~TRUE~ 的样本个数）
- ~FP~ 表示实际标签为 ~FALSE~ ，但分类器预测为 ~TRUE~ 的样本个数
  （即被分类器错误分类的，样本真实标签为 ~FALSE~ 的样本个数）

显然，在主对角线上的值，表示分类器能够正确分类的样本的数量。其它
的值表示被错误分类的样本数量。在MATLAB中，我们可以使用
~confusionmat()~ 函数计算混淆矩阵。在计算混淆矩阵之前，我们先要获
取之前训练的分类器 ~NaiveModelPetal~ 对每个样本预测的标签，然后再
输入函数进行计算。代码如下：

#+BEGIN_QUOTE
代码
#+END_QUOTE

如预期的一样（ $4\%$ 的训练误差），只有6个样本被错误分类了。通过
混淆矩阵我们知道它们原本属于的类是 ~versicolor~ 和 ~virginica~ 。
为理解为何这6个样本会被错分，我们可以使用以花瓣的长度和宽度为坐标
轴的二维散点图来帮助理解。为了更好地绘制图表，我们可以先使用如下
代码确定坐标轴的范围：

#+BEGIN_QUOTE
代码
#+END_QUOTE

现在我们可以绘制网格图：

#+BEGIN_QUOTE
代码
#+END_QUOTE

# zen: 画图技巧
接着我们可以使用之前训练好的分类器，对网格图中每单元网格进行预测：

#+BEGIN_QUOTE
代码
#+END_QUOTE

现在我们可以绘制预测结果的散点图：

#+BEGIN_QUOTE
代码
#+END_QUOTE

为了让图片更加直观，我们对图中添加了标题和横纵坐标轴的标签。下图
展示了分类器 ~NaiveModelPetal~ 是如何根据花瓣的长度和宽度进行分类
的：

#+BEGIN_QUOTE
图5.6：分类结果分布示意图
#+END_QUOTE

** 'dian_snp'分类



'dian_snp'是由Fisher在1936年提出的线性判别分析（Linear
Discriminant Analysis, LDA）演变而来的统计学方法。最早是使用一维
函数描述两组或多组分类样本，并将样本按照类别分类。与之前的方法相
同，判别分析同样适用于分类问题。它要求有一组预先定义好的类别标签，
以及由多个样本、每个样本的多个特征值所组成的特征值矩阵，及其对应
的标签向量（训练数据集）。判别分析也可用于判断，任意一组特征值是
否足够对训练集进行有效的分类。

在MATLAB中，'dian_snp'基于如下假设：

- 每个类别都服从多元正态分布（可以看作是混合高斯分布的一种特殊情
  况）
- 对线性判别分析，所有类别服从标准差相同的正态分布，只有均值不同
- 对二次判别分析，均值和标准差都可以不同

基于上面的假设，'dian_snp'模型的目标函数可被表示为最小化期望分类
损失：

$$Y=\text{arg}\min_{y=1,...,K}\sum_{k=1}^{K}{P(k|x)C(y,k)}$$

这里：

- $Y$ 代表对样本 $x$ 的分类标签
- $K$ 代表分类标签的个数
- $P(k|x)$ 代表样本 $x$ 属于第 $k$ 类别的'poste_snp'
- $C(y,k)$ 是损失函数，样本 $x$ 的真实标签是 $y$ 。损失函数 $C$
  衡量将样本分类到第 $k$ 类别所造成的损失

在这里我们继续以 Iris 数据集举例来学习判别分析。记得先使用以下代
码导入数据集：

#+BEGIN_QUOTE
代码
#+END_QUOTE

MATLAB提供了 ~fitcdiscr()~ 函数返回一个训练完毕的判别分析模型。这
个模型使用高斯分布对每个类别进行估计。下面的代码将使用整个数据集
训练判别模型：

#+BEGIN_QUOTE
代码
#+END_QUOTE

与之前相同，我们可以使用 ~.~ 运算来访问成员方法和属性。注意上面代
码返回的输出中的倒数第二行 ~Mu~ 变量，它代表了每个特征值对应每个
分类的高斯分布的均值。我们可以使用以下代码获取这些数据：

#+BEGIN_QUOTE
代码
#+END_QUOTE

其中，每一行代表一个类别，从上往下依次代表setosa, versicolor 和
virginica。每一列代表一个特征值，从左到右依次代表萼片的长宽、花瓣
的长宽。

下面我们来研究下 ~DiscrModel~ 的 ~Coeffs~ 属性：

#+BEGIN_QUOTE
代码
#+END_QUOTE

这个属性返回大小为 $n \times n$ 的结构体矩阵，在我们的例子中，因
为有 $3$ 个类别，所以 $n=3$ 。每个结构体数组都包含着界定两类线性
分类边界的系数。为何我们要讨论线性边界呢？因为'dian_snp'将 $n$ 维
空间分为多个区域，每个区域属于一个类别。这些线性边界正是这多个区
域的分界线。当使用训练好的判别分析模型进行预测时，我们输入未经标
注的样本的特征向量，并观察这个特征向量处于 $n$ 维空间的哪个区域，
并使用这个区域所属的类别标签对其分类。

因此 ~Coeffs(i,j)~ 代表的是第 $i$ 类和第 $j$ 类之间的线性分类边界。
这个边界可表示为线性方程：

$$\text{Const} + \text{Linear} \times x = 0$$

其中， $x$ 表示输入样本的特征向量。为便于可视化，与之前相同，接下
来我们只使用两个特征，花瓣的长度和宽度训练模型，这样我们才能将任
意结果展示在二维图表中：

#+BEGIN_QUOTE
代码
#+END_QUOTE

仅使用花瓣的特征值进行训练：

#+BEGIN_QUOTE
代码
#+END_QUOTE

绘制训练集中特征矩阵的散点图，并使用标签向量对每个样本进行标注：

#+BEGIN_QUOTE
代码
#+END_QUOTE

获取类别 ~setosa~ 和 ~versicolor~ 之间线性边界的代码（标签 $1$
$2$ 按顺序与之对应）：

#+BEGIN_QUOTE
代码
#+END_QUOTE

在图上绘制出两个类别间的线性边界：

#+BEGIN_QUOTE
代码
#+END_QUOTE

同理，获取类别 ~versicolor~ 和 ~virginica~ 之间线性边界的代码（标
签 $2$ $3$ 按顺序与之对应）：

#+BEGIN_QUOTE
代码
#+END_QUOTE

绘制两类之间的分类边界：

#+BEGIN_QUOTE
代码
#+END_QUOTE

最后在图上绘制坐标轴标签和图表标题：

#+BEGIN_QUOTE
代码
#+END_QUOTE

下图显示了Iris数据集的散点图，并且绘制了'dian_snp'训练后得到的不
同类别间的线性分类边界：

#+BEGIN_QUOTE
图5.7 添加了不同类别间的线性分类边界的Iris数据集散点图
#+END_QUOTE

现在我们使用训练好的模型，对三个新的花朵样本进行分类。如图5.8所示，
下面三个点落在了三个分类区域中：

- $P1$ ：花瓣长度为 $2cm$ ；花瓣宽度为 $0.5cm$
- $P2$ ：花瓣长度为 $5cm$ ；花瓣宽度为 $1.5cm$
- $P3$ ：花瓣长度为 $6cm$ ；花瓣宽度为 $2cm$

首先我们对上面的三个样本构建训练数据（特征向量）：

#+BEGIN_QUOTE
代码
#+END_QUOTE

为了使用训练好的模型对新样本进行预测，我们使用 ~predict()~ 函数。
这个函数将返回与输入的特征向量顺序一一对应的，模型预测的标签向量
结果：

#+BEGIN_QUOTE
代码
#+END_QUOTE

现在我们在之前的散点图中画出新添加的这三个点：

#+BEGIN_QUOTE
代码
#+END_QUOTE

图5.8是在图5.7中添加了三个新样本之后的结果。这幅图可以允许我们通
过观察新样本所属于的区域的标签，来验证 ~predict()~ 函数分类的正确
性：

#+BEGIN_QUOTE
图5.8 添加了不同类别间的线性分类边界的Iris数据集散点图，并添加3个
新样本
#+END_QUOTE

从图5.8中可以看出，除了少部分点落在 ~versicolor~ 和 ~virginica~
之间的分类边界上外，判别分析模型的分类效果还是不错的。我们可以通
过使用更高次的模型对数据集进行拟合，以求达到更好的分类效果。为达
到此目的我们可以将模型中 ~DiscrimType~ 键值对设置为
~pseudoLinear~ 或者 ~pseudoQuadratic~ 。

为检测模型效果，我们可以使用以下代码计算训练误差：

#+BEGIN_QUOTE
代码
#+END_QUOTE

$2\%$ 的训练误差表明模型对Iris数据集具有很好的拟合效果。
与'naba_snp'部分相同，为理解错分样本的分布状况，我们可以计
算'confmat_snp'。同样，在计算混淆矩阵前我们首先需要得到模型对训练
样本的预测结果：

#+BEGIN_QUOTE
代码
#+END_QUOTE

与预期相同，我们只在 ~versicolor~ 和 ~virginica~ 分类中有3个错分
样本。我们可以通过绘制下图观察是哪三个样本被错分了：

#+BEGIN_QUOTE
代码
#+END_QUOTE

这里我们解释下上面的代码。首先我们使用 ~strcmp()~ 函数对模型预测
结果和训练集中真实的标签向量进行比较，如果两个字符串相同，函数返
回逻辑值 $1$ ，否则返回 $0$ 。比较的结果保存在向量 ~Err~ 中，它接
下来会被作为一个 ~logical mask~ 向量（MATLAB术语，作用是过滤
掉~logical mask~ 向量中值不为 $1$ 的元素。这是MATLAB中非常强大且
常用的技巧，建议读者自行学习）。接着，我们以花瓣的长宽为坐标轴，
绘制Iris数据集的二维散点图。最后，我们将错分样本标注在散点图上。

下图显示了标注有错分样本的散点图：

#+BEGIN_QUOTE
图5.9 标注有错分样本的散点图
#+END_QUOTE

正如预期的，错分样本是落在 ~versicolor~ 和 ~virginica~ 分类边界上
的点。

** 'knn_snp'

分类问题的一个重要任务就是证明不同类别的样本的特征向量之间存在显
著差异，这是分类模型起作用的基本条件。当我们使用训练集训练完分类
模型后，我们可以使用另外一组经过标注，但模型训练过程中没用到过的
数据，输入模型得到预测结果，来验证模型的泛化性能（在未知数据集上
模型的预测能力），这样的数据集根据使用的阶段、目标不同，分别被称
为'vs_snp'和'tes_snp'。之前训练好的模型的泛化性能就可以通过观察对
这些样本的预测结果进行检验。

'knn_snp'是众多分类算法中的一种，它基于样本间的距离（距离的定义有
很多种，是多种衍生算法的核心区别），将待分类样本赋值到距离它最近
的 $k$ 个邻近样本中，最多样本所属的分类。这也是本章中我们介绍的唯
一不需要训练模型的分类方法，它直接使用训练数据集就可以完成分类任
务。经典'knn_snp'使用欧氏距离衡量样本间的距离，欧式距离的定义是：

$$D=\sqrt{{\sum_i{(x_i-y_i)^2}}}$$

在二维空间中，两点之间最短的欧式距离是链接两点的直线。这个距离是
按照上面的公式，使用两个向量之差的平方开根号计算的（公式中使用的
是代数表示而非矩阵表示）。

一个样本将被归类为其周围 $K$ 个近邻样本中，大多数样本所属的类别。
其中 $K$ 是算法的可选参数，表示每次对新加入样本分类时，考虑 $k$
个距离其最近的样本。如果 $K=1$ ，那么新样本被分类为其最近样本所属
的分类。当然这并不是最好的参数设置，因为只基于最近样本分类必将导
致非常高的分类误差。

因此，我们通常考虑 $2$ 到 $10$ 个最近的邻居并使用其中多数样本所属
的分类。 $K$ 值的选取通常基于人们的先验知识，包括对数据集的预先观
察。总的来说，相对较大的 $K$ 值通常会得到更少的噪声、更好的结果，
但是对不同数据集效果并不相同。多数情况下，我们选取奇数作为 $K$ 值
以尽可能避免有相同多个最大样本数的分类的情况；尽管这样仍然可能出
现多个候选分类，这时我们可以通过具体衡量每个样本到新样本的距离来
进行选择。

'knn_snp'算法的一大优势是不需要训练，能够直接对新加入样本进行分类。
另外，它能够对非线性分类（类别之间不存在线性分类边界）问题进行分
类。KNN具有非常好的鲁棒性，数据集中少量的噪声很难引起分类结果的变
化。

KNN最大的缺点是需要保存全部数据集。对于大数据应用而言，KNN算法非
常耗费内存。此外一个显著限制是，经典算法有非常大的计算量——为计算K
邻近样本，需要计算新加入样本与数据集中每个样本的欧式距离。然而这
个问题在其衍生算法的程序化实现中，可以通过借助一些特殊设计的数据
结构（如树结构）解决。另外一个缺陷就是，尽管在大数据中难以应用，
KNN算法仍需要大量标注好的数据才能达到令人满意的精度。

在MATLAB中，KNN分类器可以使用 ~fitcknn()~ 函数构建。接下来我们仍
使用 Iris 数据集学习KNN。首先导入数据集：

#+BEGIN_QUOTE
代码
#+END_QUOTE

KNN，我们需要设置参数 $K$ 。在这里我们选取距离新样本
最近的 $3$ 个样本：

#+BEGIN_QUOTE
代码
#+END_QUOTE

上面的代码中，我们令 ~fitcknn()~ 函数返回一个 ~ClassificationKNN~
类型的变量 ~KnnModel~ 。这个类型中，控制距离算法的属性 ~Distance~
和代表参数 $K$ 的属性 ~NumNeighbors~ 都是可以随时更改的。

#+BEGIN_QUOTE
小贴士：我们随时可以双击'workspace_snp'中的 ~KnnModel~ 变量来查看
其属性值。
#+END_QUOTE

在'workspace_snp'中双击 ~KnnModel~ 将打开'variables_window_snp'，
其中将显示一长串的变量的全部属性。我们可以通过双击任一属性来查看
它的值。下图显示了'variables_window_snp'：

#+BEGIN_QUOTE
图5.10: 'variables_window_snp'
#+END_QUOTE

与之前相同，我们仍可以使用 ~.~ 运算获取属性值。例如我们可以使用如
下代码获取分类标签的名称：

#+BEGIN_QUOTE
代码
#+END_QUOTE

为检验模型分类表现，我们可以使用如下代码计算训练误差（对于KNN算法
而言没有训练过程，此处直接将算法应用到训练集上预测每一样本的分类
结果，从而得到训练误差）：

#+BEGIN_QUOTE
代码
#+END_QUOTE

结果显示KNN错分了 $4\%$ 的样本。与之前相同，我们仍可以先获得KNN对
训练集样本的预测，再计算'confmat_snp'来更好地分析模型错分的原因。
代码如下：

#+BEGIN_QUOTE
代码
#+END_QUOTE

如训练误差所示，总共有 $7$ 个样本被错分了，并且我们看到与之前几个
分类算法相同，它们仍属来自 ~versicolor~ 类和 ~virginica~ 类。之前
我们解释过，训练误差过于简单，无法使我们深入理解模型错分的原因。
而'confmat_snp'可以使我们更加直观、详细地了解模型犯错误的类型及原
因。但注意，我们之前强调过，无论是训练误差，还是混淆矩阵，都只能
衡量模型对训练集拟合程度的好坏，不代表任何对模型泛化能力相关的衡
量。

#+BEGIN_QUOTE
'zyr_snp' 作者写作过于随性。原文中介绍'cv_snp'的下面两段被挪到上
一节('dian_snp')中。上节中已经使用过'cv_snp'，放在这节才重
复'cv_snp'的概念不合常理。
#+END_QUOTE

与'dian_snp'小节相同，接下来我们仍将使用'cv_snp'来检验KNN的泛化能
力：

#+BEGIN_QUOTE
代码
#+END_QUOTE

上面的代码将返回一个 ~ClassificationPartitionedModel~ 类型的变量。

现在，我们可以查看交叉验证所得出的， $K$ 折平均模型预测误差了：

#+BEGIN_QUOTE
代码
#+END_QUOTE

上面的交叉验证结果与我们直接使用训练集得到的训练误差非常近似（绝
大多数情况并非如此，因为KNN本身没有训练即求解参数过程，训练误差的
计算相当于1折交叉验证，因此才会出现这种特例）。因此，我们可以假设
即使对新的样本模型仍有接近于 $96\%$ 精确度的预测能力。

之前我们提到过，参数 $K$ 的选择将决定模型表现。现在我们更改 $K$
的取值来验证这点。正如之前所说，我们可以通过修改 ~NumNeighbors~
属性达到这个目标。这里我们将其设置为 $5$ ：

#+BEGIN_QUOTE
代码
#+END_QUOTE

执行上述代码后，我们可以执行以下命令来查看新修改的模型的训练误差，
并和之前的模型进行比较：

#+BEGIN_QUOTE
代码
#+END_QUOTE

同样，我们也可以使用交叉验证对其泛化能力进行检验：

#+BEGIN_QUOTE
代码
#+END_QUOTE

可以看出，当 $K=5$ 时，对于Iris数据集而言，KNN算法具有更好的表现。
我们可以通过计算混淆矩阵进一步验证：

#+BEGIN_QUOTE
代码
#+END_QUOTE

通过修改参数 $K$ ，我们降低了错分样本数量，这次只有5个样本被错分
了。

在开头我们提到，对KNN算法除了可以设定参数 $K$ 之外，我们还可以更
改距离的衡量指标。除了欧式距离，是否其它距离能够进一步优化模型效
果呢？

正如之前提到的，我们可以通过修改 ~Distance~ 属性实现这点。这个属
性可以接收既定的字符串参数（距离名称），也可以接收用户自定义的距
离的函数句柄。这里我们使用 ~cosine~ 距离，并且让算法将待分类样本
同整个数据集进行比较（设置 ~'NSMethod'~ 属性为 ~'exhaustive'~ ）：

#+BEGIN_QUOTE
代码
#+END_QUOTE

计算训练误差：

#+BEGIN_QUOTE
代码
#+END_QUOTE

现在我们可以进一步计算混淆矩阵：

#+BEGIN_QUOTE
代码
#+END_QUOTE

我们看到，通过使用 ~cosine~ 距离，只有3个样本被错分了。


** MATLAB分类器APP

在之前的小节中，我们学习了一些MATLAB封装好的分类模型函数。为了理
解不同算法的区别我们以Iris数据集为例进行了多次试验。现在我们已经
充分理解了这些模型的概念，我们可以抛开代码，直接使用MATLAB封装好
的可视化APP Classification Learner APP 来完成以上任务。

这个APP可以让我们可视化、交互式地完成与之前完全相同的分类任务。它
可以让我们及其简单地、自动化地完成包括（上文在举例函数代码时没有
使用到的参数设置）特征选择、交叉验证参数设置、模型训练等任务。它
提供的分类模型包括：'detree_snp'，'dian_snp'，支持向量机（Support
Vector Machines, SVM），'lore_snp'，'knn_snp'，以及集成分类
（ensemble classification）。

Classification Learner APP 提供的都是监督学习类分类算法，从带有标
注的数据集中学习、优化模型参数、训练模型。并使用训练后的模型对新
加入的样本进行预测。训练好的模型可以导入到'workspace_snp'中，也可
以根据模型训练结果自动生成相应的MATLAB代码以便以后重复使用。

下面我们开始学习Classification Learner APP，首先我们导入Iris数据
集：

#+BEGIN_QUOTE
代码
#+END_QUOTE 

在调用APP之前，我们先创建一个 ~table~ 类型的变量来保存相关数据：

#+BEGIN_QUOTE
代码
#+END_QUOTE

现在数据已经在'workspace_snp'中可见了，我们可以开始使用APP完成分
类工作。在工具栏中点击 ~APPS~ 选项卡，并点击 ~Classification
Learner~ 图标，APP就会自动打开，如下图所示：

#+BEGIN_QUOTE
图5.11：Classification Learner APP
#+END_QUOTE

为向APP中导入'workspace_snp'中存在的数据，在 ~File~ 部分点击 ~New
Session~ 按钮，将会打开一个 ~New Session~ 对话框。它包含三块内容
（如图5.12）：

- 第一步：选择一个 ~table~ 或 ~mat~ 类型的变量。这里我们选择训练
  集
- 第二步：选择特征值（APP中称为predictors，预测变量）和标签向量
  （APP中称为response，响应值）。这里我们可以设置变量及其类型
- 第三步：定义'cv_snp'参数。这里我们可以设置交叉验证选用的方法

#+BEGIN_QUOTE
小贴士：'cv_snp'允许精确、鲁棒地衡量训练模型的泛化能力。这个工具
能够帮助我们选择有最好泛化能力的模型设置
#+END_QUOTE

下图中展示了 ~New Session~ 对话框及其三部分：

#+BEGIN_QUOTE
图5.12：Classification Learner APP 中的 ~New Session~ 对话框
#+END_QUOTE

在图5.12中，第一步我们选择了之前生成的训练集数据 ~Irisable~ 。选
定之后，第二步中就会显示 ~table~ 类型的变量中所保存的各种变量名及
其值。此外，APP会自动尝试将变量分成特征值（这里称为predictor，预
测值）和分类标签（这里称为response，响应值）。如果必要的话，我们
随时可以APP的自动分类结果进行更改。当修改完交叉验证参数后，我们可
以点击 ~Start Session~ 按钮完成数据导入。

#+BEGIN_QUOTE
小贴士：在'cv_snp'中，我们可以设置 $K$ 折（数据集被等分为 $K$ 份，
且重复验证 $K$ 次）参数。对 *Holdout Validation* （即1折交叉验证，
数据集被简单地拆分为训练集和测试集），我们可以选择拆分的比例。最
后我们可以选择 *No Validation* 选项即不适用任何验证方法，但是这样
非常容易导致'ovfi_snp'。
#+END_QUOTE

现在我们可以使用监督学习在数据集上训练模型。APP将使用训练数据集中
标注好的数据求解模型参数，以建立从特征矩阵（APP中称为predictors，
多个预测值）到分类标签（APP中称为response，响应值）的映射关系。

在 ~Model Type~ 部分中你将发现有如下模型可以选择：

- ~Decision Trees~ 'detree_snp'
- ~Discriminant Analysis~ 'dian_snp'
- ~Logistic Regression~ 'lore_snp'
- ~Support Vector Machines~ 'svm_snp'
- ~Nearest Neighbor Classifiers~ 'knn_snp'
- ~Ensemble Classifiers~ 集成分类

为简单的话我们可以使用 ~All Quick-To-Train~ 选项，点击图5.11中的
~Train~ 按钮直接使用全部算法进行训练。当全部算法训练完毕后，拥有
最佳表现的模型会在对话框中被高亮显示。下图显示了这种操作的训练结
果：

#+BEGIN_QUOTE
图5.13：全部模型训练结果图
#+END_QUOTE

为了理解表现优秀的模型都拥有哪些改进，我们可以直接对比表现最差的
和最好的模型。在 ~History~ 部分中，我们可以看到表现最差的模型是
~Coarse KNN~ ，只有 $64\%$ 的准确率；表现最好的 ~Medium KNN~ 模型
则有 $96.7\%$ 的准确率。

查看分类误差非常简单，直接在对话窗口中双击要查看的模型就可以弹出
散点图。通过观察下图我们非常容易理解为何 ~Medium KNN~ 比另一个模
型效果好出许多，因为第二个散点图中有更多的叉状散点（代表错分样本）：

#+BEGIN_QUOTE
图5.14：最优、最劣模型的散点图比较
#+END_QUOTE

最后，在 ~Classification Learner~ 选项卡的 ~Export~ 部分（工具条
的右侧），有三个选项可供选择：

- *Export Model* （导出模型）：这个选项将训练好的模型以一个 ~struct~ 类型导出到
  'workspace_snp'，同时将导出训练数据
- *Export Compact Model* （导出紧凑模型）：这个选项只导出模型，不
  包括训练数据
- *Generate MATLAB Code* （生成MATLAB代码）：这个选项将导出APP后
  台使用的，训练选中模型的全部代码。这些代码可以用于以后基于新的
  数据集训练新的模型


** 总结

在本章中，我们学习了如何使用MATLAB提供的各种函数、APP完成分类任务。
首先我们学习了'detree_snp'，理解
了'node_snp'、'leaf_snp'和'branch_snp'等概念。并且我们完整地重现
了决策树是如何一步步将样本归类到每个节点的子分支及其子节点的。之
后我们学习了如何使用决策树对新样本进行分类。

接着我们研究了概率分类模型。这类模型基于概率学原理，给出每个样本
属于每个类别的概率是多少。我们学习了概率学的基础概念：频率学派的
概率定义、贝叶斯学派的概率定义、独立和非独立事件、联合概率分布和
条件概率分布。接着我们学习了如何使用'naba_snp'进行分类。

我们也介绍了'dian_snp'方法。我们举了几个例子来比较不同设置的优劣。
我们同时也学习到如何创建模型以最小化期望错分率。并且我们了解了检
验模型训练误差和计算'confmat_snp'的方法。

在下一节中我们学习了'knn_snp'，我们展示了如何根据距离衡量指标对样
本进行分类。通过实验我们发现调整参数 $K$ 以及距离衡量指标能够对模
型分类性能进行改进。并且我们通过'cv_snp'展示了此点。

最后我们展示了 Classification Learner APP，以及使用这个APP构建分
类模型的分步操作。现在，从导入、查看数据集，到特征选择、交叉验证
参数设置、训练模型、评估模型，都变得非常简单。

在下章中，我们讲学习多种聚类方法，以及实践中如何根据实际情况挑选
不同的聚类方法。我们将了解聚类方法的基本概念例如相似度指标，并学
习如何预处理聚类方法所需的数据集。我们还将讨论'kmean_snp'、聚类
树、'depl_snp'等模型。













