#+LATEX_HEADER: \usepackage{ctex}
#+LATEX_COMPILER: xelatex

* 第六章：无监督学习

'cluster_snp'方法能够从未经标注的数据集中自动发现隐藏的模式、分组
方法。与之前章节中介绍过的监督学习方法需要从标注过的数据集中获得
信息不同，聚类算法能够通过衡量样本间的'sim_snp'，从而在未经标注的
数据集中学习识别聚类中心、分组方法。

这类算法的目标函数一般是最小化'intradist_snp'的同时最大
化'interdist_snp'。其中，样本、组类间的距离将使用'sim_snp'或
者'dissim_snp'等指标来衡量。

与其它多元统计模型不同，此类聚类算法不需要预先对模型、分类的拓扑
结构等作出任何假设（如'naba_snp'中假设'CI_snp'）。这样聚类算法能
够避免之前算法，因包含先验知识、预先假设，将样本按照先验知识的偏
误而错分的可能性。聚类算法具有发现数据集中我们确定仅仅存在，但还
不能描述的关系、结构的功能。因此，聚类算法实际上是'induction_snp'
的结果，是完全基于实证分析得出的结论。

本章将讲述如何将数据集中的样本归集到聚类中心（cluster），或者如何
对相似的样本进行分组。你将学会'kmean_snp'和'kmed_snp'。我们还将介
绍'hiclu_snp'的相关内容。

我们将学习如下模型：

- 'hiclu_snp'
- 'kmean_snp'
- 'kmed_snp'
- 'gmm_snp'
- 'dendrogram_snp'

在本章结尾，我们将理解聚类算法的基本概念、相似度的衡量指标、如何
进行数据预处理、这些算法聚类的整个流程。并且将学会如何对不同特点
的数据集选用不同算法。

** 聚类分析简介

在分类问题中，我们的任务仅仅是分类，即把样本赋值类别标签。在聚类
分析中，我们不止想要给样本进行一次分类，我们还希望理解每个单独的
类别中，更细致一层的子分类。聚类算法的最终结果往往是给出整个数据
集层次化的分类结构。

分类算法往往通过标注过的数据集学习分类规则。聚类算法中的样本没有
分类标签，我们通过定义距离指标，使用样本在距离空间中的分布情况来
归纳聚类规则。

样本集中的区域被归结为聚类中心（cluster）。如果我们能观察到一个区
域中的样本紧凑地围绕在一个聚类中心周围，并且远离另一聚类中心的样
本，我们则可以假设两个聚类中心的样本满足不同的分类条件。在这种情
况下，我们可以继续研究两个问题：

- 如何衡量样本的相似度
- 如何定义分组方法

样本距离的定义、分组方法是聚类算法的两个基本要素。


*** 相似度与离散度指标

'cluster_snp'涉及到识别数据集中的分组情况。然而只有预先定义好样本
间的'prodis_snp'，我们才能实现这个目标。'prodis_snp'的定义是指样
本间的'sim_snp'或者'dissim_snp'。当邻近测度定义好后，我们就可以对
什么是“一组数据”进行定义。在很多情况下，这个测度涉及到高维空间中
的距离，而聚类结果的好坏很大程度上取决于我们衡量这个距离所采用的
指标。聚类算法基于样本间的距离对样本进行分组，两个样本是否属于同
一聚类中心取决于它们与这个聚类中心的距离。因此，如果一些样本距离
某一聚类中心比其他聚类中心近，我们则说它们同属这一聚类中心。

那么什么是'sim_snp'、'dissim_snp'呢？'sim_snp'指的是数值化的，衡
量两个样本相似程度的指标。因此，相似的两个样本具有高相似度。相似
度的取值范围通常是0（完全不相似）到1（完全相等）。

相反的，'dissim_snp'指的是衡量两个样本间，不相似程度的指标。两个
样本越不相似，离散度越高。一般情况下，我们所说的“距离”指的就是样
本的离散度。与相似度类似，离散度也可以在0到1的范围内取值，但更常
见的是在0到正无穷的范围内取值。

'dissim_snp'可以使用距离指标衡量。距离就是一种满足某种性质的离散
度。例如，两样本 $x$ $y$ 间的欧式距离 $d$ 可以定义如下：

$$d(x,y)=\sqrt{{\sum_i{(x_i-y_i)^2}}}$$

#+BEGIN_QUOTE
小贴士：记得在二维空间中，欧式距离是两点间的最短距离？欧式距离使
用两个特征向量的特征值之差的平方根进行定义。
#+END_QUOTE

除欧式距离外，距离还有非常多的衡量方式，例如欧式距离实际上是
*Minkowski* 距离的一种特例：

$$d(x,y)=\bigg(\sum_i^{}{|x_i-y_i|^r}\bigg)^{\frac{1}{r}}$$

在上面的公式中， $r$ 是一个可选参数。例如，当 $r=1$ 时，我们称之
为 *Manhattan* 距离。它的公式是：

$$d(x,y)=\sum_i^{}{|x_i-y_i|}$$

此外，在介绍'knn_snp'的章节中我们还使用过余弦（cosine）距离，它衡
量从原点出发，两个向量间的余弦大小。余弦距离的定义是：

$$d(x,y)=\frac{\sum x_i y_i}{\sum x_i^2 \sum y_i^2}$$

一旦我们有了距离定义，我们可以开始对样本进行聚类了。目前为止，我
们学习的距离都是使用数值方法进行计算的，我们是否可以使用名义上的
方法衡量离散度呢？

对于'nominal_var_snp'，例如字符串，同样存在多种距离衡量方式。例如，
我们可以衡量两个字符串中有多少相同字母出现在同一位置。下图举例了
两个距离为4的字符串，因为他们有4个不同字母出现在相同位置：

#+BEGIN_QUOTE
图6.1：两个字符串间的距离
#+END_QUOTE

另一个例子是衡量两个字符串间的编辑距离（Edit Distance），即最少需
要多少次如下操作才能将一个字符串变成另一个字符串：

- 加入一个字符
- 删除一个字符
- 更改一个字符


** 聚类方法类型简介

一旦我们选定距离公式，我们需要进一步定义样本是如何被分类的。主要
有以下两种分类方法：

- 'hiclu_snp'使用层次化的形式对数据集进行描述。不同的样本被归类到
  聚类树的各个层级、分支下，与生物学中所使用的聚类树相同
- 'paclu_snp'将数据空间划分为不同区域。数据空间被分为各个区域及子
  区域，且不同的区域、子区域间不相互重叠


*** 层次聚类

在层次聚类中，聚类中心是按照从上到下或从下到上的顺序，递归地被划
分的。我们可以将聚类方法分为两类：

- 从下至上聚类：在初始迭代中，最相近的样本被分到同一聚类中心；每
  次迭代，都按照某一阈值，将相似的聚类中心继续合并；算法在所有聚
  类中心被合并为一个聚类中心时停止
- 从上至下聚类：在初始迭代中，所有样本被分到一个聚类中心；每次迭
  代，都按照样本的离散程度将聚类中心拆分为多个子聚类中心

这两种算法都会得到一个分层嵌套表示的聚类树图。用户可以根据想要的
相似度水平，选择某一层级的聚类方式作为聚类结果。聚类中心的分裂、
合并，是按照预先设定的某一准则以及相似度的衡量指标来进行计算的。

下图显示了一个层次聚类树图：

#+BEGIN_QUOTE
图6.2：层次聚类图示例
#+END_QUOTE


*** 原型聚类

'paclu_snp'将数据集划分到多个离散的聚类中心。给定一个数据集，原型
聚类法将划分多个区域，每个区域代表一个聚类中心。这类方法从一个初
始划分情形出发，在每次迭代中都将变更聚类中心的位置，以及每个样本
所属的聚类中心。这种方法一般需要用户预先设定好聚类中心的数量。为
了达到用户指定的划分数量，这种方法通常需要迭代非常多次才能收敛。
下图显示了一个原型聚类结果图：

#+BEGIN_QUOTE
图6.3：原型聚类图示例
#+END_QUOTE

在'paclu_snp'中，我们通常先随机初始化一种划分方法，接着在后续的迭
代过程中不断优化划分方法，来满足用户预先定义的目标函数。在这种方
法中，我们往往最小化每个聚类内部的距离，同时最大化聚类中心之间的
距离。这可以通过在每次迭代中，将各个样本归属到与上次迭代不同的聚
类中心实现。


** 层次聚类算法

在MATLAB中，'hiclu_snp'通过对样本逐层分组，来生成一个聚类树状图。
其中上层的聚类中心是由下层的多个子聚类中心合并而来的。每一次合并，
都是根据用户预先定义的准则，将最相似的聚类中心合并。在'smltb_snp'
中提供了'hiclu_snp'所需要的全部函数。通多使用 ~pdist()~ 、
~linkage~ 以及 ~cluster~ 函数， ~clusterdata()~ 函数能够使用从下
至上的方法对数据集进行聚类。聚类的结果可以使用聚类树状图显示出来。

如上所述，整个聚类过程需要用到多个函数。其中 ~clusterdata()~ 是主
函数，用来调用其它函数执行结果。

接下来我们逐步解析每个函数的输入、输出，来学习MATLAB中如何进行层
次聚类：

- ~pdist()~ 如上所述，聚类分析的基础是相似度或离散度衡量指标。因
  此开始聚类的第一步就是对相似度进行定义。 ~pdist()~ 函数就提供了
  多种用于衡量样本对间距离的指标
- ~linkage()~ 当距离指标定义完毕后，我们需要将样本分组到层次聚类
  二叉树中。 ~linkage()~ 函数提供了将近邻样本聚类到同一聚类中心的
  方法。每次迭代中，此函数将使用上次迭代所计算的距离数据，对子聚
  类中心进行合并，以生成包含样本更多的父聚类中心。这就是我们之前
  所说的从下至上聚类算法
- ~cluster()~ 在得到聚类树后，我们需要决定使用哪一层的分组结果，
  作为数据集的聚类结果。 ~cluster()~ 函数以聚类树为输入数据，可以
  根据用户指定的算法、阈值，来计算满足某一相似度阈值下，数据集的
  聚类情况。此函数可以直接使用聚类树中某一层次的聚类结果，也可以
  根据用户定义的算法、阈值，在任意相似度下计算聚类结果

接下来我们通过一个简单的例子来学习这些函数。


*** 层次聚类中的相似度指标

之前我们提到过，聚类分析通过衡量样本间的'prodis_snp'，能够对数据
集中的样本进行自动分组（聚类）。接下来我们展示MATLAB中的代码实现。

首先，我们使用 ~pdist()~ 函数计算数据集中所有样本两两间的距离。对于
一个有 $n$ 个样本的数据集而言，一共存在 $\frac{n*(n-1)}{2}$ 组距
离。计算结果可被称为距离矩阵或离散度矩阵，下图展示了一个计算结果：

#+BEGIN_QUOTE
图6.4：距离矩阵
#+END_QUOTE

~pdist()~ 函数默认计算样本间的欧氏距离。与其它函数相同，这个函数
也提供了许多其他距离指标。可选的指标有： ~euclidean~ 、
~squaredeuclidean~ 、 ~seuclidean~ 、 ~cityblock~ 、 ~minkowski~
、 ~chebychev~ 、 ~mahalanobis~ 、 ~cosine~ 、 ~correlation~ 、
~spearman~ 、 ~hamming~ 、 ~jaccard~ 。此外，用户依然可以使用自定
义的距离函数。

接下来我们学习一个例子。现在我们在二维平面中定义6个点如下：

1. $A = (100,100)$
2. $B = (90,90)$
3. $C = (10,10)$
4. $D = (10,20)$
5. $E = (90,70)$
6. $F = (50,50)$

这六个点展示在下图中：

#+BEGIN_QUOTE
图6.5：二维平面中的六个点
#+END_QUOTE

下面的命令可以在MATLAB中定义包含这六个点坐标的向量：

#+BEGIN_QUOTE
代码
#+END_QUOTE

现在我们使用 ~pdist()~ 函数来计算每个样本点间的距离：

#+BEGIN_QUOTE
代码
#+END_QUOTE

我们可以看到， ~pdist()~ 按照 $AB$ 、 $AC$ 、$AD$ 的顺序来计算全
部距离，然而之前我们说 ~pdist()~ 可以返回一个矩阵，上面返回的却是
向量的形式，这样的形式不利于我们查看。

为了更方便地查看样本点间的距离，我们可以使用 ~squareform()~ 函数
对 ~DistanceCalc~ 向量进行重构。在新生成的矩阵中，点 $(i,j)$ 表示
的就是从点 $i$ 到点 $j$ 之间的距离。例如，矩阵中的 $(2,3)$ 元素表
示的就是从点 $B$ 到点 $C$ 的距离。

#+BEGIN_QUOTE
代码
#+END_QUOTE

我们可以看到矩阵是对称的，因为两点间的距离跟顺序无关。另外很多情
况下计算距离之前，我们需要对距离矩阵进行正则化（样本点所包含的特
征值的单位不相同）， ~zscore()~ 这个函数可以帮你应对这种情形。它
将把特征值矩阵映射到 ~(0,1)~ 空间。


*** 定义层次聚类中的簇

如本章开头所说，'prodis_snp'和组的定义是聚类算法中两个最重要的定
义。上一节中我们讲述了如何计算样本间距，接下来我们开始定义样本是
如何被归为一个聚类中心的。为了实现这点，我们先要使用 ~linkage()~
函数进行计算。基于 ~pdist()~ 函数的计算结果，我们使用二元聚类中心，
首先把距离子聚类中心最近的样本，与子聚类中心一起归并成父聚类中心，
以此类推，从下至上，直至所有样本都被归到最顶层的聚类中心。

现在我们使用上节中的计算结果计算聚类中心：

#+BEGIN_QUOTE
代码
#+END_QUOTE

仅仅通过一行代码，我们已经完成了上段冗长的文字所描述的任务。为了
理解这个函数及其结果，我们来仔细学习下 ~GroupMatrix~ 矩阵。

这个矩阵中，每一行代表一个聚类中心。需要特别指出的是，在层次聚类
算法中，每个样本就是最底层的聚类中心。因此在 ~GroupMatrix~ 中，聚
类中心的 ~ID~ 是 $样本数+行数$ 求和得到的。例如，第一行的聚类中心
的 ~ID~ 其实是 $6+1=7$ 。矩阵中，前两列代表组成当前行聚类中心的两
个子聚类中心的 ~ID~ 。如第一行中 ~3.0000 4.0000~ 指的是第 ~7~ 聚
类中心是由两个子聚类中心 ~3~ 和 ~4~ 组成的，即样本点 ~C~ 和 ~D~
。第三列代表两个子聚类中心的距离，这个值与 ~DistanceCalc~ 矩阵中
两样本点间的距离是一致的。矩阵 ~GroupsMatrix~ 从第一行到最后一行
的聚类结果，就是聚类算法从下至上，每次选择最近的样本，将更多的样
本与子聚类中心合并为更大的父聚类中心的过程。

有了上面的背景知识，我们接下来对照样本点的二维平面图，来一一解读
~GroupsMatrix~ 中每一行的含义。如上所述，数据集中已经有6个样本点，
因此已经有最底层的， ~ID~ 从 $1$ 至 $6$ 的6个聚类中心，即 ~A~ 到
~F~ 。矩阵的第一行表示的是 ~ID~ 为 $7$ （ $7=6+1$ ）的聚类中心，
其构成是两个子聚类中心 ~3~ 和 ~4~ 组成的，即样本点 ~C~ 和 ~D~ 。
同理，第 $8$ 个聚类中心，即矩阵的第二行所表示的聚类中心，是由第
$1$ 和第 $2$ 个聚类中心，即点 ~A~ 和点 ~B~ 构成的。这两个结果显示
在下图中：

#+BEGIN_QUOTE
图6.6：矩阵中最开始的两行，即第七、第八个聚类中心
#+END_QUOTE

矩阵的第三行通过将样本点 $E$ ，即 ~ID~ 为 $5$ 的聚类中心，归并到
聚类中心 $8$ 中，形成了父聚类中心 $9$ 。接着又将样本点 $F$ 归并到
聚类中心 $9$ 中形成聚类中心 $10$ 。第 $11$ 个聚类中心将子聚类中心
$10$ 和 $7$ 连在一起，至此所有样本点都被归并到同一聚类中心，即聚
类中心 $11$ 。至此算法结束。

#+BEGIN_QUOTE
图6.7：层次聚类算法演示图
#+END_QUOTE

值得指出的细节是， ~linkage()~ 函数基于 ~pdist()~ 函数计算的样本
间距离，对样本进行聚类。在这个过程中， ~linkage()~ 函数必须还要能
够计算样本到聚类中心、两个聚类中心的距离。函数默认使用的算法是单
链接算法，除此之外还有 ~average~ , ~centroid~ , ~complete~ ,
~median~ , ~single~ , ~ward~ , ~weighted~ 算法。感兴趣的读者可以
查阅帮助手册学习每个算法是如何计算距离的。

最后展示上面例子所生成的'dendrogram_snp'。我们可以使用
~dendrogram()~ 函数来生成图片。这个图片中，每条聚类中心的横线的纵
坐标，代表了其两个子聚类中心间的距离。如图中最后一个聚类中心 $11$
，即最高的横线纵坐标为 $50$ ，意味着其自聚类中心 $7$ 和 $10$ 间的
距离为 $50$ 。

#+BEGIN_QUOTE
图6.8：'dendrogram_snp'
#+END_QUOTE

图6.8非常直观地展示了层次聚类法是如何对样本点进行分组的。但是为了
更好地理解聚类结果，我们还要继续深入学习。

*** 如何理解层次聚类图

'dendrogram_snp'是用来图形化表示层次聚类结果的树状
图。'dendrogram_snp'的横轴表示数据集中的样本点，纵轴表示样本聚类
中心间的距离。最底层的聚类中心会延 $Y$ 轴伸出一条向上的射线，这些竖线
代表了最底层的聚类中心，即直接使用样本本身作为聚类中心。接下来会
有一些横线链接两条竖线，这代表这些底层聚类中心作为子聚类中心形成
了包含更多样本点的父聚类中心。图中所有的竖线都对应一个聚类中心，
链接两条竖线的横线则表示了聚类中心的合并。两个聚类中心越早（ $Y$
值越低）合并，表示两个聚类中心距离越近。

当需要得到某两个样本间的距离时，我们可以根据树状图，画出链接两个
样本点的最短路径。在这条最短路径上，拥有最大 $Y$ 值的横线，其 $Y$
值就是两个样本点间的距离。

接下来我们正式定义'dendrogram_snp'中的元素。图中链接两条竖线的横
线称为聚类中心线（clade），每条横线都表示了一个由两个子聚类中心合
并而成的父聚类中心。每个竖线代表了父聚类中心的一个分支（chunk），
用于指向组成父聚类中心的子聚类中心。图中最底层的节点称为叶节点
（leaf），它既是聚类中心，同时也是原始数据集中的样本点（其序号按
照样本点在数据集中的行号排列）。如果图中的每个聚类中心都有两个分
支，我们称其为二元聚类树，如果有三个分支，则称为三元聚类树，以此
类推。聚类中心可以有无限多个分支。MATLAB中的层次聚类算法一般使用
二元聚类树。下图显示了一个层次聚类树图：

#+BEGIN_QUOTE
图6.9：层次聚类树图中的聚类中心线（clade）、分支线（chunk）和叶节
点（leaf）
#+END_QUOTE

如前文提到的，聚类中心线的纵坐标表示了其所连接的两个子聚类中心的
距离，聚类中心线越低，子聚类中心的距离越近，反之则越远。其所表示
的距离，即为函数 ~linkage()~ 所计算的结果。任何可被衡量距离的样本
点都可以使用这种方法进行聚类分析。我们有两个角度分
析'dendrogram_snp'：

- 每个聚类中心包含的样本点
- 聚类中心间的距离（相似程度）

对于第一个角度，我们想找到属于某个聚类中心的所有样本点，我们需要
按照从上往下的顺序阅读树状图。例如在图6.8中，我们可以看到聚类中心
~10~ 是由样本点 ~A~ 、 ~B~ 、 ~E~ 、 ~F~ 组成的。在图中，聚类中心
~7~ 直接和聚类中心 ~10~ 组成了最顶层的聚类中心，这表明组成聚类中
心 ~7~ 的样本点 ~A~ 、 ~B~ 与其它样本点的距离相对较远。反之距离较
近的其余四点是逐个被添加进子聚类中心组成父聚类中心的，表示这些样
本点距离较近。

现在我们从距离（相似程度）的角度出发来分析上图。我们已经知道，横
线表示的是使用 ~linkage()~ 函数衡量的，所连接的两个聚类中心间的距
离。当我们想知道任意两个聚类中心的距离（相似程度）时，我们可以从
下往上阅读聚类树图。如图6.8所示，聚类图中最底层的序号 $1$ 到 $6$
表示的是原始数据集中的样本点的顺序，在图中已经标注了对应的点。这
些点逐层地被横线链接起来。任意两个样本点间的距离可以通过寻找最早
链接两个样本点的横线的 $Y$ 值获得。例如，我们想知道样本点 ~A~ 和
样本点 ~E~ 之间的距离，通过从下向上观察树状图，我们发现最早链接两
点的是聚类中心 $9$ 。因此我们得到两样本点间的距离是 $20$ 。另外一
个细节是，图中横线的从左到右、由低到高的顺序，与 ~linkage()~ 函数
返回的计算结果 ~GroupsMatrix~ 的行数一致。 ~GroupsMatrix~ 的第一
行，即聚类中心 $7$ 由子聚类中心 $3$ 和 $4$ 组成，距离是 $10$ 。最后
一行，即聚类中心 $11$ 由子聚类中心 $7$ 和 $10$ 组成，距离是 $50$
，这与图中最低的横线和最高的横线是一一对应的。


*** 验证聚类结果

在之前的章节中，我们学习了如何进行层次聚类、理解聚类树状图。接下
来我们学习如何衡量层次聚类算法的性能。'smltb_snp'提供了完成这一任
务所需的全部函数。

之前我们说过，聚类树状图中横线的纵轴值代表了两个聚类中心的距离，
值越大，距离越远，反之越近，这个纵轴值在生物学中被称为cophenetic
距离。我们可以通过计算聚类结果中的cophenetic距离，与 ~pdist()~ 函
数计算的，原始数据集中样本点间距离矩阵的相关性，来衡量聚类结果的
好坏。这个任务可使用 ~cophenet()~ 函数完成：

#+BEGIN_QUOTE
代码
#+END_QUOTE

这个指标显示了聚类结果的好坏。在一个好的聚类结果中，聚类中心的构
成应该与原始数据集中样本点间的距离有强相关性。 ~cophenet()~ 函数
正是计算此相关性，其值越接近 $1$ 表示聚类结果越好。

为进一步改善聚类结果，我们可以使用其他距离指标重新使用 ~pdist()~
函数计算距离矩阵：

#+BEGIN_QUOTE
代码
#+END_QUOTE

现在通过使用 cosine 距离重新计算的距离矩阵，我们可以重新使
用~linkage()~ 函数对样本进行聚类。与上次不同，这次我们也将使用新
的计算聚类中心间距、样本到聚类中心距离的指标。我们将采用
~weighted~ 算法来计算上述距离，这个算法将计算聚类中心所包括的全部
样本的加权平均距离：

#+BEGIN_QUOTE
代码
#+END_QUOTE

最后，我们将重新调用 ~cophenet()~ 函数评估聚类结果：

#+BEGIN_QUOTE
代码
#+END_QUOTE

结果显示，使用 ~cosine~ 距离和 ~weighted~ 距离计算的聚类结果要好
于默认算法的聚类结果。


** K-means聚类——基于均值聚类
*** K-means算法
*** kmeans()函数
*** silhouette图——可视化聚类结果
** K-medoids聚类——基于样本中心聚类
*** medoids定义
*** kmedoids函数
*** 执行聚类
** 高斯混合模型(GMM)聚类
*** 高斯分布
*** MATLAB中的GMM支持
*** 使用后验概率分布进行聚类
** 总结




























** 概率分类模型——'naba_snp'

贝叶斯分类属于统计学中，用于判断样本属于某一分类的概率的一种方法。
这种方法可被用于，例如，我们基于顾客的工作状况、年龄、收入、喜爱
的运动等信息，判断顾客有多大可能购买一辆跑车。

这种方法的理论基础是贝叶斯理论。贝叶斯是英国18世纪的一位数学家。
这个理论给出了'poste_snp'与'prior_snp'和'like_snp'之间的关系。后
验概率是指，当观察到某些情况已经发生后，待观察的事件发生的概率是
多少。

'naba_snp'利用这个理论，进一步假设，当给定一个样本的分类时，样本
的特征向量中，每个特征值的取值概率，与其它特征值的取值'CI_snp'。
这个假设能够大大简化联合概率分布的计算复杂度，因此被称为朴素
(naive)。当数据集真正满足'CI_snp'这个条件时，'naba_snp'与更加复杂
的模型有同样优秀的结果。


***  概率论基础

在正式学习之前，我们先为读者回顾一些概率学基本概念。如果你已经熟
悉这些概念，那么可以跳过本节。我们建议读者首先确保对基本概念的熟
悉再继续阅读下面模型方面的内容。

首先考虑一个简单的例子。假设有一个不透明箱子，里面有7个白球和3个
黑球，并假设每个球之间除了颜色，其它属性（如重量、材质等）是完全
一致的。现在随机从箱子中取出一个球，请问取出黑球的概率是多少？

- 箱子中共有10个球，因此总共有10种取到不同的球的情况。并且取到任
  何一个具体的球的可能性是均匀分布的，任何球都有相等的可能性被取
  到
- 在这10种情况中，只有3种是取到黑球

因此，在 $取到的球是黑球$ 这个 *事件* 中，10种情况中只有3种符合这
个 *事件* 。我们将 *概率* 定义为 *事件* 发生的情况数在总情况数中的比
率，因此我们得到：

$$取到黑球的概率 = 3/10 = 0.3 = 30\%$$

由此可见，一个 *事件* 发生的概率可以被表示为：

- 分数： $3/10$
- 小数： $0.3$
- 百分数： $30\%$

有了粗略的概念后，我们给出 *概率* 数学公式上的定义。一个 *事件*
$E$ 的概率被定义为事件发生的情况数 $s$ 占总共可能的情况数 $n$ 的
比率。假设所有可能的情况都是等可能发生的（非等可能的情况稍后讨论），
那么可用公式表示为：

$$P=P(E)=\frac{符合事件的情况数}{总可能情况数}=\frac{s}{n}$$

我们来看两个例子：

- 扔一个硬币，硬币朝上面是正面的概率是多少？扔硬币结果的总可能情
  况数为 $2$ ，即 $\{正面，反面\}$ ，因此符合事件的情况数是 $1$
  。所以 $P{朝上面=正面}=\frac{1}{2}=0.5=50\%$
- 扔一个色子，朝上面是$5$的概率是多少？总可能的情况数为$6$，即色
  子总共有6个面。符合事件的情况数是1，因此概率为
  $P(朝上面=5)=\frac{1}{6}=0.166=16.6\%$ 

在上面的定义中，我们用到了“等可能性”这个概念。为更清楚地表述这个
概念，我们引用无差别原则（the Principle of Indifference）来进行解
释：

#+BEGIN_QUOTE
有一组情况，如果没有任何可被证实地理由来证明，某些情况发生的可能
性高于另外一些情况，那么我们认为所有情况发生的可能性是相同的
#+END_QUOTE

在计算总共可能发生的情况、符合事件的情况时，我们经常需要用到排列
组合的知识。

我们已经知道概率可被定义为两个数的比率。完整的定义还应包括，概率
的取值范围是$0$到$1$：

$$0\leq P(E) \leq 1$$

- 概率为0的事件被称为不可能事件。例如，假设我们一个箱子中有6个红
  球，那么从箱子中取一个球，取出的球是黑球的概率为0，即不可能事件
- 概率为1的事件被称为确定事件。在上面的例子中，取出红球的概率为1，
  即确定事件

关于概率的经典定义有很多局限，首先它是从频率角度出发，使用离散且
有限的数字进行定义的，这种定义难以扩展到其它领域。此外，定义中假
设了事件发生的等可能性，即我们事先知道所有可能发生的情况，并且知
道每种情况是等可能发生的，这种极强的假设进一步限制了这种定义的应
用范围。

经典概率的定义是从频率论(Frequentist)的角度出发的，现代概率论
(Probability Theory)与之相比的一大进步就是从频率角度出发引入
了'prior'（例如，我们预先知道一个硬币的质地是不均匀的，反面比正面
重，那么我们在抛硬币之前就已经可以假设，抛掷这个硬币的实验结果是
$75\%$ 可能性正面，这里的 $75\%$ 就是'prior'），即在未观察数据集
中样本之前，人们对这个事件固有的先验知识的概念，并将'prior'与数据
集中观察到的实际情况相结合，从而得到事件的概率。现在我们首先将事
件发生的概率定义扩展为，当有无穷多次重复试验时，事件发生的概率所
逼近的极限值。注意，这个定义是可以适用于没有先验知识，且无需假设
等概率发生的可能性。这个定义的唯一假设是，事件的试验是可以被重复
无穷多次，且每次重复的其它条件完全相同。

有了这个定义，我们就可以使用频率角度下的概率值来逼近概率的极限值。
如果我们有关于一个事件的基于相同条件、大量重复次数的试验结果，那
么我们可以假设基于这个试验的频率所得到的值，是逼近于极限情况下的
真实概率值的：

$$频率 \approx 概率$$

从贝叶斯学派的角度出发，概率是对于一个论断（即事件发生的可能程度）
可信程度的度量。这个定义可被应用于任意事件上。从贝叶斯公式出发，
概率是可以双向推断的，我们可以使用先验概率、似然概率（这些概念稍
后会进行解释）推断后验概率，也可以从后验概率出发推断先验概率。在
贝叶斯公式中，先验概率是指，人们（很多情况是专家、论文结论）对某
一事件发生情况的先验、固有经验，与事件的本次实际试验结果完全无关。
因此先验概率完全是主观的。有了先验概率，我们就可以结合实际试验中
得到的结果（频率角度），来估计事件发生的后验概率。这种方法的精髓
就是将先验经验、知识，与可能存在样本误差的试验结果结合起来，对真
实的概率分布进行估计。

目前为止，我们已经讨论过单一事件发生的概率问题。那么如何估计多个
事件发生的概率呢？现在我们假设有两个相互独立事件（即一个事件发生
的可能性与另外一个事件不相关） $A$ 和 $B$ 。例如，我们有52副扑克
牌，当我们从每副扑克牌中抽取一张卡片时，以下两个事件是相互独立的：

- $E1$ 从第一副扑克牌中抽到 $A$
- $E2$ 从第二副扑克牌中抽到梅花花色的牌

这两个事件是相互独立的，无论一个事件有怎样的结果，都不会改变另一
个事件发生的概率。

与之相反，相互依赖的事件是指，事件 $A$ 发生的概率随着事件 $B$ 是
否发生而改变。假设我们有一副扑克牌（52张），如果我们依次不放回地
连续抽取两张扑克，那么以下两个事件是相互依赖的：

- $E1$ 第一张抽到的牌是 $A$
- $E2$ 第二张抽到的牌是 $A$

准确地说， $E2$ 发生的概率是随 $E1$ 是否发生而改变的：

- $E1$ 发生的概率是 $4/52$
- 如果 $E1$ 发生，那么 $E2$ 发生的概率是 $3/54$
- 如果 $E1$ 没发生，那么 $E2$ 发生的概率是 $4/54$

我们接着研究两个事件中其它模式的依赖关系。如果两个事件不可能同时
发生，我们说两个事件是互斥事件。例如在上面的例子中，以下两个事件
是互斥事件：

- $E1$ 抽到的牌是红桃 $A$
- $E2$ 抽到的牌是人脸牌（纸牌中的 J Q K）

如果一次试验中，两个事件必然有一个发生，我们说这两个事件是互补的。
例如上面的例子中，以下两个事件为互补事件：

- $E1$ 抽到的牌是数字牌
- $E2$ 抽到的牌是人脸牌（纸牌中的 J Q K）

现在我们来考虑多个事件的联合概率分布。假设我们有两个相互独立事件
$A$ 和 $B$ ，那么这两个事件的联合概率分布，等于各自事件的概率分布
的乘积：

$$P(A\cap B)=P(A) \times P(B)$$

举例而言，假设我们有两副扑克牌（52张每副）。我们从两副牌中各抽一
张牌，那么以下两个事件是相互独立事件：

- $A$ 第一副中抽到的牌是 $A$
- $B$ 第二副中抽到的牌是梅花

那么两个事件同时发生的概率为：

- $P(A)=4/52$
- $P(B)=13/52$
- $P(A\cap B)=4/52\cdot 13/52 = 1/52$

如果两个事件是相互依赖的，上面的公式就不成立了。但是我们可以通过
条件概率分布来计算联合概率分布。条件概率分布是指，我们有概率分布
$P(B|A)$ 即当事件 $A$ 发生的条件下，事件 $B$ 的概率分布。由此我们
得到如下公式：

$$P(A\cap B) = P(A) \times P(B|A)$$

例如，假设一个箱子中有2个白球和3个红球，我们从箱子中逐次不放回地
取出两个球。那么，两个球同时是白球的概率为：

- 概率 $P(A)$ 第一次取到白球的概率是 $2/5$
- 条件概率 $P(B|A)$ 如果第一次取到的是白球，那么第二次仍取到白球
  的概率为 $1/4$

根据公式，我们知道两个事件的联合概率为：

$$P(A\cap B)=2/5 \cdot 1/4=1/10$$

理解了上面一系列的例子后，我们开始正式定义条件概率。当已知事件
$B$ 发生后，事件 $A$ 发生的概率，称为事件 $A$ 的条件概率，并使用
符号 $P(A|B)$ 表示。条件概率可以通过如下公式计算（从现在开始我们
使用符号 $P(A,B)$ 代替前文中的 $P(A\cap B)$ ）：

$$P(A|B)=\frac{P(A,B)}{P(B)}$$

通常我们只有在事件 $A$ 依赖于事件 $B$ 时才使用条件概率。如果事件
$A$ 和 $B$ 是相互独立的，那么条件概率退化为（因为相互独立事件
$P(A,B)=P(A)\times P(B)$ ，代入条件概率公式即得如下结果）：

$$P(A|B)=P(A)$$

举例而言，在上面从盒子取两个球的例子中，假设我们已经知道取出的第
一个球是白球，那么取第二个球仍是白球的概率是多少呢？在上面的例子
中我们已经得出，两个球都是白球的概率是 $P(A,B)=1/10$ 。代入条件概
率公式即得如下结果：

$$P(B|A)=\frac{P(A,B)}{P(B)}=\frac{1/10}{2/5}=1/4$$

我们再考虑一个例子。现在假设我们投掷一枚色子，我们已经知道结果是
个奇数，请问结果是 $1$ 的概率是多少？这里我们令事件 $A$ 为投掷结
果是奇数，事件 $B$ 为投掷结果是 $1$ 。

在这个例子中，投掷结果为奇数的概率为 $P(A)=3/6=1/2$ 。因为 $1$ 本
身就是奇数，因此联合概率 $P(A,B)$ 即投掷结果是 $1$ 且是奇数的概率
为 $P(A,B) = 1/6$ 。因此我们得到条件概率：

$$P(B|A)=\frac{P(A,B)}{P(A)}=\frac{1/6}{1/2}=\frac{1}{3}$$




*** MATLAB 中的贝叶斯方法

本节我们开始介绍'naba_snp'及其在MATLAB中的实现。就像在本章开头部
分所说的，'naba_snp'利用'CI_snp'，进一步假设，当给定一个样本的分
类时，样本的特征向量中，每个特征值的取值概率，与其它特征值的取
值'CI_snp'。这个假设能够大大简化联合概率分布的计算复杂度，因此被
称为朴素(naive)。当数据集真正满足'CI_snp'这个条件时，'naba_snp'与
更加复杂的模型有同样优秀的结果。

在MATLAB中，使用'naba_snp'需要两步：

- 第一步， *训练* ：'naba_snp'首先使用预先标注好类别标签的数据集，在
  这个数据集上求解模型参数，即根据'CI_snp'假设，分别估计每个特征
  值的概率分布
- 第二步，预测分类标签（下面简称 *预测* ）：对新的，未经标注的数
  据集，使用第一步中训练好的分类器，计算每个样本属于任一类别
  的'poste_snp'。预测分类标签的结果是使每个样本'poste_snp'最大的
  那个标签

在'detree_snp'部分中我们已经使用过 Iris 花朵数据集。这个数据集非
常精炼，是众多教程中帮助读者理解'naba_snp'的经典数据集。本书中我
们延续这一传统。我们将继续使用 Iris 数据集来学习'naba_snp'。具体
而言，我们将使用 Iris 数据集中的花瓣(petals)数据（长度和宽度）构
建贝叶斯分类器。

为了训练'naba_snp'，我们将使用 ~fitcnb()~ 这个函数。这个函数可以
返回一个多分类问题的朴素贝叶斯分类器。在实践中，我们最好预先将类
别标签排序，这样我们才能使用 ~fitcnb()~ 函数解决多分类问题。在此，
我们将使用花瓣的长度和宽度作为输入数据（特征向量），类别标签则有
setosa, versicolor 和 virginica。

与之前相同，我们使用如下代码加载 Iris 数据集：

#+BEGIN_QUOTE
代码
#+END_QUOTE

首先，我们从 ~meas~ 矩阵中提取第三、四列特征，即花瓣的长度和宽度。
接着，我们创建一个 ~table~ 类型的变量 ~PetalTable~ 来储存这些特征
值：

#+BEGIN_QUOTE
代码
#+END_QUOTE

训练'naba_snp'代码如下：

#+BEGIN_QUOTE
代码
#+END_QUOTE

执行上面的代码后， ~fitcnb()~ 函数将返回一个类型为
~ClassificationNaiveBayes~ 的变量 ~NaiveModelPetal~ 。这个变量有
许多的方法和属性，我们可以使用 ~.~ 操作来访问。例如，我们可以通过
如下代码来查看训练好的（已求解出参数的）贝叶斯分类器，对每个类别
所估计的高斯分布（即 $P(特征值|标签)$ ）的均值和标准差：

#+BEGIN_QUOTE
代码
#+END_QUOTE

在上面 $3 \times 2$ 的 ~cell~ 矩阵中，每个 ~cell~ 单元格
（ $2\times 1$ ~double~ 类型）都保存了其所对应的均值和方差。对应
关系为，每一行代表一类，在这里从上到下分别表示 setosa, versicolor
和 virginica；每一列代表一个特征值，在这里从左到右分别表示花瓣的
长度和花瓣的宽度。因此，为了得到贝叶斯分类器所估计的 ~versicolor~
类的，花瓣的长度概率分布函数的均值和标准差，我们可以执行如下代码
（在下面的执行结果中，第一个值是均值，第二个值是标准差）：

#+BEGIN_QUOTE
代码
#+END_QUOTE

同理，得到 ~setosa~ 类花瓣宽度的代码为：

#+BEGIN_QUOTE
代码
#+END_QUOTE

为检验训练完毕模型的拟合效果，我们可以计算模型的训练误差。训练误
差（训练误差 training error, 也在MATLAB的某些工具箱中也称为
再代入误差 resubstitution error，前者更为通用、易于理解）计算的是
训练好的模型，对于训练集中样本的分类结果的错误分类的比率。训练误
差能够告诉我们模型对训练集的拟合效果的好坏。我们可以通过如下代码
进行计算：

#+BEGIN_QUOTE
代码
#+END_QUOTE

结果显示，有 $4\%$ 的样本被错误分类了。训练误差虽然计算简单，但是
它不能告诉我们训练模型都犯了什么类型的错误。具体而言我们无法回答
以下问题：

- 这 $4\%$ 的误差，在三个类别中是均匀分布的吗？
- 如果不是均匀分布，那么这 $4\%$ 的误差是由单一类别引起的，其它类
  别全部分类正确吗？

为了更好地理解模型错分的样本，我们可以计算一种被称为'confmat_snp'
的矩阵。与训练误差类似，'confmat_snp'也仅使用样本的真实标签，和分
类器所预测的标签进行计算，但是包含更为丰富的内容。我们常常使用混
淆矩阵来评估分类器的性能，而非简单地使用训练误差。下面的表格展示
了二分类问题的'confmat_snp'（混淆矩阵及其缩写，属于机器学习领域读
者必须熟知的概念，因此不翻译英文，请读者务必熟记）：

|              | Predicted Positive | Predicted Negative  |
|--------------+--------------------+---------------------|
| Actual TRUE  | TP (True Positive) | TN                  |
| Actual FALSE | FP                 | FN (False Negative) |

其中， ~Actual TRUE~ 表示的是在实际数据集中，真实分类标签为~TRUE~
的样本（注意此处仅考虑二分类问题，分类标签仅有 ~TRUE~ 和 ~FALSE~
两种）； ~Predicted Positive~ 表示的是分类器预测结果为 ~TRUE~ 的
样本。表格中的每个值代表如下含义：

- ~TP~ 表示实际标签为 ~TRUE~ ，且分类器预测为 ~TRUE~ 的样本个数
  （即分类器能够正确分类的，样本真实标签为 ~TRUE~ 的样本个数）
- ~FN~ 表示实际标签为 ~FALSE~ ，且分类器预测为 ~FALSE~ 的样本个数
  （即分类器能够正确分类的，样本真实标签为 ~FALSE~ 的样本个数）
- ~TN~ 表示实际标签为 ~TRUE~ ，但分类器预测为 ~FALSE~ 的样本个数
  （即被分类器错误分类的，样本真实标签为 ~TRUE~ 的样本个数）
- ~FP~ 表示实际标签为 ~FALSE~ ，但分类器预测为 ~TRUE~ 的样本个数
  （即被分类器错误分类的，样本真实标签为 ~FALSE~ 的样本个数）

显然，在主对角线上的值，表示分类器能够正确分类的样本的数量。其它
的值表示被错误分类的样本数量。在MATLAB中，我们可以使用
~confusionmat()~ 函数计算混淆矩阵。在计算混淆矩阵之前，我们先要获
取之前训练的分类器 ~NaiveModelPetal~ 对每个样本预测的标签，然后再
输入函数进行计算。代码如下：

#+BEGIN_QUOTE
代码
#+END_QUOTE

如预期的一样（ $4\%$ 的训练误差），只有6个样本被错误分类了。通过
混淆矩阵我们知道它们原本属于的类是 ~versicolor~ 和 ~virginica~ 。
为理解为何这6个样本会被错分，我们可以使用以花瓣的长度和宽度为坐标
轴的二维散点图来帮助理解。为了更好地绘制图表，我们可以先使用如下
代码确定坐标轴的范围：

#+BEGIN_QUOTE
代码
#+END_QUOTE

现在我们可以绘制网格图：

#+BEGIN_QUOTE
代码
#+END_QUOTE

# zen: 画图技巧
接着我们可以使用之前训练好的分类器，对网格图中每单元网格进行预测：

#+BEGIN_QUOTE
代码
#+END_QUOTE

现在我们可以绘制预测结果的散点图：

#+BEGIN_QUOTE
代码
#+END_QUOTE

为了让图片更加直观，我们对图中添加了标题和横纵坐标轴的标签。下图
展示了分类器 ~NaiveModelPetal~ 是如何根据花瓣的长度和宽度进行分类
的：

#+BEGIN_QUOTE
图5.6：分类结果分布示意图
#+END_QUOTE

** 'dian_snp'分类



'dian_snp'是由Fisher在1936年提出的线性判别分析（Linear
Discriminant Analysis, LDA）演变而来的统计学方法。最早是使用一维
函数描述两组或多组分类样本，并将样本按照类别分类。与之前的方法相
同，判别分析同样适用于分类问题。它要求有一组预先定义好的类别标签，
以及由多个样本、每个样本的多个特征值所组成的特征值矩阵，及其对应
的标签向量（训练数据集）。判别分析也可用于判断，任意一组特征值是
否足够对训练集进行有效的分类。

在MATLAB中，'dian_snp'基于如下假设：

- 每个类别都服从多元正态分布（可以看作是混合高斯分布的一种特殊情
  况）
- 对线性判别分析，所有类别服从标准差相同的正态分布，只有均值不同
- 对二次判别分析，均值和标准差都可以不同

基于上面的假设，'dian_snp'模型的目标函数可被表示为最小化期望分类
损失：

$$Y=\text{arg}\min_{y=1,...,K}\sum_{k=1}^{K}{P(k|x)C(y,k)}$$

这里：

- $Y$ 代表对样本 $x$ 的分类标签
- $K$ 代表分类标签的个数
- $P(k|x)$ 代表样本 $x$ 属于第 $k$ 类别的'poste_snp'
- $C(y,k)$ 是损失函数，样本 $x$ 的真实标签是 $y$ 。损失函数 $C$
  衡量将样本分类到第 $k$ 类别所造成的损失

在这里我们继续以 Iris 数据集举例来学习判别分析。记得先使用以下代
码导入数据集：

#+BEGIN_QUOTE
代码
#+END_QUOTE

MATLAB提供了 ~fitcdiscr()~ 函数返回一个训练完毕的判别分析模型。这
个模型使用高斯分布对每个类别进行估计。下面的代码将使用整个数据集
训练判别模型：

#+BEGIN_QUOTE
代码
#+END_QUOTE

与之前相同，我们可以使用 ~.~ 运算来访问成员方法和属性。注意上面代
码返回的输出中的倒数第二行 ~Mu~ 变量，它代表了每个特征值对应每个
分类的高斯分布的均值。我们可以使用以下代码获取这些数据：

#+BEGIN_QUOTE
代码
#+END_QUOTE

其中，每一行代表一个类别，从上往下依次代表setosa, versicolor 和
virginica。每一列代表一个特征值，从左到右依次代表萼片的长宽、花瓣
的长宽。

下面我们来研究下 ~DiscrModel~ 的 ~Coeffs~ 属性：

#+BEGIN_QUOTE
代码
#+END_QUOTE

这个属性返回大小为 $n \times n$ 的结构体矩阵，在我们的例子中，因
为有 $3$ 个类别，所以 $n=3$ 。每个结构体数组都包含着界定两类线性
分类边界的系数。为何我们要讨论线性边界呢？因为'dian_snp'将 $n$ 维
空间分为多个区域，每个区域属于一个类别。这些线性边界正是这多个区
域的分界线。当使用训练好的判别分析模型进行预测时，我们输入未经标
注的样本的特征向量，并观察这个特征向量处于 $n$ 维空间的哪个区域，
并使用这个区域所属的类别标签对其分类。

因此 ~Coeffs(i,j)~ 代表的是第 $i$ 类和第 $j$ 类之间的线性分类边界。
这个边界可表示为线性方程：

$$\text{Const} + \text{Linear} \times x = 0$$

其中， $x$ 表示输入样本的特征向量。为便于可视化，与之前相同，接下
来我们只使用两个特征，花瓣的长度和宽度训练模型，这样我们才能将任
意结果展示在二维图表中：

#+BEGIN_QUOTE
代码
#+END_QUOTE

仅使用花瓣的特征值进行训练：

#+BEGIN_QUOTE
代码
#+END_QUOTE

绘制训练集中特征矩阵的散点图，并使用标签向量对每个样本进行标注：

#+BEGIN_QUOTE
代码
#+END_QUOTE

获取类别 ~setosa~ 和 ~versicolor~ 之间线性边界的代码（标签 $1$
$2$ 按顺序与之对应）：

#+BEGIN_QUOTE
代码
#+END_QUOTE

在图上绘制出两个类别间的线性边界：

#+BEGIN_QUOTE
代码
#+END_QUOTE

同理，获取类别 ~versicolor~ 和 ~virginica~ 之间线性边界的代码（标
签 $2$ $3$ 按顺序与之对应）：

#+BEGIN_QUOTE
代码
#+END_QUOTE

绘制两类之间的分类边界：

#+BEGIN_QUOTE
代码
#+END_QUOTE

最后在图上绘制坐标轴标签和图表标题：

#+BEGIN_QUOTE
代码
#+END_QUOTE

下图显示了Iris数据集的散点图，并且绘制了'dian_snp'训练后得到的不
同类别间的线性分类边界：

#+BEGIN_QUOTE
图5.7 添加了不同类别间的线性分类边界的Iris数据集散点图
#+END_QUOTE

现在我们使用训练好的模型，对三个新的花朵样本进行分类。如图5.8所示，
下面三个点落在了三个分类区域中：

- $P1$ ：花瓣长度为 $2cm$ ；花瓣宽度为 $0.5cm$
- $P2$ ：花瓣长度为 $5cm$ ；花瓣宽度为 $1.5cm$
- $P3$ ：花瓣长度为 $6cm$ ；花瓣宽度为 $2cm$

首先我们对上面的三个样本构建训练数据（特征向量）：

#+BEGIN_QUOTE
代码
#+END_QUOTE

为了使用训练好的模型对新样本进行预测，我们使用 ~predict()~ 函数。
这个函数将返回与输入的特征向量顺序一一对应的，模型预测的标签向量
结果：

#+BEGIN_QUOTE
代码
#+END_QUOTE

现在我们在之前的散点图中画出新添加的这三个点：

#+BEGIN_QUOTE
代码
#+END_QUOTE

图5.8是在图5.7中添加了三个新样本之后的结果。这幅图可以允许我们通
过观察新样本所属于的区域的标签，来验证 ~predict()~ 函数分类的正确
性：

#+BEGIN_QUOTE
图5.8 添加了不同类别间的线性分类边界的Iris数据集散点图，并添加3个
新样本
#+END_QUOTE

从图5.8中可以看出，除了少部分点落在 ~versicolor~ 和 ~virginica~
之间的分类边界上外，判别分析模型的分类效果还是不错的。我们可以通
过使用更高次的模型对数据集进行拟合，以求达到更好的分类效果。为达
到此目的我们可以将模型中 ~DiscrimType~ 键值对设置为
~pseudoLinear~ 或者 ~pseudoQuadratic~ 。

为检测模型效果，我们可以使用以下代码计算训练误差：

#+BEGIN_QUOTE
代码
#+END_QUOTE

$2\%$ 的训练误差表明模型对Iris数据集具有很好的拟合效果。
与'naba_snp'部分相同，为理解错分样本的分布状况，我们可以计
算'confmat_snp'。同样，在计算混淆矩阵前我们首先需要得到模型对训练
样本的预测结果：

#+BEGIN_QUOTE
代码
#+END_QUOTE

与预期相同，我们只在 ~versicolor~ 和 ~virginica~ 分类中有3个错分
样本。我们可以通过绘制下图观察是哪三个样本被错分了：

#+BEGIN_QUOTE
代码
#+END_QUOTE

这里我们解释下上面的代码。首先我们使用 ~strcmp()~ 函数对模型预测
结果和训练集中真实的标签向量进行比较，如果两个字符串相同，函数返
回逻辑值 $1$ ，否则返回 $0$ 。比较的结果保存在向量 ~Err~ 中，它接
下来会被作为一个 ~logical mask~ 向量（MATLAB术语，作用是过滤
掉~logical mask~ 向量中值不为 $1$ 的元素。这是MATLAB中非常强大且
常用的技巧，建议读者自行学习）。接着，我们以花瓣的长宽为坐标轴，
绘制Iris数据集的二维散点图。最后，我们将错分样本标注在散点图上。

下图显示了标注有错分样本的散点图：

#+BEGIN_QUOTE
图5.9 标注有错分样本的散点图
#+END_QUOTE

正如预期的，错分样本是落在 ~versicolor~ 和 ~virginica~ 分类边界上
的点。

** 'knn_snp'

分类问题的一个重要任务就是证明不同类别的样本的特征向量之间存在显
著差异，这是分类模型起作用的基本条件。当我们使用训练集训练完分类
模型后，我们可以使用另外一组经过标注，但模型训练过程中没用到过的
数据，输入模型得到预测结果，来验证模型的泛化性能（在未知数据集上
模型的预测能力），这样的数据集根据使用的阶段、目标不同，分别被称
为'vs_snp'和'tes_snp'。之前训练好的模型的泛化性能就可以通过观察对
这些样本的预测结果进行检验。

'knn_snp'是众多分类算法中的一种，它基于样本间的距离（距离的定义有
很多种，是多种衍生算法的核心区别），将待分类样本赋值到距离它最近
的 $k$ 个邻近样本中，最多样本所属的分类。这也是本章中我们介绍的唯
一不需要训练模型的分类方法，它直接使用训练数据集就可以完成分类任
务。经典'knn_snp'使用欧氏距离衡量样本间的距离，欧式距离的定义是：

$$D=\sqrt{{\sum_i{(x_i-y_i)^2}}}$$

在二维空间中，两点之间最短的欧式距离是链接两点的直线。这个距离是
按照上面的公式，使用两个向量之差的平方开根号计算的（公式中使用的
是代数表示而非矩阵表示）。

一个样本将被归类为其周围 $K$ 个近邻样本中，大多数样本所属的类别。
其中 $K$ 是算法的可选参数，表示每次对新加入样本分类时，考虑 $k$
个距离其最近的样本。如果 $K=1$ ，那么新样本被分类为其最近样本所属
的分类。当然这并不是最好的参数设置，因为只基于最近样本分类必将导
致非常高的分类误差。

因此，我们通常考虑 $2$ 到 $10$ 个最近的邻居并使用其中多数样本所属
的分类。 $K$ 值的选取通常基于人们的先验知识，包括对数据集的预先观
察。总的来说，相对较大的 $K$ 值通常会得到更少的噪声、更好的结果，
但是对不同数据集效果并不相同。多数情况下，我们选取奇数作为 $K$ 值
以尽可能避免有相同多个最大样本数的分类的情况；尽管这样仍然可能出
现多个候选分类，这时我们可以通过具体衡量每个样本到新样本的距离来
进行选择。

'knn_snp'算法的一大优势是不需要训练，能够直接对新加入样本进行分类。
另外，它能够对非线性分类（类别之间不存在线性分类边界）问题进行分
类。KNN具有非常好的鲁棒性，数据集中少量的噪声很难引起分类结果的变
化。

KNN最大的缺点是需要保存全部数据集。对于大数据应用而言，KNN算法非
常耗费内存。此外一个显著限制是，经典算法有非常大的计算量——为计算K
邻近样本，需要计算新加入样本与数据集中每个样本的欧式距离。然而这
个问题在其衍生算法的程序化实现中，可以通过借助一些特殊设计的数据
结构（如树结构）解决。另外一个缺陷就是，尽管在大数据中难以应用，
KNN算法仍需要大量标注好的数据才能达到令人满意的精度。

在MATLAB中，KNN分类器可以使用 ~fitcknn()~ 函数构建。接下来我们仍
使用 Iris 数据集学习KNN。首先导入数据集：

#+BEGIN_QUOTE
代码
#+END_QUOTE

KNN，我们需要设置参数 $K$ 。在这里我们选取距离新样本
最近的 $3$ 个样本：

#+BEGIN_QUOTE
代码
#+END_QUOTE

上面的代码中，我们令 ~fitcknn()~ 函数返回一个 ~ClassificationKNN~
类型的变量 ~KnnModel~ 。这个类型中，控制距离算法的属性 ~Distance~
和代表参数 $K$ 的属性 ~NumNeighbors~ 都是可以随时更改的。

#+BEGIN_QUOTE
小贴士：我们随时可以双击'workspace_snp'中的 ~KnnModel~ 变量来查看
其属性值。
#+END_QUOTE

在'workspace_snp'中双击 ~KnnModel~ 将打开'variables_window_snp'，
其中将显示一长串的变量的全部属性。我们可以通过双击任一属性来查看
它的值。下图显示了'variables_window_snp'：

#+BEGIN_QUOTE
图5.10: 'variables_window_snp'
#+END_QUOTE

与之前相同，我们仍可以使用 ~.~ 运算获取属性值。例如我们可以使用如
下代码获取分类标签的名称：

#+BEGIN_QUOTE
代码
#+END_QUOTE

为检验模型分类表现，我们可以使用如下代码计算训练误差（对于KNN算法
而言没有训练过程，此处直接将算法应用到训练集上预测每一样本的分类
结果，从而得到训练误差）：

#+BEGIN_QUOTE
代码
#+END_QUOTE

结果显示KNN错分了 $4\%$ 的样本。与之前相同，我们仍可以先获得KNN对
训练集样本的预测，再计算'confmat_snp'来更好地分析模型错分的原因。
代码如下：

#+BEGIN_QUOTE
代码
#+END_QUOTE

如训练误差所示，总共有 $7$ 个样本被错分了，并且我们看到与之前几个
分类算法相同，它们仍属来自 ~versicolor~ 类和 ~virginica~ 类。之前
我们解释过，训练误差过于简单，无法使我们深入理解模型错分的原因。
而'confmat_snp'可以使我们更加直观、详细地了解模型犯错误的类型及原
因。但注意，我们之前强调过，无论是训练误差，还是混淆矩阵，都只能
衡量模型对训练集拟合程度的好坏，不代表任何对模型泛化能力相关的衡
量。

#+BEGIN_QUOTE
'zyr_snp' 作者写作过于随性。原文中介绍'cv_snp'的下面两段被挪到上
一节('dian_snp')中。上节中已经使用过'cv_snp'，放在这节才重
复'cv_snp'的概念不合常理。
#+END_QUOTE

与'dian_snp'小节相同，接下来我们仍将使用'cv_snp'来检验KNN的泛化能
力：

#+BEGIN_QUOTE
代码
#+END_QUOTE

上面的代码将返回一个 ~ClassificationPartitionedModel~ 类型的变量。

现在，我们可以查看交叉验证所得出的， $K$ 折平均模型预测误差了：

#+BEGIN_QUOTE
代码
#+END_QUOTE

上面的交叉验证结果与我们直接使用训练集得到的训练误差非常近似（绝
大多数情况并非如此，因为KNN本身没有训练即求解参数过程，训练误差的
计算相当于1折交叉验证，因此才会出现这种特例）。因此，我们可以假设
即使对新的样本模型仍有接近于 $96\%$ 精确度的预测能力。

之前我们提到过，参数 $K$ 的选择将决定模型表现。现在我们更改 $K$
的取值来验证这点。正如之前所说，我们可以通过修改 ~NumNeighbors~
属性达到这个目标。这里我们将其设置为 $5$ ：

#+BEGIN_QUOTE
代码
#+END_QUOTE

执行上述代码后，我们可以执行以下命令来查看新修改的模型的训练误差，
并和之前的模型进行比较：

#+BEGIN_QUOTE
代码
#+END_QUOTE

同样，我们也可以使用交叉验证对其泛化能力进行检验：

#+BEGIN_QUOTE
代码
#+END_QUOTE

可以看出，当 $K=5$ 时，对于Iris数据集而言，KNN算法具有更好的表现。
我们可以通过计算混淆矩阵进一步验证：

#+BEGIN_QUOTE
代码
#+END_QUOTE

通过修改参数 $K$ ，我们降低了错分样本数量，这次只有5个样本被错分
了。

在开头我们提到，对KNN算法除了可以设定参数 $K$ 之外，我们还可以更
改距离的衡量指标。除了欧式距离，是否其它距离能够进一步优化模型效
果呢？

正如之前提到的，我们可以通过修改 ~Distance~ 属性实现这点。这个属
性可以接收既定的字符串参数（距离名称），也可以接收用户自定义的距
离的函数句柄。这里我们使用 ~cosine~ 距离，并且让算法将待分类样本
同整个数据集进行比较（设置 ~'NSMethod'~ 属性为 ~'exhaustive'~ ）：

#+BEGIN_QUOTE
代码
#+END_QUOTE

计算训练误差：

#+BEGIN_QUOTE
代码
#+END_QUOTE

现在我们可以进一步计算混淆矩阵：

#+BEGIN_QUOTE
代码
#+END_QUOTE

我们看到，通过使用 ~cosine~ 距离，只有3个样本被错分了。


** MATLAB分类器APP

在之前的小节中，我们学习了一些MATLAB封装好的分类模型函数。为了理
解不同算法的区别我们以Iris数据集为例进行了多次试验。现在我们已经
充分理解了这些模型的概念，我们可以抛开代码，直接使用MATLAB封装好
的可视化APP Classification Learner APP 来完成以上任务。

这个APP可以让我们可视化、交互式地完成与之前完全相同的分类任务。它
可以让我们及其简单地、自动化地完成包括（上文在举例函数代码时没有
使用到的参数设置）特征选择、交叉验证参数设置、模型训练等任务。它
提供的分类模型包括：'detree_snp'，'dian_snp'，支持向量机（Support
Vector Machines, SVM），'lore_snp'，'knn_snp'，以及集成分类
（ensemble classification）。

Classification Learner APP 提供的都是监督学习类分类算法，从带有标
注的数据集中学习、优化模型参数、训练模型。并使用训练后的模型对新
加入的样本进行预测。训练好的模型可以导入到'workspace_snp'中，也可
以根据模型训练结果自动生成相应的MATLAB代码以便以后重复使用。

下面我们开始学习Classification Learner APP，首先我们导入Iris数据
集：

#+BEGIN_QUOTE
代码
#+END_QUOTE 

在调用APP之前，我们先创建一个 ~table~ 类型的变量来保存相关数据：

#+BEGIN_QUOTE
代码
#+END_QUOTE

现在数据已经在'workspace_snp'中可见了，我们可以开始使用APP完成分
类工作。在工具栏中点击 ~APPS~ 选项卡，并点击 ~Classification
Learner~ 图标，APP就会自动打开，如下图所示：

#+BEGIN_QUOTE
图5.11：Classification Learner APP
#+END_QUOTE

为向APP中导入'workspace_snp'中存在的数据，在 ~File~ 部分点击 ~New
Session~ 按钮，将会打开一个 ~New Session~ 对话框。它包含三块内容
（如图5.12）：

- 第一步：选择一个 ~table~ 或 ~mat~ 类型的变量。这里我们选择训练
  集
- 第二步：选择特征值（APP中称为predictors，预测变量）和标签向量
  （APP中称为response，响应值）。这里我们可以设置变量及其类型
- 第三步：定义'cv_snp'参数。这里我们可以设置交叉验证选用的方法

#+BEGIN_QUOTE
小贴士：'cv_snp'允许精确、鲁棒地衡量训练模型的泛化能力。这个工具
能够帮助我们选择有最好泛化能力的模型设置
#+END_QUOTE

下图中展示了 ~New Session~ 对话框及其三部分：

#+BEGIN_QUOTE
图5.12：Classification Learner APP 中的 ~New Session~ 对话框
#+END_QUOTE

在图5.12中，第一步我们选择了之前生成的训练集数据 ~Irisable~ 。选
定之后，第二步中就会显示 ~table~ 类型的变量中所保存的各种变量名及
其值。此外，APP会自动尝试将变量分成特征值（这里称为predictor，预
测值）和分类标签（这里称为response，响应值）。如果必要的话，我们
随时可以APP的自动分类结果进行更改。当修改完交叉验证参数后，我们可
以点击 ~Start Session~ 按钮完成数据导入。

#+BEGIN_QUOTE
小贴士：在'cv_snp'中，我们可以设置 $K$ 折（数据集被等分为 $K$ 份，
且重复验证 $K$ 次）参数。对 *Holdout Validation* （即1折交叉验证，
数据集被简单地拆分为训练集和测试集），我们可以选择拆分的比例。最
后我们可以选择 *No Validation* 选项即不适用任何验证方法，但是这样
非常容易导致'ovfi_snp'。
#+END_QUOTE

现在我们可以使用监督学习在数据集上训练模型。APP将使用训练数据集中
标注好的数据求解模型参数，以建立从特征矩阵（APP中称为predictors，
多个预测值）到分类标签（APP中称为response，响应值）的映射关系。

在 ~Model Type~ 部分中你将发现有如下模型可以选择：

- ~Decision Trees~ 'detree_snp'
- ~Discriminant Analysis~ 'dian_snp'
- ~Logistic Regression~ 'lore_snp'
- ~Support Vector Machines~ 'svm_snp'
- ~Nearest Neighbor Classifiers~ 'knn_snp'
- ~Ensemble Classifiers~ 集成分类

为简单的话我们可以使用 ~All Quick-To-Train~ 选项，点击图5.11中的
~Train~ 按钮直接使用全部算法进行训练。当全部算法训练完毕后，拥有
最佳表现的模型会在对话框中被高亮显示。下图显示了这种操作的训练结
果：

#+BEGIN_QUOTE
图5.13：全部模型训练结果图
#+END_QUOTE

为了理解表现优秀的模型都拥有哪些改进，我们可以直接对比表现最差的
和最好的模型。在 ~History~ 部分中，我们可以看到表现最差的模型是
~Coarse KNN~ ，只有 $64\%$ 的准确率；表现最好的 ~Medium KNN~ 模型
则有 $96.7\%$ 的准确率。

查看分类误差非常简单，直接在对话窗口中双击要查看的模型就可以弹出
散点图。通过观察下图我们非常容易理解为何 ~Medium KNN~ 比另一个模
型效果好出许多，因为第二个散点图中有更多的叉状散点（代表错分样本）：

#+BEGIN_QUOTE
图5.14：最优、最劣模型的散点图比较
#+END_QUOTE

最后，在 ~Classification Learner~ 选项卡的 ~Export~ 部分（工具条
的右侧），有三个选项可供选择：

- *Export Model* （导出模型）：这个选项将训练好的模型以一个 ~struct~ 类型导出到
  'workspace_snp'，同时将导出训练数据
- *Export Compact Model* （导出紧凑模型）：这个选项只导出模型，不
  包括训练数据
- *Generate MATLAB Code* （生成MATLAB代码）：这个选项将导出APP后
  台使用的，训练选中模型的全部代码。这些代码可以用于以后基于新的
  数据集训练新的模型


** 总结

在本章中，我们学习了如何使用MATLAB提供的各种函数、APP完成分类任务。
首先我们学习了'detree_snp'，理解
了'node_snp'、'leaf_snp'和'branch_snp'等概念。并且我们完整地重现
了决策树是如何一步步将样本归类到每个节点的子分支及其子节点的。之
后我们学习了如何使用决策树对新样本进行分类。

接着我们研究了概率分类模型。这类模型基于概率学原理，给出每个样本
属于每个类别的概率是多少。我们学习了概率学的基础概念：频率学派的
概率定义、贝叶斯学派的概率定义、独立和非独立事件、联合概率分布和
条件概率分布。接着我们学习了如何使用'naba_snp'进行分类。

我们也介绍了'dian_snp'方法。我们举了几个例子来比较不同设置的优劣。
我们同时也学习到如何创建模型以最小化期望错分率。并且我们了解了检
验模型训练误差和计算'confmat_snp'的方法。

在下一节中我们学习了'knn_snp'，我们展示了如何根据距离衡量指标对样
本进行分类。通过实验我们发现调整参数 $K$ 以及距离衡量指标能够对模
型分类性能进行改进。并且我们通过'cv_snp'展示了此点。

最后我们展示了 Classification Learner APP，以及使用这个APP构建分
类模型的分步操作。现在，从导入、查看数据集，到特征选择、交叉验证
参数设置、训练模型、评估模型，都变得非常简单。

在下章中，我们讲学习多种聚类方法，以及实践中如何根据实际情况挑选
不同的聚类方法。我们将了解聚类方法的基本概念例如相似度指标，并学
习如何预处理聚类方法所需的数据集。我们还将讨论'kmean_snp'、聚类
树、'depl_snp'等模型。















黑色孤儿

河谷镇

NCIS
