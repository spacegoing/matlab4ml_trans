#+LATEX_HEADER: \usepackage{ctex}
#+LATEX_COMPILER: xelatex

* 第六章：无监督学习

'cluster_snp'方法能够从未经标注的数据集中自动发现隐藏的模式、分组
方法。与之前章节中介绍过的监督学习方法需要从标注过的数据集中获得
信息不同，聚类算法能够通过衡量样本间的'sim_snp'，从而在未经标注的
数据集中学习识别聚类中心、分组方法。

这类算法的目标函数一般是最小化'intradist_snp'的同时最大
化'interdist_snp'。其中，样本、组类间的距离将使用'sim_snp'或
者'dissim_snp'等指标来衡量。

与其它多元统计模型不同，此类聚类算法不需要预先对模型、分类的拓扑
结构等作出任何假设（如'naba_snp'中假设'CI_snp'）。这样聚类算法能
够避免之前算法，因包含先验知识、预先假设，将样本按照先验知识的偏
误而错分的可能性。聚类算法具有发现数据集中我们确定仅仅存在，但还
不能描述的关系、结构的功能。因此，聚类算法实际上是'induction_snp'
的结果，是完全基于实证分析得出的结论。

本章将讲述如何将数据集中的样本归集到聚类中心（cluster），或者如何
对相似的样本进行分组。你将学会'kmean_snp'和'kmed_snp'。我们还将介
绍'hiclu_snp'的相关内容。

我们将学习如下模型：

- 'hiclu_snp'
- 'kmean_snp'
- 'kmed_snp'
- 'gmm_snp'
- 'dendrogram_snp'

在本章结尾，我们将理解聚类算法的基本概念、相似度的衡量指标、如何
进行数据预处理、这些算法聚类的整个流程。并且将学会如何对不同特点
的数据集选用不同算法。

** 聚类分析简介

在分类问题中，我们的任务仅仅是分类，即把样本赋值类别标签。在聚类
分析中，我们不止想要给样本进行一次分类，我们还希望理解每个单独的
类别中，更细致一层的子分类。聚类算法的最终结果往往是给出整个数据
集层次化的分类结构。

分类算法往往通过标注过的数据集学习分类规则。聚类算法中的样本没有
分类标签，我们通过定义距离指标，使用样本在距离空间中的分布情况来
归纳聚类规则。

样本集中的区域被归结为聚类中心（cluster）。如果我们能观察到一个区
域中的样本紧凑地围绕在一个聚类中心周围，并且远离另一聚类中心的样
本，我们则可以假设两个聚类中心的样本满足不同的分类条件。在这种情
况下，我们可以继续研究两个问题：

- 如何衡量样本的相似度
- 如何定义分组方法

样本距离的定义、分组方法是聚类算法的两个基本要素。


*** 相似度与离散度指标

'cluster_snp'涉及到识别数据集中的分组情况。然而只有预先定义好样本
间的'prodis_snp'，我们才能实现这个目标。'prodis_snp'的定义是指样
本间的'sim_snp'或者'dissim_snp'。当邻近测度定义好后，我们就可以对
什么是“一组数据”进行定义。在很多情况下，这个测度涉及到高维空间中
的距离，而聚类结果的好坏很大程度上取决于我们衡量这个距离所采用的
指标。聚类算法基于样本间的距离对样本进行分组，两个样本是否属于同
一聚类中心取决于它们与这个聚类中心的距离。因此，如果一些样本距离
某一聚类中心比其他聚类中心近，我们则说它们同属这一聚类中心。

那么什么是'sim_snp'、'dissim_snp'呢？'sim_snp'指的是数值化的，衡
量两个样本相似程度的指标。因此，相似的两个样本具有高相似度。相似
度的取值范围通常是0（完全不相似）到1（完全相等）。

相反的，'dissim_snp'指的是衡量两个样本间，不相似程度的指标。两个
样本越不相似，离散度越高。一般情况下，我们所说的“距离”指的就是样
本的离散度。与相似度类似，离散度也可以在0到1的范围内取值，但更常
见的是在0到正无穷的范围内取值。

'dissim_snp'可以使用距离指标衡量。距离就是一种满足某种性质的离散
度。例如，两样本 $x$ $y$ 间的欧式距离 $d$ 可以定义如下：

$$d(x,y)=\sqrt{{\sum_i{(x_i-y_i)^2}}}$$

#+BEGIN_QUOTE
小贴士：记得在二维空间中，欧式距离是两点间的最短距离？欧式距离使
用两个特征向量的特征值之差的平方根进行定义。
#+END_QUOTE

除欧式距离外，距离还有非常多的衡量方式，例如欧式距离实际上是
*Minkowski* 距离的一种特例：

$$d(x,y)=\bigg(\sum_i^{}{|x_i-y_i|^r}\bigg)^{\frac{1}{r}}$$

在上面的公式中， $r$ 是一个可选参数。例如，当 $r=1$ 时，我们称之
为 *Manhattan* 距离。它的公式是：

$$d(x,y)=\sum_i^{}{|x_i-y_i|}$$

此外，在介绍'knn_snp'的章节中我们还使用过余弦（cosine）距离，它衡
量从原点出发，两个向量间的余弦大小。余弦距离的定义是：

$$d(x,y)=\frac{\sum x_i y_i}{\sum x_i^2 \sum y_i^2}$$

一旦我们有了距离定义，我们可以开始对样本进行聚类了。目前为止，我
们学习的距离都是使用数值方法进行计算的，我们是否可以使用名义上的
方法衡量离散度呢？

对于'nominal_var_snp'，例如字符串，同样存在多种距离衡量方式。例如，
我们可以衡量两个字符串中有多少相同字母出现在同一位置。下图举例了
两个距离为4的字符串，因为他们有4个不同字母出现在相同位置：

#+BEGIN_QUOTE
图6.1：两个字符串间的距离
#+END_QUOTE

另一个例子是衡量两个字符串间的编辑距离（Edit Distance），即最少需
要多少次如下操作才能将一个字符串变成另一个字符串：

- 加入一个字符
- 删除一个字符
- 更改一个字符


** 聚类方法类型简介

一旦我们选定距离公式，我们需要进一步定义样本是如何被分类的。主要
有以下两种分类方法：

- 'hiclu_snp'使用层次化的形式对数据集进行描述。不同的样本被归类到
  聚类树的各个层级、分支下，与生物学中所使用的聚类树相同
- 'paclu_snp'将数据空间划分为不同区域。数据空间被分为各个区域及子
  区域，且不同的区域、子区域间不相互重叠


*** 层次聚类

在层次聚类中，聚类中心是按照从上到下或从下到上的顺序，递归地被划
分的。我们可以将聚类方法分为两类：

- 从下至上聚类：在初始迭代中，最相近的样本被分到同一聚类中心；每
  次迭代，都按照某一阈值，将相似的聚类中心继续合并；算法在所有聚
  类中心被合并为一个聚类中心时停止
- 从上至下聚类：在初始迭代中，所有样本被分到一个聚类中心；每次迭
  代，都按照样本的离散程度将聚类中心拆分为多个子聚类中心

这两种算法都会得到一个分层嵌套表示的聚类树图。用户可以根据想要的
相似度水平，选择某一层级的聚类方式作为聚类结果。聚类中心的分裂、
合并，是按照预先设定的某一准则以及相似度的衡量指标来进行计算的。

下图显示了一个层次聚类树图：

#+BEGIN_QUOTE
图6.2：层次聚类图示例
#+END_QUOTE


*** 原型聚类

'paclu_snp'将数据集划分到多个离散的聚类中心。给定一个数据集，原型
聚类法将划分多个区域，每个区域代表一个聚类中心。这类方法从一个初
始划分情形出发，在每次迭代中都将变更聚类中心的位置，以及每个样本
所属的聚类中心。这种方法一般需要用户预先设定好聚类中心的数量。为
了达到用户指定的划分数量，这种方法通常需要迭代非常多次才能收敛。
下图显示了一个原型聚类结果图：

#+BEGIN_QUOTE
图6.3：原型聚类图示例
#+END_QUOTE

在'paclu_snp'中，我们通常先随机初始化一种划分方法，接着在后续的迭
代过程中不断优化划分方法，来满足用户预先定义的目标函数。在这种方
法中，我们往往最小化每个聚类内部的距离，同时最大化聚类中心之间的
距离。这可以通过在每次迭代中，将各个样本归属到与上次迭代不同的聚
类中心实现。


** 层次聚类算法

在MATLAB中，'hiclu_snp'通过对样本逐层分组，来生成一个聚类树状图。
其中上层的聚类中心是由下层的多个子聚类中心合并而来的。每一次合并，
都是根据用户预先定义的准则，将最相似的聚类中心合并。在'smltb_snp'
中提供了'hiclu_snp'所需要的全部函数。通多使用 ~pdist()~ 、
~linkage~ 以及 ~cluster~ 函数， ~clusterdata()~ 函数能够使用从下
至上的方法对数据集进行聚类。聚类的结果可以使用聚类树状图显示出来。

如上所述，整个聚类过程需要用到多个函数。其中 ~clusterdata()~ 是主
函数，用来调用其它函数执行结果。

接下来我们逐步解析每个函数的输入、输出，来学习MATLAB中如何进行层
次聚类：

- ~pdist()~ 如上所述，聚类分析的基础是相似度或离散度衡量指标。因
  此开始聚类的第一步就是对相似度进行定义。 ~pdist()~ 函数就提供了
  多种用于衡量样本对间距离的指标
- ~linkage()~ 当距离指标定义完毕后，我们需要将样本分组到层次聚类
  二叉树中。 ~linkage()~ 函数提供了将近邻样本聚类到同一聚类中心的
  方法。每次迭代中，此函数将使用上次迭代所计算的距离数据，对子聚
  类中心进行合并，以生成包含样本更多的父聚类中心。这就是我们之前
  所说的从下至上聚类算法
- ~cluster()~ 在得到聚类树后，我们需要决定使用哪一层的分组结果，
  作为数据集的聚类结果。 ~cluster()~ 函数以聚类树为输入数据，可以
  根据用户指定的算法、阈值，来计算满足某一相似度阈值下，数据集的
  聚类情况。此函数可以直接使用聚类树中某一层次的聚类结果，也可以
  根据用户定义的算法、阈值，在任意相似度下计算聚类结果

接下来我们通过一个简单的例子来学习这些函数。


*** 层次聚类中的相似度指标

之前我们提到过，聚类分析通过衡量样本间的'prodis_snp'，能够对数据
集中的样本进行自动分组（聚类）。接下来我们展示MATLAB中的代码实现。

首先，我们使用 ~pdist()~ 函数计算数据集中所有样本两两间的距离。对于
一个有 $n$ 个样本的数据集而言，一共存在 $\frac{n*(n-1)}{2}$ 组距
离。计算结果可被称为距离矩阵或离散度矩阵，下图展示了一个计算结果：

#+BEGIN_QUOTE
图6.4：距离矩阵
#+END_QUOTE

~pdist()~ 函数默认计算样本间的欧氏距离。与其它函数相同，这个函数
也提供了许多其他距离指标。可选的指标有： ~euclidean~ 、
~squaredeuclidean~ 、 ~seuclidean~ 、 ~cityblock~ 、 ~minkowski~
、 ~chebychev~ 、 ~mahalanobis~ 、 ~cosine~ 、 ~correlation~ 、
~spearman~ 、 ~hamming~ 、 ~jaccard~ 。此外，用户依然可以使用自定
义的距离函数。

接下来我们学习一个例子。现在我们在二维平面中定义6个点如下：

1. $A = (100,100)$
2. $B = (90,90)$
3. $C = (10,10)$
4. $D = (10,20)$
5. $E = (90,70)$
6. $F = (50,50)$

这六个点展示在下图中：

#+BEGIN_QUOTE
图6.5：二维平面中的六个点
#+END_QUOTE

下面的命令可以在MATLAB中定义包含这六个点坐标的向量：

#+BEGIN_QUOTE
代码
#+END_QUOTE

现在我们使用 ~pdist()~ 函数来计算每个样本点间的距离：

#+BEGIN_QUOTE
代码
#+END_QUOTE

我们可以看到， ~pdist()~ 按照 $AB$ 、 $AC$ 、$AD$ 的顺序来计算全
部距离，然而之前我们说 ~pdist()~ 可以返回一个矩阵，上面返回的却是
向量的形式，这样的形式不利于我们查看。

为了更方便地查看样本点间的距离，我们可以使用 ~squareform()~ 函数
对 ~DistanceCalc~ 向量进行重构。在新生成的矩阵中，点 $(i,j)$ 表示
的就是从点 $i$ 到点 $j$ 之间的距离。例如，矩阵中的 $(2,3)$ 元素表
示的就是从点 $B$ 到点 $C$ 的距离。

#+BEGIN_QUOTE
代码
#+END_QUOTE

我们可以看到矩阵是对称的，因为两点间的距离跟顺序无关。另外很多情
况下计算距离之前，我们需要对距离矩阵进行正则化（样本点所包含的特
征值的单位不相同）， ~zscore()~ 这个函数可以帮你应对这种情形。它
将把特征值矩阵映射到 ~(0,1)~ 空间。


*** 定义层次聚类中的簇

如本章开头所说，'prodis_snp'和组的定义是聚类算法中两个最重要的定
义。上一节中我们讲述了如何计算样本间距，接下来我们开始定义样本是
如何被归为一个聚类中心的。为了实现这点，我们先要使用 ~linkage()~
函数进行计算。基于 ~pdist()~ 函数的计算结果，我们使用二元聚类中心，
首先把距离子聚类中心最近的样本，与子聚类中心一起归并成父聚类中心，
以此类推，从下至上，直至所有样本都被归到最顶层的聚类中心。

现在我们使用上节中的计算结果计算聚类中心：

#+BEGIN_QUOTE
代码
#+END_QUOTE

仅仅通过一行代码，我们已经完成了上段冗长的文字所描述的任务。为了
理解这个函数及其结果，我们来仔细学习下 ~GroupMatrix~ 矩阵。

这个矩阵中，每一行代表一个聚类中心。需要特别指出的是，在层次聚类
算法中，每个样本就是最底层的聚类中心。因此在 ~GroupMatrix~ 中，聚
类中心的 ~ID~ 是 $样本数+行数$ 求和得到的。例如，第一行的聚类中心
的 ~ID~ 其实是 $6+1=7$ 。矩阵中，前两列代表组成当前行聚类中心的两
个子聚类中心的 ~ID~ 。如第一行中 ~3.0000 4.0000~ 指的是第 ~7~ 聚
类中心是由两个子聚类中心 ~3~ 和 ~4~ 组成的，即样本点 ~C~ 和 ~D~
。第三列代表两个子聚类中心的距离，这个值与 ~DistanceCalc~ 矩阵中
两样本点间的距离是一致的。矩阵 ~GroupsMatrix~ 从第一行到最后一行
的聚类结果，就是聚类算法从下至上，每次选择最近的样本，将更多的样
本与子聚类中心合并为更大的父聚类中心的过程。

有了上面的背景知识，我们接下来对照样本点的二维平面图，来一一解读
~GroupsMatrix~ 中每一行的含义。如上所述，数据集中已经有6个样本点，
因此已经有最底层的， ~ID~ 从 $1$ 至 $6$ 的6个聚类中心，即 ~A~ 到
~F~ 。矩阵的第一行表示的是 ~ID~ 为 $7$ （ $7=6+1$ ）的聚类中心，
其构成是两个子聚类中心 ~3~ 和 ~4~ 组成的，即样本点 ~C~ 和 ~D~ 。
同理，第 $8$ 个聚类中心，即矩阵的第二行所表示的聚类中心，是由第
$1$ 和第 $2$ 个聚类中心，即点 ~A~ 和点 ~B~ 构成的。这两个结果显示
在下图中：

#+BEGIN_QUOTE
图6.6：矩阵中最开始的两行，即第七、第八个聚类中心
#+END_QUOTE

矩阵的第三行通过将样本点 $E$ ，即 ~ID~ 为 $5$ 的聚类中心，归并到
聚类中心 $8$ 中，形成了父聚类中心 $9$ 。接着又将样本点 $F$ 归并到
聚类中心 $9$ 中形成聚类中心 $10$ 。第 $11$ 个聚类中心将子聚类中心
$10$ 和 $7$ 连在一起，至此所有样本点都被归并到同一聚类中心，即聚
类中心 $11$ 。至此算法结束。

#+BEGIN_QUOTE
图6.7：层次聚类算法演示图
#+END_QUOTE

值得指出的细节是， ~linkage()~ 函数基于 ~pdist()~ 函数计算的样本
间距离，对样本进行聚类。在这个过程中， ~linkage()~ 函数必须还要能
够计算样本到聚类中心、两个聚类中心的距离。函数默认使用的算法是单
链接算法，除此之外还有 ~average~ , ~centroid~ , ~complete~ ,
~median~ , ~single~ , ~ward~ , ~weighted~ 算法。感兴趣的读者可以
查阅帮助手册学习每个算法是如何计算距离的。

最后展示上面例子所生成的'dendrogram_snp'。我们可以使用
~dendrogram()~ 函数来生成图片。这个图片中，每条聚类中心的横线的纵
坐标，代表了其两个子聚类中心间的距离。如图中最后一个聚类中心 $11$
，即最高的横线纵坐标为 $50$ ，意味着其自聚类中心 $7$ 和 $10$ 间的
距离为 $50$ 。

#+BEGIN_QUOTE
图6.8：'dendrogram_snp'
#+END_QUOTE

图6.8非常直观地展示了层次聚类法是如何对样本点进行分组的。但是为了
更好地理解聚类结果，我们还要继续深入学习。

*** 如何理解层次聚类图

'dendrogram_snp'是用来图形化表示层次聚类结果的树状
图。'dendrogram_snp'的横轴表示数据集中的样本点，纵轴表示样本聚类
中心间的距离。最底层的聚类中心会延 $Y$ 轴伸出一条向上的射线，这些竖线
代表了最底层的聚类中心，即直接使用样本本身作为聚类中心。接下来会
有一些横线链接两条竖线，这代表这些底层聚类中心作为子聚类中心形成
了包含更多样本点的父聚类中心。图中所有的竖线都对应一个聚类中心，
链接两条竖线的横线则表示了聚类中心的合并。两个聚类中心越早（ $Y$
值越低）合并，表示两个聚类中心距离越近。

当需要得到某两个样本间的距离时，我们可以根据树状图，画出链接两个
样本点的最短路径。在这条最短路径上，拥有最大 $Y$ 值的横线，其 $Y$
值就是两个样本点间的距离。

接下来我们正式定义'dendrogram_snp'中的元素。图中链接两条竖线的横
线称为聚类中心线（clade），每条横线都表示了一个由两个子聚类中心合
并而成的父聚类中心。每个竖线代表了父聚类中心的一个分支（chunk），
用于指向组成父聚类中心的子聚类中心。图中最底层的节点称为叶节点
（leaf），它既是聚类中心，同时也是原始数据集中的样本点（其序号按
照样本点在数据集中的行号排列）。如果图中的每个聚类中心都有两个分
支，我们称其为二元聚类树，如果有三个分支，则称为三元聚类树，以此
类推。聚类中心可以有无限多个分支。MATLAB中的层次聚类算法一般使用
二元聚类树。下图显示了一个层次聚类树图：

#+BEGIN_QUOTE
图6.9：层次聚类树图中的聚类中心线（clade）、分支线（chunk）和叶节
点（leaf）
#+END_QUOTE

如前文提到的，聚类中心线的纵坐标表示了其所连接的两个子聚类中心的
距离，聚类中心线越低，子聚类中心的距离越近，反之则越远。其所表示
的距离，即为函数 ~linkage()~ 所计算的结果。任何可被衡量距离的样本
点都可以使用这种方法进行聚类分析。我们有两个角度分
析'dendrogram_snp'：

- 每个聚类中心包含的样本点
- 聚类中心间的距离（相似程度）

对于第一个角度，我们想找到属于某个聚类中心的所有样本点，我们需要
按照从上往下的顺序阅读树状图。例如在图6.8中，我们可以看到聚类中心
~10~ 是由样本点 ~A~ 、 ~B~ 、 ~E~ 、 ~F~ 组成的。在图中，聚类中心
~7~ 直接和聚类中心 ~10~ 组成了最顶层的聚类中心，这表明组成聚类中
心 ~7~ 的样本点 ~A~ 、 ~B~ 与其它样本点的距离相对较远。反之距离较
近的其余四点是逐个被添加进子聚类中心组成父聚类中心的，表示这些样
本点距离较近。

现在我们从距离（相似程度）的角度出发来分析上图。我们已经知道，横
线表示的是使用 ~linkage()~ 函数衡量的，所连接的两个聚类中心间的距
离。当我们想知道任意两个聚类中心的距离（相似程度）时，我们可以从
下往上阅读聚类树图。如图6.8所示，聚类图中最底层的序号 $1$ 到 $6$
表示的是原始数据集中的样本点的顺序，在图中已经标注了对应的点。这
些点逐层地被横线链接起来。任意两个样本点间的距离可以通过寻找最早
链接两个样本点的横线的 $Y$ 值获得。例如，我们想知道样本点 ~A~ 和
样本点 ~E~ 之间的距离，通过从下向上观察树状图，我们发现最早链接两
点的是聚类中心 $9$ 。因此我们得到两样本点间的距离是 $20$ 。另外一
个细节是，图中横线的从左到右、由低到高的顺序，与 ~linkage()~ 函数
返回的计算结果 ~GroupsMatrix~ 的行数一致。 ~GroupsMatrix~ 的第一
行，即聚类中心 $7$ 由子聚类中心 $3$ 和 $4$ 组成，距离是 $10$ 。最后
一行，即聚类中心 $11$ 由子聚类中心 $7$ 和 $10$ 组成，距离是 $50$
，这与图中最低的横线和最高的横线是一一对应的。


*** 验证聚类结果

在之前的章节中，我们学习了如何进行层次聚类、理解聚类树状图。接下
来我们学习如何衡量层次聚类算法的性能。'smltb_snp'提供了完成这一任
务所需的全部函数。

之前我们说过，聚类树状图中横线的纵轴值代表了两个聚类中心的距离，
值越大，距离越远，反之越近，这个纵轴值在生物学中被称为cophenetic
距离。我们可以通过计算聚类结果中的cophenetic距离，与 ~pdist()~ 函
数计算的，原始数据集中样本点间距离矩阵的相关性，来衡量聚类结果的
好坏。这个任务可使用 ~cophenet()~ 函数完成：

#+BEGIN_QUOTE
代码
#+END_QUOTE

这个指标显示了聚类结果的好坏。在一个好的聚类结果中，聚类中心的构
成应该与原始数据集中样本点间的距离有强相关性。 ~cophenet()~ 函数
正是计算此相关性，其值越接近 $1$ 表示聚类结果越好。

为进一步改善聚类结果，我们可以使用其他距离指标重新使用 ~pdist()~
函数计算距离矩阵：

#+BEGIN_QUOTE
代码
#+END_QUOTE

现在通过使用 cosine 距离重新计算的距离矩阵，我们可以重新使
用~linkage()~ 函数对样本进行聚类。与上次不同，这次我们也将使用新
的计算聚类中心间距、样本到聚类中心距离的指标。我们将采用
~weighted~ 算法来计算上述距离，这个算法将计算聚类中心所包括的全部
样本的加权平均距离：

#+BEGIN_QUOTE
代码
#+END_QUOTE

最后，我们将重新调用 ~cophenet()~ 函数评估聚类结果：

#+BEGIN_QUOTE
代码
#+END_QUOTE

结果显示，使用 ~cosine~ 距离和 ~weighted~ 距离计算的聚类结果要好
于默认算法的聚类结果。


** K-means聚类——基于均值聚类

'kmean_snp'是'paclu_snp'中的一种，它同样也是将原始数据集中的样本
分类到多个聚类中心。对原始数据集，'paclu_snp'从随机初始化的划分方
法出发，通过迭代式地将样本划分到不同区域，以提高某一目标值，最终
实现聚类结果。


*** K-means算法

'kmean_snp'是1967年由 MacQueen 提出的一种，根据样本特征值将数据集
中的样本分成 $K$ 个划分的算法。它可以看作是'gmm_snp'和
EM（expectation-maximization）算法的简化形式：K-means算法使用欧氏
距离来计算样本间的相似程度，GMM算法则使用高斯概率分布函数。

在K-means算法中，它假设每个样本都可被表示为来自同一向量空间的特征
向量。它的目标是最小化簇内方差（或标准差）。每个聚类中心由簇内所
有样本特征向量的均值表示。

本算法由以下过程迭代式计算：

1. 用户设定聚类中心个数 $K$
2. 随机初始化 $K$ 个聚类中心
3. 样本初始化归类：归类的标准是，计算各个样本到每个聚类中心的距离，
   样本点被归类到距离最短的聚类中心。最终原始数据集中形成$K$ 簇划
   分
4. *更新聚类中心* ：根据归类结果，重新计算 $K$ 个划分中所有样本点
   的均值中心，计算结果作为新一轮的 $K$ 个聚类中心
5. *更新样本点归类结果* ：使用新的聚类中心，重新计算各个样本到每
   个聚类中心的距离，样本点被归类到距离最短的聚类中心。形成新的
   $K$ 簇划分
6. 重复步骤4、5，直到所有样本点所属的聚类中心不再发生变化，或满足
   一定的迭代次数

算法初始化时需要预先指定 $K$ 个聚类中心。初始化时聚类中心的位置非
常重要，不同的初始化可能会得到完全不同的结果。最好的选择就是令初
始化的聚类中心尽可能远离彼此。当我们初始化好聚类中心后，我们就可
以根据样本距离最近的聚类中心给每个样本点归类，至此算法的初始化阶
段完成。接下来我们将使用样本点的归类结果，重新计算 $K$ 个划分中所
有样本点的均值中心，计算结果作为新一轮的 $K$ 个聚类中心替代旧的结
果。一旦有了新的聚类中心，我们又可以开始新的一轮对样本点的归类。
这两个步骤将不断重复直到满足用户定义好的某种收敛条件。如果读者逐
步观察计算结果，将发现上述迭代过程将使聚类中心逐渐移动向最佳归类
结果。在下图中，我们展示了一个已经收敛的算法所返回的 $K$ 个聚类中
心：

#+BEGIN_QUOTE
图6.10：数据集中的 $K$ 个聚类中心
#+END_QUOTE


*** kmeans()函数

在MATLAB中，我们可以使用 ~kmeans()~ 函数实现'kmean_snp'，它的输入
数据为原始数据集，输出数据为与原始数据集行数（样本数）相同的向量，
向量中的值对应每个样本所属聚类中心的ID。

正如前文所说，kmeans算法的目标是最小化簇内每个样本到聚类中心的距
离总和。与之前的聚类方法相同，MATLAB同样提供了多种衡量距离的方法，
这里进行简要介绍：

- ~speuclidean~ ：平方欧氏距离（默认值）。聚类中心定义为簇内所有
  样本特征向量的均值
- ~cityblock~ ：差分绝对值的平方。聚类中心定义为，其向量中每个元
  素都是簇内所有样本对应特征值的中位数
- ~cosine~ ： $1$ 减去两样本间的余弦值。聚类中心定义为簇内所有样
  本特征向量正则化到零一区间后的均值
- ~correlation~ ： $1$ 减去两样本的相关性系数。聚类中心定义为，其
  向量中每个元素都是簇内所有样本对应特征值正则化到均值为 $0$ ，标准差
  为 $1$ 的正态分布区间后的中位数
- ~hamming~ ：这个指标只能对二元特征向量使用。聚类中心定义为，其
  向量中每个元素都是簇内所有样本对应特征值的中位数

~kmeans()~ 函数默认使用 ~k-means++~ 算法进行算法初始化，并默认使
用 ~sqeuclidean~ 距离。

现在正式开始代码实现。这里我们使用一组矿石指标数据进行举例。文件
~Minerals.xls~ 中保存了多种矿石的硬度和重量指标。首先我们将其导入
MATLAB：

#+BEGIN_QUOTE
代码
#+END_QUOTE

在导入MATLAB中的矩阵中（大小为 $2470 \times 7$ ），我们仅使用前两
列：

#+BEGIN_QUOTE
代码
#+END_QUOTE

我们通过散点图来观察下数据集：

#+BEGIN_QUOTE
代码
#+END_QUOTE

下图显示了数据集中样本点的分布状况：

#+BEGIN_QUOTE
图6.11：矿石数据集散点图
#+END_QUOTE

从图中我们可以看出，这些点在二维平面中似乎分为四个区域，因此我们
可以设置参数 $K=4$ 。因为算法会进行随机初始化，为了使结果可被重现，
我们先固定随机种子：

#+BEGIN_QUOTE
代码
#+END_QUOTE

这里 ~rgn(1)~ 函数将MATLAB用于生成伪随机数的随机种子固定为 $1$ ，
因此接下来无论在哪台机器上执行多少次，计算结果应该都是相同的。现
在我们开始调用 ~kmeans()~ 函数：

#+BEGIN_QUOTE
代码
#+END_QUOTE

这里创建了两个矩阵： ~IdCluster~ 和 ~Centroid~ 。变量 ~IdCluster~
是一个保存与原始数据集 ~InputData~ 行数（样本数）相同的向量，向量
中的值对应每个样本所属聚类中心的ID。变量 ~Centroid~ 则是一个大小
为 $K\times 2$ 的向量，这里是 $4\times 2$ ，它保存的是每个聚类中
心对应的特征向量。之前提到过，这里的结果是使用 ~k-means++~ 算法初
始化以及 ~sqeuclidean~ 距离得到的。

现在我们可以使用聚类结果对原先散点图进行染色：

#+BEGIN_QUOTE
代码
#+END_QUOTE

下图显示了四个聚类中心的散点图：

#+BEGIN_QUOTE
图6.12：聚类中心散点图
#+END_QUOTE

图6.12中，每种颜色、形状，代表一个聚类。这些颜色和形状能够让我们
看清处于聚类分界处的样本点所属的类别。我们可以看出，从实证角度而
言，kmeans算法具有很好的聚类效果。

我们接着可以将聚类中心标记在上图中：

#+BEGIN_QUOTE
代码
#+END_QUOTE

为了强调聚类中心的位置，我们将聚类中心用粗为4，颜色为黑色，大小为
25号字的叉号将聚类中心标记在散点图中。

#+BEGIN_QUOTE
图6.13：带有聚类中心标记的散点图
#+END_QUOTE

从图6.13中我们可以看出，每个聚类中心都是其聚类所含的样本点的几何
中心。之前提到过，这里的聚类中心是最小化样本点距离之和的结果。

仔细观察图6.13我们还会发现邻近边界处的样本点分类效果并不是很好，
有些区域属于不同聚类的样本点杂糅在了一起。


*** silhouette图——可视化聚类结果

为了搞清楚边界处的聚类效果到底有多好，我们可以使用 ~silhouette~
图来可视化 ~kmeans()~ 函数的聚类结果。 ~silhouette~ 图衡量一个聚
类中的样本点距离相邻聚类的样本点有多近。函数 ~silhouette()~ 使用
原始数据集和聚类结果矩阵作为输入参数。

其中，聚类结果矩阵可以是'categorical_variable_snp'、数值型向量、
字符矩阵、或者由聚类名称的字符向量组成的 ~cell~ 向量。
~silhouette()~ 函数将忽略其中的空值或 ~NaN~ 值，并对应地忽略原数
据集中的样本点。该函数默认使用 ~sqeuclidean~ 距离进行计算：

#+BEGIN_QUOTE
代码
#+END_QUOTE

#+BEGIN_QUOTE
图6.14：kmeans聚类结果的silhouette图
#+END_QUOTE

图6.14中，横坐标表示 ~silhouette~ 值，它的取值范围是 $[-1,1]$ ，
其中 $1$ 表示样本点距离相邻聚类中心非常远， $0$ 表示比较近，而
$-1$ 则表示有可能错分了样本点。

#+BEGIN_QUOTE
小贴士：silhouette图衡量的是一个聚类中的样本点距离其邻近聚类中心
的离散程度
#+END_QUOTE

silhouette图提供了一种可视化的方法用来验证聚类结果的好坏。图中的
值越高，说明聚类结果越好。如果多数样本点的值都很高，则说明聚类结
果很好。如果多数样本点的值都比较低，甚至是负值，则说明聚类结果很
差，这种情况下我们需要重新考虑算法默认参数的设置情况（初始化算法、
距离指标）。因此我们可以使用silhouette图来挑选不同的参数设置结果。

基于图6.14，我们可以看到当 $K=4$ 的情况下，绝大多数样本点都有较高
的值。而且，不同聚类的样本点间没有明显的波动，这点可以通过观察每
个聚类的图形厚度看出。所有的聚类都有相似的厚度。因此我们可以说之
前的算法默认参数设置的是合理的。

为了验证上面的说法，我们使 $K=3$ 并重新执行算法：

#+BEGIN_QUOTE
代码
#+END_QUOTE

下图显示了 $K=3$ 的结果：

#+BEGIN_QUOTE
图6.15： $K=3$ 的silhouette聚类结果图
#+END_QUOTE

通过比较上面两副图片，我们可以发现 $K=3$ 的效果不如之前好。图6.15
中存在几个问题。首先聚类 ~1~ 和 ~2~ 中都有负值出现。其次，聚类内
样本点的值波动很大。最后，3个聚类图形的厚度差别也非常大，很明显聚
类 ~2~ 的厚度远大于其它聚类。因此我们说 $K=4$ 是合理的参数设置。

上面我们验证了合理的聚类个数。但是这是通过可视化的silhouette图得
出的结论。是否有一个量化的方法能够自动选择 $K$ 值呢？MATLAB确实提
供了这种功能。

为了得到最优的聚类中心个数，我们可以使用 ~evalclusters()~ 函数。
这个函数将自动执行多个参数选项并返回最优参数设置。遗憾的是只有以
下几个函数支持这个函数：

- ~kmeans()~
- ~linkage()~
- ~gmdistribution()~

挑选的标准可以使用以下几个指标：

- CalinskiHarabasz
- DaviesBouldin
- gap
- silhouette

下面我们使用 ~CalinskiHarabasz~ 指标来验证 ~kmeans()~ 的执行结果：

#+BEGIN_QUOTE
代码
#+END_QUOTE

结果显示 $K=4$ 是最优结果。这与我们之前可视化的结论是一致的。


** K-medoids聚类——基于样本中心聚类

'kmed_snp'与'kmean_snp'类似，都是给定 $K$ 个聚类中心，对数据集中
的 $n$ 个样本进行聚类。不同的是，k-medoids聚类的最小化目标是均方
误差而非距离之和。

*** medoids定义

与kmeans算法直接使用簇内所有样本特征向量之和的均值作为聚类中心
（这个中心很可能是训练集不存在的样本点）不同，kmedoids算法是从训
练集挑选 $K$ 个样本点作为聚类中心。这样定义的聚类中心能够使簇内样
本到聚类中心的距离的均方误差最小。与kmeans算法相比，本算法对数据
集中的噪声和奇异值具有更强的鲁棒性，因为均值很容易受前两者影响，
但样本点不会因为前两者而改变。


*** kmedoids函数

在MATLAB中， ~kmedoids()~ 函数用于实现'kmed_snp'算法，它的输入数
据为原始数据集，输出数据为与原始数据集行数（样本数）相同的向量，
向量中的值对应每个样本所属聚类中心的ID。与 ~kmeans()~ 相同，此函
数默认使用 ~k-means++~ 算法进行算法初始化，并默认使用
~sqeuclidean~ 距离。

现在正式开始代码实现。这次我们考虑一个货运公司的例子。一个货运公
司有多家配送站，为了能够最小化运输成本、最短化配送时间，需要在公
司的诸多配送站中选择几个作为配送中心，负责对其它子配送站送货。现
在我们需要选出需要设立的，能够使总配送距离最短的配送中心位置及个
数。配送中心的地理坐标已经保存在了 ~PeripheralLocations.xls~ 文件
中，现在我们将其导入MATLAB：

#+BEGIN_QUOTE
代码
#+END_QUOTE

文件中的数据被存储在 ~PerLoc~ 变量中。接下来我们绘制散点图对数据
进行初步观察：

#+BEGIN_QUOTE
代码
#+END_QUOTE

下图显示了配送站的分布状况：

#+BEGIN_QUOTE
图6.16：配送站散点图
#+END_QUOTE

观察上图，我们初步的印象是这些配送站大体可被分为三个区域，这意味
着我们需要设置三个配送中心。因此我们可以假设需要有三个聚类，并且
将聚类中心设为配送中心。

显然，聚类中心必须是真实的、已经存在的配送站，而不能是一个数值上
的几何中心，因此 ~kmeans()~ 函数在这里是不适合的。我们可以使用
~kmedoids()~ 函数完成这个目标：

#+BEGIN_QUOTE
代码
#+END_QUOTE

其中：

- ~IdCluster~ 包含每个样本点（配送站）所属的聚类中心ID
- ~Kmedoid~ 是 $K$ 个聚类中心（配送中心）的坐标向量（这里是 $3$
  个）
- ~SumDist~ 包含每个聚类中，所有样本点到中心的距离之和
- ~Dist~ 包含每个样本到每个样本中心的距离
- ~IdClKm~ 包含每个聚类中心的ID
- ~info~ 包含算法的默认参数

我们来看下 ~info~ 中的参数：

#+BEGIN_QUOTE
代码
#+END_QUOTE

下图中标记出了聚类结果及聚类中心，与 kmeans 部分相同我们为了方便
观看对颜色和图形进行了相关处理。

#+BEGIN_QUOTE
图6.17：添加聚类结果及聚类中心的散点图
#+END_QUOTE

从图6.17中我们可以看出， ~kmedoids()~ 函数将上面的样本点分成了由
不同颜色表达的三个聚类。而算法所选择的聚类中心，也是我们视觉直观
上所选取的几何中心上的配送站作为配送中心。


*** 评估聚类结果

与 kmeans 小节相同，我们可以使用 ~silhouette~ 从直观视觉上评估算
法的聚类效果。 ~silhouette~ 图衡量一个聚类中的样本点距离相邻聚类
的样本点有多近。横坐标表示 ~silhouette~ 值，它的取值范围是
$[-1,1]$ ，其中 $1$ 表示样本点距离相邻聚类中心非常远， $0$ 表示比
较近，而$-1$ 则表示有可能错分了样本点。图中的值越高，说明聚类结果
越好。如果多数样本点的值都很高，则说明聚类结果很好。如果多数样本
点的值都比较低，甚至是负值，则说明聚类结果很差，这种情况下我们需
要重新考虑算法默认参数的设置情况（初始化算法、距离指标）。

#+BEGIN_QUOTE
代码
#+END_QUOTE

#+BEGIN_QUOTE
图6.18：kmedoids 聚类结果 ~silhouette~ 图
#+END_QUOTE

图6.18中我们可以看到，只有两个聚类中的两个样本点为负值，并且非常
接近 $0$ ，而且图形的厚度没有明显、剧烈的波动，几个聚类的图形大体
相同。因此我们可以看出之前 $K=3$ 的参数设置是比较好的。


** 高斯混合模型(GMM)聚类

目前我们已经讨论了多种聚类方法，这些方法在数学、模型上都很直观。
接下来我们将介绍基于概率分布的方法，这类方法通过假设样本服从某种
概率分布，使用优化算法对数据集拟合概率分布，以期达到最好的聚类效
果。在这里我们主要介绍参数估计（预先假设概率分布的形状）方法，例
如连续数据上的高斯分布和离散数据上的泊松分布。


*** 高斯分布

真实世界中，很少由数据集能够符合单一高斯分布（只有一个极值），然
而再复杂的概率分布函数，也可以使用多个高斯分布的线性组合进行一定
程度的逼近。由多个同族概率分布函数组合成的概率分布，就称为混合概
率模型。所谓'gmm_snp'，就是指由多个高斯分布函数线性组合成的，具有
多个极值点的概率分布函数。这个概率分布函数可以表示为多个高斯分布
的加权平均数，其中每个分布函数代表某个样本属于这个聚类的概率，其
权重则由这个聚类在总样本中所占比例的后验概率表示。我们可以将GMM看
作由多个期望值聚集在聚类几何中心的高斯分布函数组成的混合分布函数。


*** MATLAB中的GMM支持

在MATLAB中，我们可以使用 ~fitgmdist()~ 函数实现GMM。它将返回一个
对数据集拟合后，类型为 ~GMModel~ 包含 $K$ 个元素的变量，其中每个
元素将保存一个拟合参数（均值和方差）后的高斯分布。每个元素中，含
有长度为 $n$ （ $n$ 为样本的特征向量长度，即特征值个数）的均值向
量，大小为 $n\times n$ 的协方差矩阵，以及每个高斯分布的混合系数
（权重）。

函数默认使用 *EM算法* 对模型参数进行求解。通过某种算法（通常先对
数据集使用 kmeans 算法）进行初始化后， *EM算法* 进行以下两步：

1. *Expectation* （更新期望值）：对每个样本，算法重新计算该样本属
   于各个聚类的后验概率，其结果是一个 $n\times K$ 的后验概率矩阵
2. *Maximization* （最大化期望值）：以上面计算的后验概率矩阵为权
   重，对各个聚类的高斯分布使用最大似然估计重新优化参数

这个算法不断重复以上步骤直到算法收敛， *EM算法* 不保证收敛到全局
最优值，很可能陷入局部最优值，因此收敛结果将受初始化算法影响。算
法训练完成后，将返回一个 ~gmdistribution~ 类型的模型变量，其中包
含了如模型参数、收敛结果以及其它默认参数等信息。我们可以使用 ~.~
运算符查看这些属性。

现在我们看一个实际例子。假设我们从全球各地对两个气候指标进行采集，
现在我们的目标是对这个数据集按照气候群进行聚类。这些数据保存在
~ClimaticData.xls~ 文件中。我们首先导入数据：

#+BEGIN_QUOTE
代码
#+END_QUOTE

导入的数据保存在大小为 $1570\times 2$ 的变量 ~Data~ 中。首先我们
使用散点图初步观察数据集：

#+BEGIN_QUOTE
代码
#+END_QUOTE

下图为数据集散点图：

#+BEGIN_QUOTE
图6.19：气候数据集散点图
#+END_QUOTE

通过观察图6.19，我们可以看出图中的样本点，按照两个指标（二维特征
向量）明显分成两类。因此我们有理由假设聚类数量 $K=2$ 。值得注意的
是，这两类数据集分布形状明显不同，左侧的样本点更像圆形，上方的样
本点则像椭圆形。

两个不同形态的聚类代表这个数据集是混合了源于多个概率分布函数的样
本点组成的。之前我们提到，GMM模型可被用来近似任意形状的概率分布函
数。因此我们这里假设分布函数由两个高斯分布线性组合而成，并使用
~fitgmdist()~ 函数进行求解：

#+BEGIN_QUOTE
代码
#+END_QUOTE

在这个模型中，我们得到了两个高斯分布，上面的结果显示了其均值。此
外我们还得到了两个混合系数（每个高斯分布在线性组合中的权重）。上
文中提到， ~fitgmdist()~ 函数有多种随机初始化方法，其中包括随机初
始化以及使用 ~k-means++~ 算法进行初始化等，我们可以从帮助文档中获
得更多信息：

#+BEGIN_QUOTE
代码
#+END_QUOTE

为了验证我们确实得到了有两个高斯分布函数组成的混合高斯模型，我们
可以在散点图中画出每个高斯分布函数的等高线图。MATLAB中
~ezcontour()~ 被用来绘制等高线图，代码如下：

#+BEGIN_QUOTE
代码
#+END_QUOTE

下图中，显示了原始数据集散点图，即GMM算法所拟合的高斯分布的等高线
图：

#+BEGIN_QUOTE
图6.20：混合高斯分布模型等高线图
#+END_QUOTE

我们可以看出图中的两个高斯分布具有明显不同的形态，并且对原始数据
集有很好的拟合效果。


*** 使用后验概率分布进行聚类

我们可以使用 ~cluster()~ 函数估计各个聚类的后验概率，并通过最大后
验概率方法对每个样本进行聚类。这个函数通过比较每个样本属于各个聚
类的概率，给出使概率最大的每个样本的聚类结果（其中每个聚类的聚类
中心是高斯分布的均值点）：

#+BEGIN_QUOTE
代码
#+END_QUOTE

~cluster()~ 函数根据每个样本的成员值对样本进行归类。其中成员值具
体是指该样本是由某个高斯分布生成的后验概率值。这个函数将具有最高
成员值的聚类中心ID赋值给各个样本。我们来看下散点图中的聚类结果：

#+BEGIN_QUOTE
代码
#+END_QUOTE

接下来我们将聚类中心添加到散点图上，我们已经说过每个高斯分布的均
值就是其聚类中心，我们可以通过如下代码得到聚类中心坐标：

#+BEGIN_QUOTE
代码
#+END_QUOTE

接着将聚类中心添加到散点图上：

#+BEGIN_QUOTE
代码
#+END_QUOTE

最后我们添加分布函数的等高线图：

#+BEGIN_QUOTE
代码
#+END_QUOTE

下面的散点图中，标记出了聚类中心以及每个样本点的聚类结果：

#+BEGIN_QUOTE
图6.21：气候聚类散点图。标记了聚类中心及聚类结果。
#+END_QUOTE

最后我们展示如何可视化GMM算法中估计的高斯概率分布。我们可以使用
~gmdistribution()~ 函数对高斯分布进行可视化。这个函数将会创建一个
~gmdistribution~ 类型的变量保存需要绘制的高斯函数的信息。首先我们
需要使用GMM拟合的多个高斯分布函数的参数创建这个变量，我们可以使用
如下代码先取出参数（均值、协方差），再创建变量：

#+BEGIN_QUOTE
代码
#+END_QUOTE

最后我们可以画出三维空间中的高斯分布图：

#+BEGIN_QUOTE
代码
#+END_QUOTE

下图中展示了由两个高斯分布函数组成的混合高斯模型：

#+BEGIN_QUOTE
图6.22：拟合后的高斯混合模型
#+END_QUOTE

上图中清晰地显示了组成混合模型的两个高斯分布函数的形态。


** 总结

本章中，我们学习了如何使用MATLAB进行聚类分析。首先我们解释了相似
度的衡量方法，我们学习了诸如'prodis_snp'等概念。接下来我们介绍了
基于这些指标进行聚类的方法，例如层次聚类和原型聚类。在层次聚类中，
样本是按照从上至下或从下至上的顺序被一一归类的。在原型聚类中，算
法将原始数据集划分为多个区域。

对层次聚类法，我们介绍了 ~pdist()~ 、 ~linkage()~ 和 ~cluster()~
三个函数。这些函数是用从下至上的顺序对数据集中的样本进行层次聚类。
其中 ~pdist()~ 函数用于衡量样本间的距离。为衡量样本间
的'prodis_snp'我们可以使用 ~linkage()~ 函数。而 ~cluster()~ 函数
能够返回任意聚类水平下每个样本的聚类结果。最后我们学习了如何理
解'dendrogram_snp'以及如何验证聚类算法效果。

接下来我们首先通过'kmean_snp'算法介绍了原型聚类算法。我们介绍如何
通过迭代的方法渐进的找出多个聚类的聚类中心。同时我们学习了如何理
解 ~silhouette~ 图，以及如何通过它区别不同参数设置聚类效果的好坏。
最后我们还通过定量的方法对聚类效果进行了衡量。

之后我们介绍了另一种原型聚类算法：'kmed_snp' 。这种算法与kmeans不
同的是，它使用真实的样本点而非均值，作为聚类中心。值得再次强调的
是，尽管计算复杂度更高，但kmedoids算法比kmeans算法具有更好的鲁棒
性，能够更好应对数据集中的噪声和奇异值。

最后我们学习了基于概率的距离方法：'gmm_snp'。通过使用多个高斯分布
的线性组合，我们能够近似地逼近任意形状的概率分布。与kmeans方法相
同，GMM同样需要用户预先指定 $K$ 个概率分布。每个概率分布由长度为
$n$ （样本特征向量长度）的均值向量和 $n\times n$ 的协方差矩阵表示。
我们可以使用 ~fitgmdist()~ 函数对GMM模型在数据集上进行参数求解。
接下来我们可以使用 ~cluster()~ 函数获得每个样本的聚类结果。最后我
们学习了如何绘制三维高斯分布等高线图。

下一章将会介绍MATLAB中实现神经网络的基本知识。我们将学习如何对数
据进行预处理；如何使用神经网络拟合数据集、模式识别以及聚类分析，
并且将学习如何可视化神经网络及其计算结果，以及如何评估、优化神经
网络性能。

