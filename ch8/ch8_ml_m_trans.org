#+LATEX_HEADER: \usepackage{ctex}
#+LATEX_COMPILER: xelatex

* 第八章：降维——改进机器学习模型性能

在处理大数据时，我们往往会遇到非常复杂的问题。例如，如何对数百个
变量构建数学模型，且维持计算可解呢（很多简单的数学模型、算法计算
复杂度随特征向量维度指数级增长，尽管有解析解，但实际中计算机可能
需要几个月甚至几年才能得出结果，这种模型、算法被称为
（Computationally infeasible）计算不可解）？如何对数百维向量进行
可视化呢（人们只能理解三维图形，加上颜色、形状，可以可视化五维数
据，但更高维度的可视化需要许多技巧）？学术界针对高维数据中的处理
问题研究了一系列方法，其中最常用的一种称为降维（dimensionality
reduction）处理。降维是指将一组高维数据映射到低维数据，同时尽最大
可能减小信息损失的数据处理方法。其目标是用最少维度的数据表达高维
数据，同时最小化信息损失，通常有两类方法可以实现这个目标：特征选
择和特征提取。特征选择是直接在高维数据中，选取少数维数据代表整体。
特征提取则通过建立高维数据到低维数据映射关系的方法，降低特征矩阵
维度。

降维方法之所以有效，是因为真实世界的数据集中普遍存在两种现象：噪
声（noise）和信息冗余（特征值矩阵的多个维度相关性极高、衡量问题的
同一属性。例如圆的直径、半径和周长，这三个特征值是线性相关的，已
知一个值可以没有信息损失地计算出另外两个值）。通过降维方法，我们
能够找出特征值矩阵中最不具有相关性的几个维度，并只使用这些维度代
表整个特征值矩阵，继续后面的建模、求解等工作，以降低计算难度。降
维具有多种用处，例如，对于噪声高的数据集，降维能够很大程度去除噪
声；几十、数百维的模型是人脑无法理解的，降低维度可以增强模型的可
解释性；降维后的结果更易于可视化等。

本章将展示如何进行特征选择和特征提取。读者将了解不同方法适用的不同
场景，以及其优缺点。我们将会讲述以下主题：

- 分步回归
- 'pca_snp'

学完本章，我们将学会多种降维方法，并理解特征选择和特征提取的区别。
我们将能够针对不同数据集选择最适用的方法进行降维。


** 特征选择

一般而言，当我们处理高维数据时，降维，将输入数据集中到最有代表性
（最不相关）的几个维度并丢弃其它维度，往往能够带来更好的结果，例
如使模型具有更强的泛化性能。特征选择是一种降维方法，它能够通过一
系列的优化和运算，发现数据集中最显著的几维特征值。特征选择通过降
低输入特征值矩阵的维度，能够使只能在小特征矩阵上应用的简单模型、
或者基于非常强假设（例如线性方程假设特征值间没有多重共线性）对大
型矩阵进行处理。下图显示了特征选择的主要流程：

#+BEGIN_QUOTE
图8.1：特征选择的主要流程
#+END_QUOTE

通常，大型矩阵中会包含诸多冗余信息，甚至错误信息（噪声）。特征选
择，降低特征值矩阵维度能够帮助研究人员建立更有效率的模型，例如，
特征选择能够显著降低运算量，提高CPU和内存的使用效率。特征选择的作
用通常有：

- 清理数据集（减少噪声）、使数据集更易于理解
- 减少模型变量，增强模型可解释性
- 减少求解时间
- 降低特征值间的关联度，避免过拟合问题，增强模型泛化能力

特征选择是指在特征值矩阵中，通过使用迭代的方法，不断比较各种组合
的计算结果，选择一组数量较少的特征值子集来代替原有的高维矩阵。具
有最小误差的特征值组合将被选为最终结果，并用于后续模型的应用。

在进行特征选择前，我们首先需要制定特征选择的标准。通常选用使用不
同特征组合训练出的模型，在训练集上的误差指标作为评选标准。基于这
种方式选择出的特征值组合，在数据集上能够使模型拥有最佳表现。当然，
这种方法有非常强的局限性，只适用于较小的数据集以及计算复杂度较低
的模型。

特征选择在需要从诸多特征值中选择几种最有效特征值，且不能够对特征
值进行二次处理、必须使用原始特征值时最为适用。另外再处理不易于进
行数值映射（如类别数据）的数据类型时也极为有效。


*** 分步回归

在第四章开始讲述回归分析前，我们介绍过回归分析有两种用途，其中一
种是使用回归结果得到的权重来理解自变量对因变量的解释程度。换句话
说，我们通过回归方程中自变量的权重，能够决定每个自变量对因变量的
重要程度有多大。权重绝对值越大，自变量对因变量的影响力就越高。分
步回归正是一种逐个向回归方程中添加自变量，以找到最能解释因变量的
自变量组合的方法。在多种自变量逐个添加的算法中，有三种方式最常用：

- 前向方法：首先计算所有自变量与因变量的相关性系数，接着按照系数
  从大到小的顺序，逐个向拥有 $0$ 个自变量的回归方程中，添加自变量
  进入回归方程，直到即使添加变量对改进回归方程拟合效果不产生显著
  影响，则停止加入新的自变量。此时产生的自变量组合就是特征选择结果
- 后向方法：首先计算所有自变量与因变量的相关性系数，接着按照系数
  从小到大的顺序，从使用全部自变量作为回归方程的方程中，逐个剔除
  自变量，直到剔除自变量对拟合结果产生显著影响，则停止剔除。此时
  产生的自变量组合就是特征选择结果
- 双向方法：按照多种拟合结果评价指标，来回地添加、剔除变量

下图中显示了三种选择算法梗概：

#+BEGIN_QUOTE
图8.2：前向、后向、双向特征选择方法

前向方法：每次迭代添加一个自变量
后向方法：每次迭代剔除一个自变量
双向方法：每个自变量都有可能被添加、剔除
#+END_QUOTE

为进一步理解分步回归是如何挑选特征的，接下来我们使用MATLAB一步步
编写代码实现分步回归。


*** MATLAB中的分步回归支持

MATLAB中分步回归作为建模前筛选特征值的重要手段已经被完全自动化了。
MATLAB提供了 ~stepwiselm()~ 函数用于建立分步回归方程。这个方程将
返回一个线性模型，用于逐步添加或删除用户指定的特征值矩阵中的特征
值。用户可以选择 ~stepwiselm()~ 函数是用前向、后向或者双向算法进
行特征选择。这个函数首先根据属性 ~modelspec~ 创建一个初始化方程，
接着不断添加、删除特征值，并对每次迭代后模型拟合能力进行比较。通
过多次迭代，最具影响力的特征值被加入线性方程，影响力小的特征值则
被剔除出方程。

每次迭代，算法都会按照 ~Criterion~ 变量值所设定的比较准则对特征值
进行挑选，之后对模型计算结果计算 $P$ 值或者 $F$ 值，用于衡量加入、
剔除特征值对模型拟合能力所产生的影响。如果一个特征值还没有被加入
模型，那么假设检验的原假设是，加入这个特征值对模型拟合能力不产生
影响，即这个特征值的系数为 $0$ 。如果求解参数后，该特征值的系数显
著不为 $0$ ，那么则拒绝原假设并将特征值加入方程。相反的，如果特征
值已经包含在模型中，那么原假设则是该特征值系数为 $0$ 。如果该特征
值的系数显著为 $0$ ，那么则接受原假设并将特征值剔除出方程。

分步回归经过了以下步骤：

1. 初始化模型
2. 如果存在一些未被加入方程的特征值，其 $p$ 值小于加入模型的阈值，
   则加入具有最小 $p$ 值的特征值，并重复这步；否则继续执行第三步
3. 如果存在一些被加入模型中的特征值，其 $p$ 值大于剔除出模型的阈
   值，则将 $p$ 值最大的特征值剔除出模型，并返回第二步；否则结束
   算法

即使原始特征矩阵相同，分步回归的最终结果会随着模型初始化不同、特
征值加入及剔除的顺序不同等因素而改变。当没有特征值被添加、剔除出
模型时，算法停止并返回该次迭代特征值组合作为最终结果。

接下来我们使用'uci_snp'中的真实数据举例学习分步回归。

这里我们使用 *Yacht Hydrodynamics* 数据集进行学习。这个数据集记录
了帆船在行驶过程中的流体动力学数据，例如加速度和方向。在实际生产
中，帆船设计的非常重要的一步就是评估不同帆船参数对帆船在水中的阻
力所产生的影响，对阻力预测的精确程度直接影响帆船设计的商业价值。
其中帆船的船体数据和速度是非常重要的两个指标。在这个数据集中，输
入数据主要包含了船体的几何参数、Froude系数等指标，目标向量则是每单
位重量受到的阻力大小。数据集中包含了以下指标：

- 浮力的纵向中心坐标
- 菱形系数
- 长度迁移系数
- 船宽吃水比例
- 船长吃水比例
- Froude系数
- 每单位重量受到的阻力值

首先我们需要从'uci_snp'中下载数据，并保存到当前文件夹。MATLAB提供
了 ~websave()~ 函数帮助我们完成这个工作，它能够访问用户指定的
~URL~ 地址，下载数据，新建并保存到当前文件夹中的文件。这里我们首
先制定 UCI 数据集的下载地址：

#+BEGIN_QUOTE
代码
#+END_QUOTE

现在我们使用 ~websave()~ 函数将数据保存在名为
~yacht_hydrodynamics.csv~ 的文件中：

#+BEGIN_QUOTE
代码
#+END_QUOTE

接着我们对文件中的变量按照上文介绍的顺序进行命名：

#+BEGIN_QUOTE
代码
#+END_QUOTE

现在我们可以将数据读取到 ~table~ 类型的变量中：

#+BEGIN_QUOTE
代码
#+END_QUOTE

至此MATLAB的工作空间中已经导入了数据，并保存为 ~table~ 类型的变量。
现在我们可以开始进行分步回归了。下图中显示了名为
~YachtHydrodynamics~ 的变量：

#+BEGIN_QUOTE
图8.3：YachtHydrodynamics 数据集
#+END_QUOTE

我们首先输出一下数据集的统计特征：

#+BEGIN_QUOTE
代码
#+END_QUOTE

从结果中我们看到，这个数据集中有相当多的缺失值 ~NumMissing~ （即
空值， ~NaN~ ）。然而空值对 ~stepwiselm()~ 函数并没影响，因为这个
函数默认忽略空值。但是我们之前介绍过，我们可以使用 ~ismissing()~ 、
~standardizeMissing()~ 、 和 ~rmmissing()~ 对空值进行处理，它们的
用法在第三章中有详细叙述。

为了避免空值对算法产生潜在不良影响，这里我们首先将有空值的样本删
去。我们可以使用 ~rmmissing()~ 函数对任意 ~array~ 和 ~table~ 类型
的变量进行此项工作：

#+BEGIN_QUOTE
代码
#+END_QUOTE

为了确认有缺失值的样本确实被移除了，我们可以比较移除前后两个矩阵
的大小：

#+BEGIN_QUOTE
代码
#+END_QUOTE

其中 ~YachtHydrodynamicsClean~ 变量行数更少，因为有缺失值的样本被
删除了。为了进一步确认，我们可以重新输出变量的统计信息：

#+BEGIN_QUOTE
代码
#+END_QUOTE
我们可以看到这里已经没有缺失值了（为了节省空间这里省略输出结果）。
使用MATLAB的一个最佳实践是，使用 ~matrix~ 类型的变量来保存特征值
矩阵，使用 ~array~ 类型的变量保存目标向量。因为我们之前使用
~table~ 类型的变量保存原始数据，这里我们需要对变量类型进行转化：

#+BEGIN_QUOTE
代码
#+END_QUOTE

在讲变量输入 ~stepwiselm()~ 函数前，我们首先对原始数据集
~YachtHydrodynamicsClean~ 进行观察。我们可以使用散点图，分别打印
出目标向量对每个特征值的散点图：

#+BEGIN_QUOTE
代码
#+END_QUOTE

#+BEGIN_QUOTE
图8.4：每个自变量对因变量的散点图矩阵
#+END_QUOTE

粗略观察下上图我们不难发现，第六个特征值与因变量具有极强的相关性
（图8.4中右下角散点图）。接下来我们看下分步回归的输出结果与我们的
观察是否一致。

我们可以使用如下代码构建对船体——阻力数据集的分步回归模型：

#+BEGIN_QUOTE
代码
#+END_QUOTE

这里我们看到，算法从一个常数开始，逐渐添加变量进入回归方程，并且
最终判定只有 $x6$ 即 ~FroudeNumber~ 对目标向量具有显著的解释作用
（权重显著不为 $0$ ）。接下来我们试试后向算法，逐渐从回归方程中剔
除变量的结果与上面的结果是否相同：

#+BEGIN_QUOTE
代码
#+END_QUOTE

这里可以看到，即使从包含全部自变量的回归方程开始逐步剔除变量，仍
然只有 $x6$ 变量被保存下来。不同的是这次算法对每个变量的显著性水
平都进行了计算，并且可以看到每次迭代都剔除了 $p$ 值最高的自变量。
接下来我们使用双向算法，对包含交叉项（有两个自变量乘积项的线性方
程）的回归方程进行试验：

#+BEGIN_QUOTE
代码
#+END_QUOTE

尽管顺序不同，双向算法的结果仍然与之前两个算法结果相同。这个算法
成对比较自变量间相互关系，并在每次迭代都将 $p$ 值最高的自变量剔除
出模型。

接下来我们使用包含交叉项的，包含全部自变量的二次方程进行试验。初
始化模型中包含针对所有自变量的二次项、交叉项、一次项和截距变量。
与之前相同，显著性最低的项会被算法移除出方程。这里之所以使用二次
项，是因为在图8.4中，变量间似乎存在二次关系：

#+BEGIN_QUOTE
代码
#+END_QUOTE

这次算法返回了较为复杂的模型。其中不止包含 $x6$ ，并且包含其二次
项 $x6^2$ 。另外还包含了 $x2$ ，及二者的交叉项 $x2\times x6$ 。虽
然模型更加复杂，但是我们从拟合结果中可以看到（ $R- squared=0.927, Adjusted R-Squared =0.926, p-value = 3.1e-139$ ），
模型对数据集有了更强的解释能力。

我们已经看到，从不同的初始模型出发， ~stepwiselm()~ 函数会返回不
同选择结果。我们来对之前的选择结果进行比较。首先我们可以比较不同
特征值组合的 $R^2$ 值：

#+BEGIN_QUOTE
代码
#+END_QUOTE

可以看到，前三个模型结果基本相同，第四个结果则显著更好。我们可以
通过绘制拟合残差图来进一步比较几组结果。这里我们可以使用
~plotResiduals()~ 函数，它将根据用户指定的残差图类型，对模型拟合
的残差结果进行绘制。这里我们针对拟合结果绘制残差图：

#+BEGIN_QUOTE
代码
#+END_QUOTE

#+BEGIN_QUOTE
小贴士：残差是模型输出结果和真实值间的差值。残差代表着模型对数据
集的拟合误差。
#+END_QUOTE

下图中，我们绘制了四组结果的残差图：

#+BEGIN_QUOTE
图8.5：四组结果残差图矩阵
#+END_QUOTE

可以看到，前三个模型误差水平完全相同。四幅图中都显示出了模型残差
与目标值间的非线性关系。然而，第四个模型残差显然更加集中。我们可
以通过比较四组结果残差的分布区间进一步观察：

#+BEGIN_QUOTE
代码
#+END_QUOTE

我们可以确认，更加复杂的模型（第四个模型）残差波动的标准差更小
（因为上面结果中残差分布区间更小）。


** 特征提取

当数据集达到普通程序、硬件无法进行处理时，我们必须建立从大数据矩
阵到低维矩阵的映射关系。这种从特征值矩阵到函数处理后低维矩阵的映
射关系被称为特征提取。特征提取基于原始数据集，通过某种方式对这些
特征值进行二次加工计算，产生新的、数量更少的特征值用于代替原始数
据集，这种方式能够显著降低特征矩阵中信息冗余水平。整个过程由下图
所示：

#+BEGIN_QUOTE
图8.6：特征提取流程图
#+END_QUOTE

通过从诸多特征中提取出更小的特征集合，不仅接下来数学模型的计算速
度会加快、拟合表现可能会提高，而且会大大提升模型的可解释性。这个
过程能够基于原始特征矩阵，衍生出新的特征值指标，这个新的、特征值
数量更少的特征矩阵，往往具有更少的噪声，因此能够提高分类准确度，
同时也提升了分类的计算效率。如果特征提取的效果足够好，那么使用降
维后的特征值矩阵往往能够获得至少和使用原始特征矩阵一样好的拟合效
果。


*** 主成分分析(PCA)

在对高维矩阵进行建模时，其中最大的困难就是太高的维数（太多的特征
值）对绝大多数理论较为简单的概率、频率模型会造成 *维度灾难* （模
型的计算复杂度随维度呈指数级增长）。从数学模型上着手，构建能够处
理大型矩阵的模型，往往需要更高深的数学知识，这是大多数人所不具备
的。

幸运的是，我们可以通过对原始大型矩阵进行降维，使这些较为简单的模
型也能够处理这些问题。之所以能够进行降维，很大程度需要归因于，在
这些大型矩阵中，许多特征值之间具有极强的相关性。因为现实世界中同
一问题的同一属性往往具有多个角度进行衡量（例如圆的半径、直径和周
长）。如果相关程度极高的多个特征值存在于同一矩阵中，那么我们说这
个矩阵存在 *信息冗余* 。如果我们能够通过某种映射关系，使用这些相
关度极高的指标作为输入数据，将其计算为一个统一的指标，实现从高维
矩阵，到一维向量的映射，那么从很大程度上我们只需要使用映射后的一
维向量，就能够代替原高维矩阵进行建模计算。这就是降维的基本思路。
下面的图片显示了矩阵中两组相关程度极高的几个指标：

#+BEGIN_QUOTE
图8.7：矩阵中的信息冗余
#+END_QUOTE

'pca_snp'能够使用原始数据集作为输入，生成一组最大程度互不相关的特
征值作为新的特征矩阵，这些新生成的特征值被称为原始矩阵的 *主成分*
（Principal Components）。每个主成分都是原始数据集中所有特征值的
线性组合。各个主成分之间则是两两正交的（主成分之间完全互不相关），
因此新生成的矩阵不存在任何信息冗余。主成分矩阵可以看做原始矩阵的
一组正交基。PCA的目标就是使用最少个数的主成分来最大程度解释原始矩
阵。因此主成分分析本质上是一种多元线性变换，它将高维矩阵映射到低
维矩阵，同时期望造成最小程度的信息损失。请读者记住，一个主成分仅
仅是原矩阵中全部特征值的一个线性组合。

在MATLAB中，我们可以使用 ~pca()~ 函数完成'pca_snp'。它将返回用于
计算主成分的系数（线性组合的权重），称为载荷（loadings）。对于一
个大小为 $n\times m$ 的原始特征矩阵，其中行数 $n$ 表示样本个数，
列数 $m$ 表示特征值个数，返回的权重系数矩阵 ~coeff~ 大小为
$m\times m$ ，其中 ~coeff~ 的每一列系数对应着计算一个主成分所需的
线性组合中，对应原始特征值的全部权重。同时每列的排列顺序是按照主
成分的重要性进行排序的。 ~pca()~ 函数默认使用奇异值分解（Singular
Value Decomposition，SVD）算法来计算主成分系数矩阵。

下面我们使用'uci_snp'中的实际数据集学习'pca_snp'。

这里我们使用一个著名的种子数据集，它包含了来自三种不同种属小麦的
种子的几何学数据。网站上有简要介绍，这三种小麦的名称为 Kama、Rosa、
和Canadian，每个种属都选取了70粒种子，并从中随机抽取一些作为样本。
采集过程中使用了高分辨率的X射线柯达照相机对种子进行了拍摄了大小为
$13 \times 18$ 厘米的图片。实验中所用到的种子，是从波兰的卢布林省
的Institute of Agrophysics of the Polish Academy of Sciences（波
兰科学院农业研究所）的试验田中得到的。

这个数据集包含了 $210$ 个样本数据，每个样本具有 $7$ 个特征值。这
些特征值包括：

- 面积
- 周长
- 密度
- 长度
- 宽度
- 不对称系数
- 种子槽长度

之前提到过，这 $210$ 个种子来自于三个种属，每个种属选取了 $70$ 个
样本。

在开始之前，我们先从'uci_snp'下载数据并保存到当前文件。MATLAB提供
了 ~websave()~ 函数帮助我们完成这个工作，它能够访问用户指定的~URL~
地址，下载数据，新建并保存到当前文件夹中的文件。这里我们首先制定
UCI 数据集的下载地址：

#+BEGIN_QUOTE
代码
#+END_QUOTE

接下来我们将下载好的数据保存在名为 ~seeds_dataset.csv~ 的文件中：

#+BEGIN_QUOTE
代码
#+END_QUOTE

我们按照上文提到的特征值的顺序对这些特征值进行命名：

#+BEGIN_QUOTE
代码
#+END_QUOTE

读取数据到MATLAB，并将数据保存为 ~table~ 类型的变量：

#+BEGIN_QUOTE
代码
#+END_QUOTE

现在之前下载好的数据已经加载在MATLAB的工作空间中，并保存为
~table~ 类型的变量，我们可以使用这些数据研究'pca_snp'了。
你还记得在介绍分步回归时，我们介绍过如何处理缺失数据吗？这里
我们再次遇到了相同的数据缺失问题：

#+BEGIN_QUOTE
代码
#+END_QUOTE

这个命令将返回一组逻辑类型的向量，来表示对应行数是否出现了缺失值。
为了从中只提取包含缺失值的样本，我们可以使用如下代码：

#+BEGIN_QUOTE
代码
#+END_QUOTE

这个命令将返回一个大小为 $22\times 1$ 向量，其数值代表着包含缺失
值的样本在 ~Seeds_dataset~ 中的行数。为了避免缺失值对算法产生潜在
影响，这里我们将其剔除出数据集。我们可以使用 ~rmmissing()~ 函数完
成这项工作（这个函数适用于任意 ~array~ 或者 ~matrix~ 类型的变量）：

#+BEGIN_QUOTE
代码
#+END_QUOTE

结果可以看出，原本大小为 $221\times 8$ 的矩阵，现在变成 
$199\times 8$ 。其中前 $15$ 行样本确实不包含任何缺失值，
结果如下图所示：

#+BEGIN_QUOTE
图8.8：种子表格中的前 $15$ 行数据（其中确实不包含缺失值）
#+END_QUOTE

在将数据集输入到 ~pca()~ 函数之前，我们需要先对数据集进行粗略观察。
数据集中前 $7$ 列数据是样本的特征值向量，第 $8$ 列保存的是样本类
别标签，即样本属于哪个种属。我们首先观察下特征值间相关性如何。通
过使用 ~plotmatrix()~ 函数我们能够构建一个MATLAB散点图矩阵。但是
我们现在使用的是 ~table~ 类型的变量，在绘制前我们首先需要对其类型
进行转换：

#+BEGIN_QUOTE
代码
#+END_QUOTE

接着我们可以使用 ~plotmatrix()~ 函数绘制散点图矩阵：

#+BEGIN_QUOTE
代码
#+END_QUOTE

其中，对角线上的箱状图绘制的是对应列数特征值的箱状图。而剩余的散
点图绘制的则是两两特征值之间的关系（第 $i$ 行第 $j$ 列图片是使用
原数据集中的第 $i$ 列和第 $j$ 列特征值绘制的），结果如下图所示：

#+BEGIN_QUOTE
图8.9：特征矩阵绘制的散点图矩阵
#+END_QUOTE

从图8.9中我们可以看出，散点图矩阵是一种可视化特征矩阵，发现特征值
间线性相关性的非常好的方法。通过对散点图矩阵的观察，我们能够定位
哪几组特征值间具备相关性，并可初步判断特征值矩阵信息冗余的程度大
小。之前已经提到，在对角线上的箱状图是使用特征值自身数值绘制的，
它能够让我们大体了解特征值的分布状况。其余图形都是使用一对特征值
绘制的散点图。具体而言，第 $i$ 行第 $j$ 列图片是使用原数据集中的
第 $i$ 列和第 $j$ 列特征值绘制的。

从图8.9中我们初步观察即可发现，特征矩阵中多对特征值存在高度的相关
性。例如我们看到使用我们代码中命名为特征值 ~Area~ 和 ~Perimeter~
以及 ~Perimeter~ 和 ~LengthK~ 绘制的散点图中，散点基本围绕在一条
从原点出发斜向上的直线周围，说明这些指标存在正向相关。而另外一些
散点图则非常散乱，观察不到明显规律，说明两个指标相关程度很低，例
如， ~LengthKG~ 这个指标跟所有其它特征值间都观察不到明显的变化规
律。

除了从视觉角度观察得出结论，我们仍然可以使用量化的方法对上述推测
予以确认。这里我们可以使用 ~corr()~ 函数来计算矩阵的相关性系数，
这里我们使用 ~r~ 命名这个函数返回的矩阵类型的变量：

#+BEGIN_QUOTE
代码
#+END_QUOTE

相关性系数的取值范围是 $-1$ 到 $1$ ，其中 $-1$ 表示完全负相关；而
当其为 $0$ 时则表示两个特征值是正交的，完全不相关；如果为 $1$ 则
表示完全正相关（相关性矩阵算法非常基础、关键，建议读者查阅相关算
法，熟练掌握）。为更好地理解散点图矩阵图8.9并学习相关性系数的用法，
这里我们使用第一个特征值 ~Area~ 和其他特征值绘制的散点图为例，将
其相关性系数绘制在散点图下方，如图所示：

#+BEGIN_QUOTE
图8.10：根据 ~Area~ 特征值绘制的，标注有相关性系数的，与其它特征
值的散点图
#+END_QUOTE

分析图8.10中我们能够得出以下结论：

- 在左边第一幅图中，散点图的分布紧密围绕在一条从原点出发斜向上的
  直线周围，说明两个指标相关程度非常高，这时我们发现相关性系数也
  几乎为 $1$ （ $0.9944$ ）
- 第二幅图中，尽管散点图的分布貌似有二次函数的关系，但是分布还是
  比较凌乱、随机，证明存在一定相关性程度但不是非常高，我们也可以
  通过相关性系数看出两者确实存在中等偏上程度的相关性（ $0.6099$ ）
- 第三幅图中，我们再次观察到了几乎成为一条直线的分布情况，但与图
  一比要分散一些，因此我们判断相关性程度应该不如图一高，相关性系
  数也验证了此点（ $0.9511$ ）
- 相似的情况也发生在图四中（ $0.9710$ ）
- 第五幅图中，我们看到完全不同的分布情况。与之前的图形相比，散点
  的分布状况非常随机，表示两个特征值几乎不存在相关性。仔细观察图
  形，还是存在一些从左上到右下的分布趋势，而相关性系数也印证了此
  点（ $-0.2228$ ）
- 最后，第六幅图又是斜向上的直线的分布状态，尽管更为松散
  （ $0.8627$ ）

通过上面的分析读者可能会问，散点图与相关性系数间的关系是什么。在
一些情况中，散点图能够通过视觉传达给我们一些相关系数所不包含的信
息。事实上，如果我们通过散点图不能观察到明显的相关性，一般相关性
系数的绝对值也不会很大，所以我们一般会遇到以下两种情况：

- 如果散点图观察不到明显相关性，那么计算相关性系数的意义不大，因
  为相关性系数只能反应两个特征值线性相关的强度
- 如果散点图观察到明显相关性但不是线性相关，那么相关性系数会产生
  误导，因为高次（如第二幅图中二次相关性）相关性也是相关性

这就是为何散点图矩阵要比相关性系数矩阵重要的原因。另一方面，我们
看到相关性系数矩阵中，一些特征值的线性相关程度在 $0.9$ 以上，这就
表示这两者高度线性相关，即数据集中存在大量信息冗余。我们可以通过
MATLAB提供的 ~pca()~ 函数来消除这种冗余性，从而实现降维：

#+BEGIN_QUOTE
代码
#+END_QUOTE

这个函数会返回如下计算结果：

- ~coeff~ ：主成分权重
- ~score~ ：主成分得分
- ~latent~ ：主成分方差
- ~tsquared~ ：每个样本的 *霍特林T平方分布* 值
- ~explained~ ：每个主成分解释的方差在总方差中的比例
- ~mu~ ：每个特征值的均值

前面数据预处理的章节中提到过，当特征矩阵中的特征值量纲（即计量单
位）不同时，对数据进行去量纲、标准化运算非常重要。这里 ~pca()~ 函
数在进行'pca_snp'运算或者奇异值分解（SVD）求取主成分矩阵之前，将
会默认对特征值矩阵进行标准化运算，以去除不同特征值量纲不同造成的
影响。

MATLAB提供了三种算法用于计算主成分：

- 奇异值分解算法（SVD）
- 协方差矩阵特征值（eigenvalue）分解算法（注意，这里的特征值指的
  是线性代数概念中，矩阵的特征值，英文为eigenvalue。机器学习中所
  说的特征值英文为feature，指的是样本的一个衡量指标）
- 最小二乘法

~pca()~ 函数默认使用奇异值分解算法计算主成分。

现在我们来仔细分析 ~pca()~ 函数返回的计算结果。首先我们来查看
~coeff~ 变量，这个变量包含了从原始特征矩阵线性变换到主成分矩阵所
使用的权重向量：

#+BEGIN_QUOTE
代码
#+END_QUOTE

~coeff~ 中的每一列代表一个主成分的权重向量，每一列中的每一行，按
顺序对应 ~VarMeas~ 矩阵即原始特征值矩阵中，每个特征值的权重。其中
主成分的排列从左到右是按照其重要程度降序排列的。即 ~coeff(:,1)~包
含的是最重要的主成分的权重向量，以此类推。从线性代数角度而言，权
重矩阵 ~coeff~ 是对原始特征值矩阵 ~VarMeas~ 的一个线性变换，
~coeff~ 的每一列都是一个以 ~VarMeas~ 每一行作为输入数据的线性方程
的权重。 ~coeff~ 七列表示的是 $7$ 个相互正交的线性方程。

这里我们用公式对主成分算法进行进一步表述。每个主成分都是原始特征
值向量的线性组合。假设原始特征矩阵大小为 $n\times m$ 即包含 $n$
个样本和 $m$ 个特征值，那么每个样本的每个主成分指标就是这 $m$ 个
特征值的线性组合。由于MATLAB在返回的结果中按照重要性将主成分排序，
因此第一个主成分具有最大的方差；第二个主成分在保持与第一个主成分
正交的前提下，拥有第二大的方差，以此类推。我们可以使用如下方程
（向量化表达）对全部样本计算第一个主成分向量：

#+BEGIN_align
PC1 = 0.8852 * Area + 0.3958 * Perimeter + 0.0043 *
Compactness + 0.1286 * LengthK + 0.1110 * WidthK - 0.1195 *
AsymCoef + 0.1290 * LengthKG
#+END_align

这里第一个主成分向量 $PC1$ 的大小为 $n\times 1$ ，即有多少个样本，
每个主成分的长度就是多少。并且向量中的每个值，都是对应样本在原始
特征矩阵中特征值向量作为输入参数，以 ~coeff~ 为权重的线性组合。第
二个主成分同理。主成分之间全部是正交的，且其重要性（方差）按顺序
递减。

~pca()~ 函数的第二个输出参数， ~score~ 包含的正是原始特征矩阵根据
权重矩阵 ~coeff~ 线性加权计算后得到的，每个样本的 $7$ 个主成分值，
即按照上面公式中的算法得到的 $PC1$ 到 $PC7$ 。因为 ~pca()~ 返回了
具有 $7$ 个主成分的权重矩阵，因此矩阵 ~VarMeas~ 和矩阵 ~score~ 的
大小相同。

我们的目标是使用更小维度的矩阵来代替原有特征值矩阵。这里得到的降
维后的主成分矩阵 ~score~ 与原特征值矩阵大小相同，显然没有起到降维
作用。但这是因为我们还没有从主成分中进行挑选。实践中，往往前一两
个主成分，对整个原始特征矩阵就有 $90%$ 以上的解释作用。这里我们首
先绘制前两个主成分的散点图进行观察。

#+BEGIN_QUOTE
小贴士： ~score~ 矩阵包含的就是每个样本每个主成分的数值。
#+END_QUOTE

为了让图片更易于理解，我们对散点图按其所属的小麦种属（类别标签）
用颜色形状加以区分：

#+BEGIN_QUOTE
代码
#+END_QUOTE

下图中，我们使用前两个主成分，对全部样本点绘制散点图：

#+BEGIN_QUOTE
图8.11：仅使用前两个主成分绘制的散点图
#+END_QUOTE

我们很容易能够看出，图8.11清晰地将三类样本点区分在了三个区间中。
属于不同种属的小麦的散点聚集在不同区域，区域之间仅有很少样本的重
叠。为了定位这些样本点，我们可以使用 ~gname~ 函数。这个函数将弹出
一个绘图窗口，并等待用户进行鼠标点击操作。移动鼠标，光标将会变成
叉号。如果你将光标叉号放在靠近某个样本点的位置，并单击一下鼠标，
MATLAB将会自动在散点图上标注出该样本点的标签（在这里将会标记样本
点的行号）。当你完成标注后，可以通过 ~Enter~ 或 ~ESC~ 键退出。

鉴于当前我们已经绘制好了图片，我们可以直接在命令窗口中输入以下命
令：

#+BEGIN_QUOTE
代码
#+END_QUOTE

通过点击靠近相邻区域，貌似类别标签重叠的样本边界区域中的样本点，
我们能够将该样本点在矩阵中的行号标注在散点图中，结果如下图所示：

#+BEGIN_QUOTE
图8.12：标注后的散点图
#+END_QUOTE

从图8.12中我们可以看到，那些我们用肉眼观察貌似分类不准确的样本点，
在主成分矩阵中的行号是多少，也就是在原始特征矩阵中是第几个样本。
通过这种方法我们能够非常精确地定位问题所在，并进一步分析这些有问
题的样本，试图找出解决方法。

现在我们继续学习 ~pca()~ 函数的输出。第三个输出变量是 ~latent~ ，
它是一个长度与主成分个数相同的向量，表示了这 $7$ 个主成分每个主成
分分别对原始特征矩阵解释了多大程度，即每一行的数值就是 ~score~ 主
成分矩阵中对应的一列主成分的方差值。之前已经提到过，主成分是按照
降序排列的，因此 ~latent~ 同样也是降序：

#+BEGIN_QUOTE
代码
#+END_QUOTE

我们可以将其绘制成曲线图帮助理解：

#+BEGIN_QUOTE
代码
#+END_QUOTE

我们将 ~latent~ 矩阵绘制成了折线图，并对两个坐标轴进行了标注，其
横坐标是主成分的序号，纵坐标是每个主成分的方差。我们可以通过这幅
折线图很容易地看出每个主成分负担了多大程度对原始数据集的解释：

#+BEGIN_QUOTE
图8.13：每个主成分对原数据集的解释程度（方差）
#+END_QUOTE

一般而言，多数主成分分析后的结果都有十分陡峭的折线图。这意味着前
几个主成分就已经能在绝大程度上解释（代表）原有数据集，其余的主成
分则仅仅对原有数据集有非常小的解释力度。在图8.13中，最大的变化出
现在第二个和第三个主成分之间。因此我们选择前两个主成分组成的矩阵
来代替原有包含 $7$ 个特征值的特征矩阵。

为更好地理解做出这种选择的原因，我们可以绘制每个主成分解释的方差
在总方差中所占的比例。这个比例已经包含在 ~pca()~ 函数返回的结果
~explained~ 中。接下来我们对 ~explained~ 变量绘制帕累托图：

#+BEGIN_QUOTE
代码
#+END_QUOTE

#+BEGIN_QUOTE
图8.12：前两个主成分对总数据集解释能力比例图
#+END_QUOTE

图8.14中我们看到两个图形，其中柱形图代表每个主成分对原有数据集解
释的比例有多大（纵轴为百分数），上面的折线则表示两个主成分，即两
个柱状图的累积和是多少。通过观察图8.14我们能够确认之前的选择是正
确的，因为前两个主成分就已经能够对原数据集中 $99\%$ 以上的信息进
行解释。

最后，我们对两个主成分对每个原始特征值的权重进行可视化，并将每个
样本点计算所得的这两个主成分的值绘制在同一张散点图上。这种散点图
被称为主成分的 *biplot* ：

#+BEGIN_QUOTE
小贴士： *biplot* 同时两组信息表示在一幅图上。一组是以每个特征值
在每个主成分的权重系数（ ~coeff~ 的第一行的前两列，即是图中
~Area~ 向量的坐标）表示成向量绘制在图中。另一组数据将 ~score~ 即
全部样本点的主成分为坐标表示成散点绘制在图中。
#+END_QUOTE

#+BEGIN_QUOTE
图8.15： 根据权重矩阵 ~coeff~ 和主成分矩阵 ~score~ 绘制的前两个主
成分的 *biplot* 图
#+END_QUOTE

在图8.15中，全部 $7$ 个特征值在计算两个主成分时所使用的权重，都以
向量的形式表示在图中。通过观察这些向量的方向和长度，我们就能够判
断每个特征值对这个主成分的贡献程度有多少。例如，对第一个主成分而
言，即横坐标轴表示的主成分，通过观察 ~coeff~ 我们看到有 $6$ 个特
征值都在第一个主成分上有正的权重，唯独特征值 ~AsymCoef~ 的权重为
负数。与此对应，有六个向量都在第一象限，只有一个向量 ~AsymCoef~
在第二象限。同时我们还可以看到，在横坐标方向上， ~Area~ 的向量长
度最长，与之对应它在 ~coeff~ 中的权重最大。

对第二个主成分而言，即纵坐标轴表示的主成分，同样在 ~coeff~ 中我们
看到有 $6$ 个特征值都在第二个主成分上有正的权重，唯独特征值
~Compactness~ 的权重为非常接近于 $0$ 。与此对应， ~Compactness~
在横坐标方向的长度非常短，几乎不可见。因此我们可以看出，图中向量
的坐标，就表示了每个向量所对应的特征值，对每个主成分的影响力的大
小。我们可以从图中很清晰地看出，对第一个主成分，指标 ~Area~ 具有
最大的影响力；指标 ~AsymCoef~ 则对第二个主成分影响力最大。


** 总结

本章我们学习使用了如何通过降维的方法，使用最少的数据量最大程度代
表原有数据集。我们首先介绍了降维的基本概念及其面对的问题，接着我
们学习了如何使用特征选择的方法进行降维，我们使用分步回归进行了举
例。最后我们学习了，当允许使用原有特征值生成新特征值的情境下，如
何使用 'pca_snp' 方法从源特征矩阵中提取主成分。

我们首先学习了MATLAB中如何使用 ~stepwiselm()~ 函数创建一个可用于
分步回归的线性方程，以及特征加入、剔除出方程的三种方式。接着我们
比对了三种方式的计算结果：我们先创建了一个空线性方程，逐渐向其中
加入变量；创建一个包含全部特征值的方程，逐步从中剔除变量；最后我
们边加入边剔除变量。此外我们回顾了MATLAB中剔除空值的方法。

接着我们学习了特征提取的算法，具体而言我们学习了'pca_snp'。PCA是
这类降维方法中最著名、最常用的一种。它通过对原特征矩阵进行线性变
化，产生一组新的被称为主成分的矩阵。其中每个主成分值都是对原始特
征矩阵进行线性变换的结果。所有的主成分都是互相正交的，因此主成分
矩阵中不存在信息冗余。整个主成分矩阵可以被看作一组原数据空间中的
正交基。

我们学习了如何在MATLAB中使用 ~pca()~ 函数实现'pca_snp'。我们学习
了这个函数的输出变量，例如权重系数矩阵、主成分矩阵、方差矩阵的实
际意义。另外，我们学习了如何通过可视化的方法，挑选最显著的几个主
成分，作为对原始特征值矩阵的代表，以实现降维的目的。实践中我们发
现，往往前几个（在我们的例子中前两个）主成分就足以解释 $99\%$ 以上
的原始数据集。最后我们介绍了如何绘制 *biplot* 以及其作用。

在介绍、学习了多种机器学习算法后，下一章我们将系统地应用这些算法
解决实际问题。作为最后一章，我希望尽量精简地介绍几个最主要的机器
学习算法如何解决实际问题。读者将学会如何把回归、分类、聚类算法应
用到实际数据集上。

