#+LATEX_HEADER: \usepackage{ctex}
#+LATEX_COMPILER: xelatex

* 第五章：模式识别之分类算法
分类算法能够学习如何对观测到的数据进行分类。进行分类学习的第一步
是对数据集进行标注。对分类的每个类别，我们都预先定义一个标签
(label)，这个标签可以是任意值（整数、字符串等，一般用正整数），接
着对每个样本的'feve_snp'，我们都对其标注一个预先定义好的标签，用
于表示这个样本属于哪个类别。我们称只有两个类别（标签）的分类问题
为二分类问题；有多个类别（标签）的则称为多分类问题。我们将从样本
的'feve_snp'到标签的映射函数称为'classifier'。

分类问题与第四章中学习的回归问题有一些相似之处。例如，它们都是从
已经标注好的数据集中，学习输入数据到输出数据的映射关系的。两者最
大的区别在于，回归问题的输出数据是连续值，而分类问题的输出数据是
离散值，即分类的类别标签。

举例而言，回归模型可被用于基于前10年的石油价格数据，预测未来的石
油价格。然而，二分类模型可被用于预测石油价格的走势，即是涨还是跌。
在回归问题中，输出数据石油价格是连续变量。而在分类问题中，尽管输
入数据不变，仍然是前10年石油价格数据，即连续变量，但输出数据变成
了涨跌分类，即价格走势是涨还是跌。

本章中，我们将展示如何使用'knn_snp'、'dian_snp'分类、'detree_snp'
分类和'naba_snp'算法。我们还将介绍概率论在分类问题中所扮演的角色。
在本章末尾，我们将理解这些算法的内容，并学会如何在MATLAB中实现、
运行这些算法。

** 'detree_snp'分类
'detree_snp'可被看为是一系列判断结果及其判断条件的图形化展示，尤
其是对那些人脑极难描述清楚的判断条件的展示。一个判断结果往往是根
据一连串层级化的判断条件得出的，这使我们很难单纯地使用表格、数字，
来正式地、易于人脑理解地描述这些判断过程。

与表格、数字相反，树形结构能够很好地帮助我们描述、理解一系列的判
断结果及其条件。通过追溯整棵树，人脑可以立即理解整个判断的过程及
所有可能的结果。与其通过一系列代数推理、公式描述来理解判定结果，
先观察到判定结果、再通过回溯整棵决策树显然更易于人脑理解。

决策树由以下几部分构成：

- 'node_snp'表示变量，其名称即变量名称
- 'branch_snp'表示变量在其定义域中能够选取的范围
- 'leaf_snp'表示分类结果。（没有子节点的节点即为叶节点）

通过上面这些定义，我们能够对数据集中每个样本进行分类，并能够给出
分类结果是正确的概率的大小。因此，'detree_snp'能够给出任意一个样
本有多大概率属于任一分类。下图展示了一颗分类树：

#+BEGIN_QUOTE
图5.1 分类树示例
#+END_QUOTE

通过学习标注后的训练集，我们能够总结出分类规律（树状结构）。当在
测试集完成测试，达到满意的精度后，算法所生成的决策树可被用来对未
经标注的数据进行分类。

#+BEGIN_QUOTE
小贴士：'detree_snp'是最简单的分类算法之一。这种算法通过遍历全部
变量、及其可选的分类条件，来生成最优分类树。
#+END_QUOTE

对于一元二分类问题（样本的类别只由一个变量决定；只有两个类别），
决策树只由一个节点、两个分支和两个叶节点构成。对于多元分类问题
（样本的类别由多个变量共同决定），决策树由多个一元决策树的线性组
合构成。

决策树由'node_snp'，'branch_snp'和'leaf_snp'构成。具体而言，每
个'node_snp'代表一个变量，并由变量名命名。每条边（分支）上标注的
是沿这个分支，这个'node_snp'的取值范围。'leaf_snp'则标注最终分类
结果，及类别标签。

当决策树用于分类时，对未标注的样本，根据其特征向量中每个变量的值，
沿着决策树从根节点走到叶节点，即得到该样本的分类结果。所走过的路
径即是对此样本分类过程中所用到的判断条件。对于一颗决策树，每条从
根节点到'leaf_snp'的路径都是一组判断条件。

使用决策树对样本进行分类的过程可总结为：

1. 从根节点开始
2. 根据当前节点的名称，选择待分类样本特征向量中，对应变量的值
3. 根据当前节点的分支的定义域，及待分类样本对应的值，选择待分类样
   本所属的分支
4. 根据分支到达下一节点。如果此节点是'leaf_snp'，算法结束并返
   回'leaf_snp'的值（即分类结果）；如果此节点是'node_snp'，则回到
   第二步继续执行

为了更好地理解这个过程，我们来考虑图5.2中的例子。这个树包含两个自
变量 $x1$ 和 $x2$。如果待分类样本的特征向量中，$x1<0.3$且$x2>0.6$，
那么它的分类结果就是 ~true~ ，反之则为 ~false~ 。整个分类过程如下：
从根节点（图中节点由$\Delta$表示）开始执行算法，当
前'node_snp'（根节点）的标签是 $x2$，根据分支条件，如果待分类样本
的特征向量中$x2$的值大于$0.6$，我们则选择右分支。右分支的子节点
是'leaf_snp'，因此算法结束并返回分类结果 ~true~ 。如果待分类样本
中$x2$小于$0.6$，则选择左分支。左分支的子节点为$x1$，因此我们根据
待分类样本中$x1$的值进行判断。如果$x1$小于$0.3$，则返回分类结果
~true~ ，反之则返回 ~false~ 。

构建并展示图5.2的代码如下：

#+BEGIN_QUOTE
代码
#+END_QUOTE

先不要着急阅读上面的代码，我们稍后会进行详细解释。首先我们先看上
面代码所生成的决策树的图形展示：

#+BEGIN_QUOTE
图5.2 决策树样例
#+END_QUOTE

对于有编程基础的读者上图非常容易理解。决策树表示成程序就是面向过
程编程中一系列的条件判断(~if-else~)语句。当到达'leaf_snp'即没有更
多 ~if~ 语句时，程序则执行结束并返回分类结果。

当我们到达'leaf_snp'即得到分类结果时，分类过程中所使用的一系列判
断条件都展示在执行算法所经过的路径上。因此，任何决策树都可以看成
是一组判断规则的集合。每条判断规则代表着从树的根节点到叶节点的路
径。一棵树有多少叶节点，表示成集合这个集合中就有多少条规则。由规
则的集合所表达的决策树，与树结构表达的决策树拥有相同的计算复杂度。

接下来我们将展示如何在MATLAB中实现整个流程。'smltb_snp'封装有从训
练数据中学习决策树所需的所有工具。为了重复整个流程，我们将使用机
器学习的经典数据集——Iris Flower数据集。这是一个由英国统计和生物学
家Ronald Fisher在其1936年发表的论文《The use of multiple
measurements in taxonomic problems》中用于展示其线性判别分析算法
所使用的，多元分类（每个样本的特征向量有多个特征值，及因变量）数
据集。

#+BEGIN_QUOTE
（已安装MATLAB的读者不需下载）可以通过如下链接：

https://archive.ics.uci.edu/ml/datasets/iris

从'uci_snp'中下载数据集及其简短描述。
#+END_QUOTE

这个数据集收集了三种Iris花(Iris setosa, Iris virginica, and Iris
versicolor)每种50个样本。每个样本的特征向量包含4个特征值（以厘米
为单位）：

- 萼片(Sepal)长度
- 萼片(Sepal)宽度
- 花瓣(Peal)长度
- 花瓣(Peal)宽度

- 数据集中类别标签为： ~Setosa~, ~Versicolour~, ~Virginica~

接下来我们通过学习Iris数据集中150个样本的特征向量（每个样本4个特
征值）到分类标签（供三种标签）的映射关系，构建决策树。MATLAB已经
自带了Iris数据集。我们可通过执行如下代码将数据集加载
到'workspace_snp'中：

#+BEGIN_QUOTE
代码
#+END_QUOTE

执行代码后，两个变量被加载到了MATLAB： ~meas~, ~species~ 。第一个
变量为150个样本的特征向量构成的矩阵（150x4 ~double~ 类型）。第二
个变量是对每个样本所属类型进行标注的标签向量 (150x1 ~cell~ 类型）。
通过执行如下代码可以得到变量的统计数据：

#+BEGIN_QUOTE
代码
#+END_QUOTE

通过执行上述代码，我们验证了这150个样本所属的类别是均匀分布的。接
下来我们通过绘制数据集的'scatter_plot_snp'来观察每种类别所对应的
特征值的分布情况。

在第三章中，我们已经学习过如何绘制'scatter_plot_snp'，但是这里我
们需要使用绘制函数的额外功能。现在我们希望除了散点，我们还能绘制
每个点所述的类别。我们可以通过 ~gplotmatrix()~ 函数实现这点。
~gplotmatrix()~ 的调用语法是：

#+BEGIN_QUOTE
代码
#+END_QUOTE

这里， ~a~ （m x n1) , ~b~ (m x n2) 表示的是全部样本（m个）的特征
矩阵。执行代码将得到(n1 x n2)副散点图。第i行j列散点图是使用 ~a~
矩阵的第 ~i~ 列特征向量和 ~b~ 矩阵的第 ~j~ 列特征向量绘制的。在我
们的例子中，我们希望根据 ~meas~ 特征矩阵，及 ~species~ 中标注的类
别标签绘制散点图：

#+BEGIN_QUOTE
代码
#+END_QUOTE

执行结果如下图所示：

#+BEGIN_QUOTE
图5.3 按照 ~species~ 进行分类绘制的散点图矩阵
#+END_QUOTE

粗略观察图5.3，我们可以看出 *setosa* 类与其它两类非常不同。相反的，
其余两类在所有散点图中都存在大量重合。

我们首先仔细观察根据花瓣的特征值（长度和宽度）绘制的散点图。我们
可以使用 ~gscatter()~ 函数实现这点。 ~gscatter()~ 函数的输入参数
要求为三个参数，两个长度相等的特征向量和一个标签向量。 ~meas~ 变
量的第3列表示花瓣长度，第4列表示花瓣宽度，因此我们的代码是：

#+BEGIN_QUOTE
代码
#+END_QUOTE

散点图中清晰地展示了三种不同的花在特征空间中是如何分布的：

#+BEGIN_QUOTE
图5.4 按照分类标签标注的散点图
#+END_QUOTE

上图表明，根据花瓣的特征值对花朵进行分类是可行的（上图只根据花瓣
的长度和宽度已经能够看出清晰的分类结果）。为对整个特征值矩阵构建
决策树，我们可以使用 ~fitctree()~ 函数。这个函数返回基于特征矩阵
和标签向量构建的二叉树状决策树：

#+BEGIN_QUOTE
代码
#+END_QUOTE

返回的二叉树中每个节点有两个分支，分支结果是按照特征值矩阵 ~meas~
中每一列向量（样本数 x 1）进行优化得出的（优化算法即 ~fitctree~ 所
封装的算法，再次我们此函数的忽略实现细节）。现在我们可以使用函数
~view()~ 绘制二叉树图。 ~view()~ 函数有两种使用方法。单纯调
用 ~view(ClassTree)~ 命令将返回文字描述的 ~if-else~ 指令集（上面讨
论过指令集等同于二叉树结构）。 ~view(ClassTree, 'mode', 'graph')~
添加参数后调用则会绘制二叉树树状图。下面我们先看第一种调用方法：

#+BEGIN_QUOTE
代码
#+END_QUOTE

正如我们所见， ~fitctree()~ 优化生成的二叉树中只使用了 $x_3$ 和
$x_4$ 两个特征，即花瓣的长度和宽度。下面我们来绘制二叉树的树状图：

#+BEGIN_QUOTE
代码
#+END_QUOTE

下图展示了二叉树树状图。每个'node_snp'及其'branch_snp'表示了选
取某条路径，样本的特征值所必须满足的取值范围。'leaf_snp'表示最终
分类结果：

#+BEGIN_QUOTE
图 5.5：二叉树树状图
#+END_QUOTE

图5.5清晰可见地展示出给花朵分类的整个流程。在完成构建决策树后（即
执行 ~fitctree()~ 后，我们可以非常方便地使用 ~ClassTree~ 对新的样
本进行分类。假设待分类新样本的特征向量如下（萼片的长度、宽度；花
瓣的长度、宽度）：

#+BEGIN_QUOTE
代码
#+END_QUOTE

为对样本进行分类，我们将待分类样本的特征向量 ~measNew~ 输入训练好
的决策树 ~ClassTree~ 中对分类结果进行预测：

#+BEGIN_QUOTE
代码
#+END_QUOTE

~predict()~ 函数返回一个最终分类的标签标量或向量，这取决于输入参
数是单一样本的特征向量还是多个样本的特征值矩阵。上面代码只会返回
一个标量，及待分类样本 ~measNew~ 经 ~ClassTree~ 分类后的分类标签。
当输入多个分类样本时，返回的向量顺序与样本的输入顺序一一对应。

目前为止，我们已经学习了如何使用标注好的数据集训练'detree_snp'。
现在我们需要验证其预测效果优劣。我们都有什么工具来检验决策树的预
测效果呢？

首先，我们需要一个计算训练误差的指标。训练误差衡量决策树对待分类
样本所预测的分类标签，与数据集中真实标签的差距的指标。这种指标可
以初步地表达模型性能的优劣。这种衡量是单调的，即指标值越高，说明
预测结果与真实值差异越大，预测效果越坏。反之，低指标值反应出模型
有很好的预测效果。

使用如下函数计算训练误差：

#+BEGIN_QUOTE
代码
#+END_QUOTE

结果显示决策树能够对绝大多数样本给出正确分类。接下来我们继续验证
模型的预测误差。与训练误差是衡量模型对训练集中已经学习过（现有参
数通过这些样本求出）样本的预测性能不同，预测误差使用预测集中的样
本，即模型仍未学习过的数据（现有参数的整个优化过程中与这些样本无
关），对模型的泛化能力（在预测集中、对新样本的预测能力）进行评估。
我们使用'cv_snp'进行验证。MATLAB中对'cv_snp'的默认设置是10折交叉
验证。'cv_snp'将整个数据集等分为10份，随机选取9份作为训练集训练决
策树，使用剩下的1份作为预测集，对模型预测结果进行检验。我们可以使
用如下代码实现'cv_snp'：

#+BEGIN_QUOTE
代码
#+END_QUOTE

我们首先调用 ~crossval()~ 函数，它将使用'cv_snp'的方法对模型进行
检验，并返回一个交叉验证模型 ~cvrtree~ 。接着，我们使
用~kfoldLoss()~ 函数来计算整个 'cv_snp' 的预测误差。结果显示即使
对模型没见过的样本（预测集中、模型训练过程没使用过的样本），决策
树依然能够对绝大多数样本给出正确分类。

** 概率分类模型——'naba_snp'








** 'dian_snp'分类
** 'knn_snp'
** MATLAB分类器APP
** 总结
